{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-ibm-tutorials","title":"Welcome to IBM Tutorials","text":"<p>Welcome to the centralized repository for all technical explainers, tutorials, and demos developed by the ibm.com technical writing team. This repository focuses on cutting-edge technologies in the fields of:</p> <p>Generative AI | Machine Learning | LLMs | Prompt Engineering</p> <p>To get started, head to our Getting Started Guide to ensure your setup is ready for use. </p> <p>If you have any questions, please reach out in our Github Discussions.</p>"},{"location":"about/","title":"About IBM Tutorials","text":"<p>These tutorials provide walk-throughs, technical guides, and working demos to help you better understand the AI ecosystem. Whether you are starting out or looking to deepen your understanding, you will find valuable content here.</p>"},{"location":"coc/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"coc/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"coc/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"coc/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"coc/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"coc/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement via email. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"coc/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"coc/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"coc/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"coc/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"coc/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"coc/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>First off, thanks for taking the time to contribute! \u2764\ufe0f</p> <p>All types of contributions are encouraged and valued. Please make sure to read the relevant section before making your contribution. It will make it a lot easier for us maintainers and smooth out the experience for all involved. The community looks forward to your contributions. \ud83c\udf89</p> <p>And if you like the project, but just don't have time to contribute, that's fine. There are other easy ways to support the project and show your appreciation, which we would also be very happy about: - Star the project - Tweet about it - Refer this project in your project's readme - Mention the project at local meetups and tell your friends/colleagues</p>"},{"location":"contributing/#i-want-to-contribute","title":"I Want To Contribute","text":""},{"location":"contributing/#legal-notice","title":"Legal Notice","text":"<p>When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content and that the content you contribute may be provided under the project license.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#local-setup","title":"Local Setup","text":"<p>For each tutorial or application, the local set up can vary. We recommend having Python 3.11 and pip installed. For any activities with JavaScript/TypeScript, we recommend having NodeJS version 23.10.0 and npm installed, though yarn may also be required depending on the project.</p> <p>It may also be necessary to have a local version of Granite, ollama, and other tools. Please be sure to check the README for any codebase or the instructions in any tutorial to ensure you have the proper set up prior to attempting any coding or notebook runs.</p> <p>Note: YMMV - as different machines have different operating parameters, we cannot guarantee all setups will run locally. If you have any questions, please reach out in our Github Discussions.</p>"},{"location":"getting-started/#cloud-setup-ibm-cloud","title":"Cloud Setup - IBM Cloud","text":"<ul> <li>Head to IBM Cloud</li> <li>Click the \u201cCreate an account\u201d link</li> <li>Enter your email and preferred password using the safe password guidelines then click next</li> <li>Wait for a verification code to come through to the email address provided</li> <li>After being verified, enter your first and last name then click next, accept the Terms and Conditions then click continue to get into our IBM Cloud account.</li> </ul>"},{"location":"getting-started/#cloud-setup-watsonx","title":"Cloud Setup - watsonX","text":"<ul> <li>Navigate to watsonX</li> <li>Head to the \u201chamburger\u201d menu in the upper left corner of the screen and open the navigation menu (assuming it isn\u2019t already open)</li> <li>Click on the Project menu and select \u201cView All Projects\u201d</li> <li>You may already have a sandbox project available, but for the purpose of our tutorials we\u2019ll create a new project by clicking the New Project button in the right corner above the projects list.</li> <li>For some tutorials we may be importing items and we can follow those instructions separately, but if not let\u2019s continue with a fresh project.</li> </ul>"},{"location":"license/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"projects-list/","title":"List of Projects","text":"<ul> <li>AI Stylist</li> <li>AI Personal Trainer</li> <li>Multi-agent Call Center Analysis</li> <li>My Retail Advisor</li> <li>React Agent IT Support</li> <li>Silly Story Time Generator</li> <li>TTRPG AI Project</li> </ul>"},{"location":"tutorials-list/","title":"Tutorials \ud83d\udc69\u200d\ud83c\udfeb","text":""},{"location":"tutorials-list/#ai-models","title":"AI Models","text":"<ul> <li>Gradient boosting classifiers in Scikit-Learn and Caret</li> </ul>"},{"location":"tutorials-list/#generative-ai","title":"Generative AI","text":"<ul> <li>Implement agentic chunking to optimize LLM inputs with Langchain and watsonx.ai</li> <li>Build a LangChain agentic RAG system using the Granite-3.0-8B-Instruct model in watsonx.ai</li> <li>DeepSeek Reasoning: Improving the R1 distilled model with RAG and watsonx.ai</li> <li>Build a document-based question answering system by using Docling with Granite 3.1</li> <li>Implement function calling with the Granite-3.0-8B-Instruct model in Python with watsonx</li> <li>Use automatic speech recognition (ASR) to generate a podcast transcript using Granite 3.3 and watsonx.ai</li> <li>Implementing graph RAG using knowledge graphs</li> <li>Create a LangChain RAG system in Python with watsonx</li> <li>LangChain tool calling using Granite-3.0-8B-Instruct in Python with watsonx.ai</li> <li>Creating a RAG with Llamaindex</li> <li>LLM Agent Orchestration with Retrieval and Action Framework</li> <li>Use LLM guardrails with Llama Guard 3-11b-vision using watsonx</li> <li>Use Llama 3.2-90b-vision-instruct for multimodal AI queries in Python with watsonx</li> <li>Ollama Tooling</li> <li>Use Pixtral 12B for multimodal AI queries in Python with watsonx</li> <li>Build a PPT AI image analysis question answering system with Granite vision model</li> <li>Implement RAG chunking strategies with LangChain and watsonx.ai</li> <li>Evaluate RAG pipeline using Ragas in Python with watsonx</li> <li>Using the watsonx.ai Time Series Forecasting API to predict energy demand</li> <li> <p>Time Series Forecasting with Lag-Llama</p> </li> <li> <p>Perform zero-shot classification with a foundation model</p> </li> </ul>"},{"location":"tutorials-list/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>Prompt Chaining with LangChain - A Comprehensive Overview</li> <li>Prompt tune a Granite model using watsonx</li> <li>Use role prompting with IBM watsonx and Granite</li> </ul>"},{"location":"tutorials-list/#instructional-guides","title":"Instructional Guides","text":"<ul> <li>Creating OpenShift Multiple Architecture Clusters with IBM Power</li> <li>Post training quantization of Granite-3.0-8B-Instruct in Python with watsonx</li> </ul>"},{"location":"tutorials-list/#useful-test-data","title":"Useful Test Data","text":"<p>NYC Hourly Temperature Data - CSV</p>"},{"location":"tutorials/ai-models/gradient_boosting_classifier/","title":"Gradient boosting classifiers in Scikit-Learn and Caret","text":"In\u00a0[\u00a0]: Copied! <pre>install.packages('gbm')\ninstall.packages('caret')\ninstall.packages('palmerpenguins')\n\nlibrary(gbm)\nlibrary(caret)  \nlibrary(palmerpenguins)\n\nhead(penguins) # head() returns the top 6 rows of the dataframe\nsummary(penguins) # prints a statistical summary of the data columns\n</pre> install.packages('gbm') install.packages('caret') install.packages('palmerpenguins')  library(gbm) library(caret)   library(palmerpenguins)  head(penguins) # head() returns the top 6 rows of the dataframe summary(penguins) # prints a statistical summary of the data columns <p>The createDataPartition function from the caret package to split the original dataset into a training and testing set and split data into training (70%) and testing set (30%).</p> In\u00a0[\u00a0]: Copied! <pre>dim(penguins)\n\n# get rid of any NA\n\npenguins &lt;- na.omit(penguins)\nparts = caret::createDataPartition(penguins$species, p = 0.7, list = F)\n\ntrain = penguins[parts, ]\ntest = penguins[-parts, ]\n</pre>  dim(penguins)  # get rid of any NA  penguins &lt;- na.omit(penguins) parts = caret::createDataPartition(penguins$species, p = 0.7, list = F)  train = penguins[parts, ] test = penguins[-parts, ] <p>Now you're ready to train and test.</p> In\u00a0[\u00a0]: Copied! <pre>model_gbm &lt;- caret::train(\"species ~ .\",\n                          data = train,\n                          method = \"gbm\", # gbm for gradient boosting machine\n                          trControl = trainControl(method = \"repeatedcv\", \n                                                   number = 5, \n                                                   repeats = 3, \n                                                   verboseIter = FALSE),\n                          verbose = 0)\n</pre> model_gbm &lt;- caret::train(\"species ~ .\",                           data = train,                           method = \"gbm\", # gbm for gradient boosting machine                           trControl = trainControl(method = \"repeatedcv\",                                                     number = 5,                                                     repeats = 3,                                                     verboseIter = FALSE),                           verbose = 0) <p>Now you can use the predictive model to make predictions on test data:</p> In\u00a0[\u00a0]: Copied! <pre>pred_test = caret::confusionMatrix(\n  data = predict(model_gbm, test),\n  reference = test$species\n)\n\nprint(pred_test)\n</pre> pred_test = caret::confusionMatrix(   data = predict(model_gbm, test),   reference = test$species )  print(pred_test) <p>This will print:</p> <pre><code>Confusion Matrix and Statistics\n           Reference\nPrediction  Adelie Chinstrap Gentoo\n  Adelie        42         0      0\n  Chinstrap      0        20      0\n  Gentoo         1         0     35\n\nOverall Statistics\n                                          \n               Accuracy : 0.9898          \n                 95% CI : (0.9445, 0.9997)\n    No Information Rate : 0.4388          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                  Kappa : 0.984           \n\n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n                     Class: Adelie Class: Chinstrap Class: Gentoo\nSensitivity                 0.9767           1.0000        1.0000\nSpecificity                 1.0000           1.0000        0.9841\nPos Pred Value              1.0000           1.0000        0.9722\nNeg Pred Value              0.9821           1.0000        1.0000\nPrevalence                  0.4388           0.2041        0.3571\nDetection Rate              0.4286           0.2041        0.3571\nDetection Prevalence        0.4286           0.2041        0.3673\nBalanced Accuracy           0.9884           1.0000        0.9921\n\n</code></pre> <p>Due to the nature of cross validation with folds the sensitivity and specificity for each class may be slightly different than what is observed here, although the accuracy will be the same. The accuracy is quite good, even with the Chinstrap penguin, which makes up on 20% of the training dataset.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install seaborn pandas scikit-learn palmerpenguins\n</pre> !pip install seaborn pandas scikit-learn palmerpenguins  <p>Now install the libraries:</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom palmerpenguins import load_penguins\n</pre> import pandas as pd import seaborn as sns from sklearn.compose import ColumnTransformer from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import classification_report from sklearn.model_selection import cross_val_score, train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler from palmerpenguins import load_penguins <p>As in the R code, there are some NAs in the penguins dataset that need to be removed. This code snippet loads the dataset, removes any NA rows, and then  splits the data into features and target.</p> In\u00a0[\u00a0]: Copied! <pre># Load the penguins\npenguins = load_penguins() #initialize the dataset\npenguins = penguins.dropna()\nX = penguins.drop(\"species\", axis=1)\ny = penguins[\"species\"]\n</pre> # Load the penguins penguins = load_penguins() #initialize the dataset penguins = penguins.dropna() X = penguins.drop(\"species\", axis=1) y = penguins[\"species\"]  <p>Now create a training and testing split of the dataset, with 70% of the data pulled for training and 30% reserved for testing.</p> In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n   X, y, test_size=0.3, random_state=42\n)\n</pre> X_train, X_test, y_train, y_test = train_test_split(    X, y, test_size=0.3, random_state=42 ) <p>Next, you'll gather two lists of the column names, one for the categorical features of X and another for the numerical features, e.g. float64 or int64. Then, use ColumnTransformer from scikit-learn to apply different preprocessing to different column types. A OneHotEncoder will be applied to categorical features to convert them into binary vectors. A StandardScaler will be applied to numerical features to standardize them around a mean f 0 and a variance of 1.</p> In\u00a0[\u00a0]: Copied! <pre># Define categorical and numerical features\ncategorical_features = X.select_dtypes(\n   include=[\"object\"]\n).columns.tolist()\n\nnumerical_features = X.select_dtypes(\n   include=[\"float64\", \"int64\"]\n).columns.tolist()\n\npreprocessor = ColumnTransformer(\n   transformers=[\n       (\"cat\", OneHotEncoder(), categorical_features),\n       (\"num\", StandardScaler(), numerical_features),\n   ]\n)\n</pre>  # Define categorical and numerical features categorical_features = X.select_dtypes(    include=[\"object\"] ).columns.tolist()  numerical_features = X.select_dtypes(    include=[\"float64\", \"int64\"] ).columns.tolist()  preprocessor = ColumnTransformer(    transformers=[        (\"cat\", OneHotEncoder(), categorical_features),        (\"num\", StandardScaler(), numerical_features),    ] ) In\u00a0[\u00a0]: Copied! <pre>pipeline = Pipeline(\n   [\n       (\"preprocessor\", preprocessor),\n       (\"classifier\", GradientBoostingClassifier(random_state=42, criterion='friedman_mse', max_features=2)),\n   ]\n)\n</pre> pipeline = Pipeline(    [        (\"preprocessor\", preprocessor),        (\"classifier\", GradientBoostingClassifier(random_state=42, criterion='friedman_mse', max_features=2)),    ] ) <p>Next, perform cross-validation to evaluate how well your machine learning pipeline performs on the training data. Calling the fit method of the pipeline you created trains the model. The loss function uses Mean Squared Error or mse by default.</p> In\u00a0[\u00a0]: Copied! <pre># Perform 5-fold cross-validation\ncv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n\n# Fit the model on the training data\npipeline.fit(X_train, y_train)\n</pre> # Perform 5-fold cross-validation cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)  # Fit the model on the training data pipeline.fit(X_train, y_train)  <p>Now that the model has been trained, predict the test set and check the performance:</p> In\u00a0[\u00a0]: Copied! <pre># Predict on the test set\ny_pred = pipeline.predict(X_test)\n\n# Generate classification report\nreport = classification_report(y_test, y_pred)\n</pre>  # Predict on the test set y_pred = pipeline.predict(X_test)  # Generate classification report report = classification_report(y_test, y_pred)  <p>Print the results:</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\nprint(\"\\nClassification Report:\")\nprint(report)\n</pre> print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\") print(\"\\nClassification Report:\") print(report)  <p>This will print out the following:</p> <pre><code>Mean Cross-Validation Accuracy: 0.9775\nClassification Report:\n              precision    recall  f1-score   support\n      Adelie       1.00      1.00      1.00        31\n   Chinstrap       1.00      1.00      1.00        18\n      Gentoo       1.00      1.00      1.00        18\n    accuracy                           1.00        67\n   macro avg       1.00      1.00      1.00        67\nweighted avg       1.00      1.00      1.00        67\n</code></pre> <p>This is very close the accuracy reported by the R methods in the first part of this tutorial.</p>"},{"location":"tutorials/ai-models/gradient_boosting_classifier/#gradient-boosting-classifiers-in-scikit-learn-and-caret","title":"Gradient boosting classifiers in Scikit-Learn and Caret\u00b6","text":"<p>Gradient boosting is a powerful and widely used machine learning algorithm in data science used for classification tasks. It's part of a family of ensemble learning methods, along with bagging, which combine the predictions of multiple simpler models to improve overall performance. Gradient boosting regression uses gradient boosting to better generate output data based on a linear regression. A gradient boosting classifier, which you\u2019ll explore in this tutorial, uses gradient boosting to better classify input data as belonging to two or more different classes. Gradient boosting is an update of the adaboost algorithm that uses decision stumps rather than trees. These decision stumps are similar to trees in a random forest but they have only one node and two leaves. The gradient boosting algorithm builds models sequentially, each step tries to correct the mistakes of the previous iteration. The training process often begins with creating a weak learner like a shallow decision tree for the training data. After that initial training, gradient boosting computes the error between the actual and predicted values (often called residuals) and then trains a new estimator to predict this error. That new tree is added to the ensemble to update the predictions to create a strong learner. Gradient boosting repeats this process until improvement stops or until a fixed number of iterations has been reached. Boosting itself is similar to gradient descent but \u201cdescends\u201d the gradient by introducing new models. Boosting has several advantages: it has good performance on tabular data and it can handle both numerical and categorical data. It works well even with default parameters and is robust to outliers in the dataset. However, it can be slow to train and often highly sensitive to the hyperparameters set for the training process. Keeping the number of trees created smaller can speed up the training process when working with a large dataset. This step is usually done through the max depth parameter. Gradient boosting can also be prone to overfitting if not tuned properly. To prevent overfitting, you can configure the learning rate for the training process. This process is roughly the same for a classifier or a gradient boosting regressor and is used in the popular xgboost, which builds on gradient boosting by adding regularization.</p> <p>In this tutorial, you'll learn how to use two different programming languages and gradient boosting libraries to classify penguins by using the popular Palmer Penguins dataset.</p>"},{"location":"tutorials/ai-models/gradient_boosting_classifier/#step-1-create-a-notebook-using-r","title":"Step 1 Create a Notebook using R\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <p>Log in to watsonx.ai by using your IBM Cloud\u00ae account.</p> <p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> <p>Create a Jupyter Notebook.</p> <p>Make sure to select \"Runtime 24.1 on R 4.3 S (4 vCPU 16 GB RAM)\" when you create the notebook. This step opens a Jupyter Notebook environment where you can copy the code from this tutorial. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This Jupyter Notebook can be found on GitHub.</p>"},{"location":"tutorials/ai-models/gradient_boosting_classifier/#step-3-configure-libraries-and-data","title":"Step 3 Configure Libraries and Data\u00b6","text":"<p>In R the caret library is a powerful tool for general data preparation and for model fitting. You'll use it to prepare data and to train the model itself.</p>"},{"location":"tutorials/ai-models/gradient_boosting_classifier/#step-4-train-and-test","title":"Step 4 Train and Test\u00b6","text":"<p>The train method from the caret library uses R formulas, where the dependent variable (often also called a target) is on the left hand side of a tilde '~' and the independent variables (often also called a features) are on the right hand side of the '~'. For instance:</p> <pre><code>height ~ age\n</code></pre> <p>This would predict height based on age.</p> <p>To caret train, you pass the formula, the training data, and the method to be used. The caret library provides methods for many different types of training, so setting the method as \"gbm\"  is where you'll specify to use gradient boosting. The next parameter configures the training process. The \"repeatedcv\" method performs X-fold cross-validation on subsamples of the training set data points. Here, you specify specify 3 repeats of 5-fold cross-validation, using a different set of folds for each cross-validation.</p>"},{"location":"tutorials/ai-models/gradient_boosting_classifier/#step-5-create-a-notebook-in-python","title":"Step 5 Create a Notebook in Python\u00b6","text":"<p>Now you'll learn how to create a gradient boosting model in Python. In the same project that you created previously, Create a Jupyter Notebook. Make sure to create a Jupyter Notebook using Python 3.11 in Watson Studio. Make sure to select \"Runtime 24.1 on Python 3.11 XXS (1 vCPU 4 GB RAM)\" when you create the notebook. You're now ready to create a Gradient Boosting Classifier using Python.</p>"},{"location":"tutorials/ai-models/gradient_boosting_classifier/#step-6-configure-libraries-and-data","title":"Step 6 Configure Libraries and Data\u00b6","text":"<p>This step install the libraries that you'll use to train and test your Gradient Boosting Classifier. The training itself is done with scikit-learn and the data comes from the palmerpenguins library.</p>"},{"location":"tutorials/ai-models/gradient_boosting_classifier/#step-7-train-and-test","title":"Step 7 Train and Test\u00b6","text":"<p>Now that you've created the feature sets and the prepocessor, you can create a pipeline to train the model. Other parameters you can configure are max_features, which sets the number of features to consider when looking for the best split. Also the criterion parameter, which measures the quality of a split for training. In this case we\u2019re using the mean squared error with improvement score by Friedman</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/","title":"Implement agentic chunking to optimize LLM inputs with Langchain and watsonx.ai","text":"In\u00a0[1]: Copied! <pre># installations\n!pip install -q langchain langchain-ibm langchain_experimental langchain-text-splitters langchain_chroma transformers bs4 langchain_huggingface sentence-transformers\n</pre> # installations !pip install -q langchain langchain-ibm langchain_experimental langchain-text-splitters langchain_chroma transformers bs4 langchain_huggingface sentence-transformers In\u00a0[\u00a0]: Copied! <pre>import getpass\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain_ibm import WatsonxLLM\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.schema import SystemMessage, HumanMessage\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.vectorstores import Chroma\nfrom langchain.tools import tool\nfrom langchain.agents import AgentExecutor\nfrom langchain.memory import ConversationBufferMemory\nfrom transformers import AutoTokenizer\nfrom ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.schema import Document\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n</pre> import getpass import requests from bs4 import BeautifulSoup from langchain_ibm import WatsonxLLM from langchain_huggingface import HuggingFaceEmbeddings from langchain_community.document_loaders import WebBaseLoader from langchain.schema import SystemMessage, HumanMessage from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain.prompts import ChatPromptTemplate from langchain.vectorstores import Chroma from langchain.tools import tool from langchain.agents import AgentExecutor from langchain.memory import ConversationBufferMemory from transformers import AutoTokenizer from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams from langchain.output_parsers import CommaSeparatedListOutputParser from langchain.embeddings import HuggingFaceEmbeddings from langchain.schema import Document from langchain.chains.combine_documents import create_stuff_documents_chain  <p>To set our credentials, we need the <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> you generated in step 1. We will also set the URL serving as the API endpoint.</p> In\u00a0[4]: Copied! <pre>WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://us-south.ml.cloud.ibm.com\"\n</pre> WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://us-south.ml.cloud.ibm.com\" In\u00a0[\u00a0]: Copied! <pre>llm = WatsonxLLM(\n    model_id= \"ibm/granite-3-8b-instruct\", \n    url=URL,\n    apikey=WATSONX_APIKEY,\n    project_id=WATSONX_PROJECT_ID,\n    params={\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.TEMPERATURE: 0,\n        GenParams.MIN_NEW_TOKENS: 5,\n        GenParams.MAX_NEW_TOKENS: 2000,\n        GenParams.REPETITION_PENALTY:1.2,\n        GenParams.STOP_SEQUENCES: [\"\\n\\n\"]\n    }\n)\n</pre> llm = WatsonxLLM(     model_id= \"ibm/granite-3-8b-instruct\",      url=URL,     apikey=WATSONX_APIKEY,     project_id=WATSONX_PROJECT_ID,     params={         GenParams.DECODING_METHOD: \"greedy\",         GenParams.TEMPERATURE: 0,         GenParams.MIN_NEW_TOKENS: 5,         GenParams.MAX_NEW_TOKENS: 2000,         GenParams.REPETITION_PENALTY:1.2,         GenParams.STOP_SEQUENCES: [\"\\n\\n\"]     } ) In\u00a0[10]: Copied! <pre># Step 2: Function to extract text from a web page\ndef get_text_from_url(url):\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to fetch the page, status code: {response.status_code}\")\n    \n    soup = BeautifulSoup(response.text, \"html.parser\")\n    \n    # Remove unwanted elements (scripts, styles)\n    for script in soup([\"script\", \"style\"]):\n        script.decompose()\n    \n    return soup.get_text(separator=\"\\n\", strip=True)\n</pre> # Step 2: Function to extract text from a web page def get_text_from_url(url):     response = requests.get(url)     if response.status_code != 200:         raise ValueError(f\"Failed to fetch the page, status code: {response.status_code}\")          soup = BeautifulSoup(response.text, \"html.parser\")          # Remove unwanted elements (scripts, styles)     for script in soup([\"script\", \"style\"]):         script.decompose()          return soup.get_text(separator=\"\\n\", strip=True) In\u00a0[\u00a0]: Copied! <pre>url = \"https://www.ibm.com/think/topics/machine-learning\"  # Example URL\nweb_text = get_text_from_url(url)  # Fetch and clean text from URL\nweb_text\n</pre> url = \"https://www.ibm.com/think/topics/machine-learning\"  # Example URL web_text = get_text_from_url(url)  # Fetch and clean text from URL web_text In\u00a0[13]: Copied! <pre># Step 3: Agentic Chunking Function\ndef agentic_chunking(text):\n    \"\"\"\n    Dynamically splits text into meaningful chunks using LLM.\n    \"\"\"\n    system_message = SystemMessage(content=\"You are an AI assistant helping to split text into meaningful chunks based on topics.\")\n    human_message = HumanMessage(content=f\"Please divide the following text into semantically different, separate and meaningful chunks:\\n\\n{text}\")\n\n    response = llm.invoke([system_message, human_message])  # LLM returns a string\n    return response.split(\"\\n\\n\")  # Split based on meaningful sections\n</pre> # Step 3: Agentic Chunking Function def agentic_chunking(text):     \"\"\"     Dynamically splits text into meaningful chunks using LLM.     \"\"\"     system_message = SystemMessage(content=\"You are an AI assistant helping to split text into meaningful chunks based on topics.\")     human_message = HumanMessage(content=f\"Please divide the following text into semantically different, separate and meaningful chunks:\\n\\n{text}\")      response = llm.invoke([system_message, human_message])  # LLM returns a string     return response.split(\"\\n\\n\")  # Split based on meaningful sections In\u00a0[\u00a0]: Copied! <pre>###Step 7- Calling the agentic Chunking defined above for Sample Text\nchunks = agentic_chunking(web_text)\nchunks\n</pre> ###Step 7- Calling the agentic Chunking defined above for Sample Text chunks = agentic_chunking(web_text) chunks In\u00a0[\u00a0]: Copied! <pre>### Lets print out the chunks for better understanding of their output structure. \nfor i, chunk in enumerate(chunks,1):\n    print(f\"Chunk {i}:\\n{chunk}\\n{'-'*40}\")\n</pre> ### Lets print out the chunks for better understanding of their output structure.  for i, chunk in enumerate(chunks,1):     print(f\"Chunk {i}:\\n{chunk}\\n{'-'*40}\") In\u00a0[\u00a0]: Copied! <pre>from langchain_core.documents import Document\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n# Initialize the embedding model\nembeddings_model = HuggingFaceEmbeddings(model_name=\"ibm-granite/granite-embedding-30m-english\")\n\n# Create a Chroma vector database\nvector_db = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings_model\n)\n\n# Convert each text chunk into a Document object\ndocuments = [Document(page_content=chunk) for chunk in chunks]\n\n# Add the documents to the vector database\nvector_db.add_documents(documents)\n</pre> from langchain_core.documents import Document from langchain.vectorstores import Chroma from langchain.embeddings import HuggingFaceEmbeddings  # Initialize the embedding model embeddings_model = HuggingFaceEmbeddings(model_name=\"ibm-granite/granite-embedding-30m-english\")  # Create a Chroma vector database vector_db = Chroma(     collection_name=\"example_collection\",     embedding_function=embeddings_model )  # Convert each text chunk into a Document object documents = [Document(page_content=chunk) for chunk in chunks]  # Add the documents to the vector database vector_db.add_documents(documents)  In\u00a0[19]: Copied! <pre>from langchain.chains import create_retrieval_chain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\nprompt_template = \"\"\"&lt;|start_of_role|&gt;user&lt;|end_of_role|&gt;Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {input}&lt;|end_of_text|&gt;\n&lt;|start_of_role|&gt;assistant&lt;|end_of_role|&gt;\"\"\"\n\nqa_chain_prompt = PromptTemplate.from_template(prompt_template)\ncombine_docs_chain = create_stuff_documents_chain(llm, qa_chain_prompt)\nrag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)\n</pre> from langchain.chains import create_retrieval_chain from langchain.prompts import PromptTemplate from langchain.chains.combine_documents import create_stuff_documents_chain  prompt_template = \"\"\"&lt;|start_of_role|&gt;user&lt;|end_of_role|&gt;Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.  {context}  Question: {input}&lt;|end_of_text|&gt; &lt;|start_of_role|&gt;assistant&lt;|end_of_role|&gt;\"\"\"  qa_chain_prompt = PromptTemplate.from_template(prompt_template) combine_docs_chain = create_stuff_documents_chain(llm, qa_chain_prompt) rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain) In\u00a0[\u00a0]: Copied! <pre>output = llm.invoke(\"What is Model optimization process\")\noutput\n</pre> output = llm.invoke(\"What is Model optimization process\") output <p>Clearly, the model was not trained on information about Model optimization process and without outside tools or information, it cannot provide us with the correct information. The model hallucinates. Now, let's try providing the same query to the RAG chain with Agentic Chunks we built.</p> In\u00a0[\u00a0]: Copied! <pre>rag_output = rag_chain.invoke({\"input\": \"What is Model optimization process?\"})\nrag_output['answer']\n</pre> rag_output = rag_chain.invoke({\"input\": \"What is Model optimization process?\"}) rag_output['answer'] <p>Great! The Granite model correctly used the Agentic RAG chunks as context to tell us correct information about Model optimization process while preserving semantic coherence.</p> <p>Summary</p> <p>In this tutorial, we generated smaller pieces of relevant information using AI agents in the chunking process and constructed a retrieval-augmented generation (RAG) pipeline.</p> <p>This method improves information retrieval and context window optimization using artificial intelligence and natural language processing (NLP). It streamlines data chunks to enhance retrieval efficiency when leveraging large language models (LLMs) like OpenAI's GPT models for better results.</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#implement-agentic-chunking-to-optimize-llm-inputs-with-langchain-and-watsonxai","title":"Implement agentic chunking to optimize LLM inputs with Langchain and watsonx.ai\u00b6","text":"<p>Author: Shalini Harkar</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#what-is-agentic-chunking","title":"What is Agentic Chunking?\u00b6","text":"<p>The way language models process and segment text is changing from the traditional static approach, to a better, more responsive process. Unlike traditional fixed-size chunking , which chunks large documents at fixed points, agentic chunking employs AI-based techniques to analyze content in a dynamic process, and to determine the best way to segment the text.</p> <p>Agentic chunking makes use of AI-based text-splitting methods, recursive chunking, and chunk overlap methods, which work concurrently to polish chunking ability, preserving links between notable ideas while optimizing contextual windows in real time. With agentic chunking, each chunk is enriched with metadata to deepen retrieval accuracy and overall model efficiency. This is particularly important in RAG applications applications , where segmentation of data can directly impact retrieval quality and coherence of the response. Meaningful context is preserved in all chunks, making this approach incredibly important to chatbots, knowledge bases, and generative ai use cases..</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#key-elements-of-agentic-chunking","title":"Key Elements of Agentic Chunking\u00b6","text":"<ol> <li><p>Adaptive Chunking Strategy \u2013 Dynamically choosing the best chunking method based on the type of content, the intent behind the query, and the needs for retrieval to ensure effective segmentation.</p> </li> <li><p>Dynamic Chunk Sizing \u2013 Modifying chunk sizes in real time by taking into account the semantic structure and context, instead of sticking to fixed token limits.</p> </li> <li><p>Context-Preserving Overlap \u2013 Smartly assessing the overlap between chunks to keep coherence intact and avoid losing essential information, thereby enhancing retrieval efficiency.</p> </li> </ol>"},{"location":"tutorials/generative-ai/Agentic-chunking/#advantages-of-agentic-chunking-over-traditional-methods","title":"Advantages of Agentic Chunking Over Traditional Methods\u00b6","text":"<ol> <li><p>Retains Context \u2013 Maintains crucial information without unnecessary breaks.</p> </li> <li><p>Smart Sizing \u2013 Adjusts chunk boundaries according to meaning and significance.</p> </li> <li><p>Query-Optimized \u2013 Continuously refines chunks to match specific queries.</p> </li> <li><p>Efficient Retrieval \u2013 Improves search and RAG by minimizing unnecessary fragmentation.</p> </li> </ol> <p>In this tutorial, you will experiment with agentic chunking strategy by using the IBM Granite-3.0-8B-Instruct model now available on watsonx.ai\u00ae. The overall goal is to perform efficient chunking to effectively implement RAG.</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#prerequisite","title":"Prerequisite\u00b6","text":"<p>You need an IBM Cloud account\u00ae to create a watsonx.ai project.</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/Agentic-chunking/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud\u00ae account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> <p>This step opens a Jupyter Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. ss</p> </li> </ol>"},{"location":"tutorials/generative-ai/Agentic-chunking/#step-2-set-up-a-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up a watsonx.ai Runtime instance and API key.\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (select your appropriate region and choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an application programming interface (API) key.</p> </li> <li><p>Associate the watsonx.ai Runtime service instance to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/Agentic-chunking/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We need a few libraries and modules for this tutorial. Make sure to import the following ones and if they're not installed, a quick pip installation resolves the problem.</p> <p>Note, this tutorial was built using Python 3.12.7.</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#step-4-initialize-your-llm","title":"Step 4. Initialize your LLM\u00b6","text":"<p>We will use Granite 3.1 as our LLM for this tutorial. To initialize the LLM, we need to set the model parameters. To learn more about these model parameters, such as the minimum and maximum token limits, refer to the documentation.</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#step-5-load-your-document","title":"Step 5. Load your document\u00b6","text":"<p>This function extracts the  text content from IBM's explainer page on Machine learning. This function removes unwanted HTML elements (scripts, styles), and returns clean, readable text.</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#lets-provide-sample-code-for-implementing-agentic-chunking-using-langchain-and-granite-model","title":"Let's provide sample code for implementing Agentic Chunking using  LangChain and granite model.\u00b6","text":""},{"location":"tutorials/generative-ai/Agentic-chunking/#step-6-instead-of-using-a-fixed-length-chunking-method-we-used-agentic-chunking-here","title":"Step 6 . Instead of using a fixed-length chunking method, we used Agentic Chunking here.\u00b6","text":""},{"location":"tutorials/generative-ai/Agentic-chunking/#great-as-you-can-see-in-the-output-the-chunks-are-successfully-created-by-the-agents","title":"Great! As you can see in the output, the chunks are successfully created by the agents.\u00b6","text":""},{"location":"tutorials/generative-ai/Agentic-chunking/#step-8-create-vector-store","title":"Step 8. Create vector store\u00b6","text":"<p>Now that we have experimented with Agentic  chunking on the given text , let's move along with our RAG implementation. For this tutorial, we will choose the chunks produced by the Agents and convert them to vector embeddings. An open source vector store we can use is Chroma DB. We can easily access Chroma functionality through the langchain_chroma package.</p> <p>Let's initialize our Chroma vector database, provide it with our embeddings model and add our documents produced by Agentic chunking.</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#step-9-structure-the-prompt-template","title":"Step 9. Structure the prompt template\u00b6","text":"<p>Now, we can create a prompt template for our LLM. This template ensures that we can ask multiple questions while maintaining a consistent prompt structure. Additionally, we can integrate our vector store as the retriever, finalizing the RAG framework.</p>"},{"location":"tutorials/generative-ai/Agentic-chunking/#step-10-prompt-the-rag-chain","title":"Step 10. Prompt the RAG chain\u00b6","text":"<p>Using our Agentic Chunks in the RAG workflow, let's invoke a user query. First, we can strategically prompt the model without any additional context from the vector store we built to test whether the model is using its built-in knowledge or truly using the RAG context. Using  Machine learning explainer from IBM's   Let's ask the LLM about Docling.</p>"},{"location":"tutorials/generative-ai/agentic-rag/","title":"Build a LangChain agentic RAG system using the Granite-3.0-8B-Instruct model in watsonx.ai","text":"In\u00a0[\u00a0]: Copied! <pre># installations\n%pip install langchain | tail -n 1\n%pip install langchain-ibm | tail -n 1\n%pip install langchain-community | tail -n 1\n%pip install ibm-watsonx-ai | tail -n 1\n%pip install chromadb | tail -n 1\n%pip install tiktoken | tail -n 1\n%pip install bs4 | tail -n 1\n</pre> # installations %pip install langchain | tail -n 1 %pip install langchain-ibm | tail -n 1 %pip install langchain-community | tail -n 1 %pip install ibm-watsonx-ai | tail -n 1 %pip install chromadb | tail -n 1 %pip install tiktoken | tail -n 1 %pip install bs4 | tail -n 1 In\u00a0[\u00a0]: Copied! <pre># imports\nimport getpass\n\nfrom langchain_ibm import WatsonxEmbeddings, WatsonxLLM\nfrom langchain.vectorstores import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.prompts import PromptTemplate\nfrom langchain.tools import tool\nfrom langchain.tools.render import render_text_description_and_args\nfrom langchain.agents.output_parsers import JSONAgentOutputParser\nfrom langchain.agents.format_scratchpad import format_log_to_str\nfrom langchain.agents import AgentExecutor\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_core.runnables import RunnablePassthrough\nfrom ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n</pre> # imports import getpass  from langchain_ibm import WatsonxEmbeddings, WatsonxLLM from langchain.vectorstores import Chroma from langchain_community.document_loaders import WebBaseLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain.prompts import PromptTemplate from langchain.tools import tool from langchain.tools.render import render_text_description_and_args from langchain.agents.output_parsers import JSONAgentOutputParser from langchain.agents.format_scratchpad import format_log_to_str from langchain.agents import AgentExecutor from langchain.memory import ConversationBufferMemory from langchain_core.runnables import RunnablePassthrough from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams <p>Input your <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> that you created in steps 1 and 2 upon running the following cell.</p> In\u00a0[\u00a0]: Copied! <pre>credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n}\n\nproject_id = getpass.getpass(\"Please enter your project ID (hit enter): \")\n</pre> credentials = {     \"url\": \"https://us-south.ml.cloud.ibm.com\",     \"apikey\": getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \") }  project_id = getpass.getpass(\"Please enter your project ID (hit enter): \") In\u00a0[\u00a0]: Copied! <pre>llm = WatsonxLLM(\n    model_id= \"ibm/granite-3-8b-instruct\", \n    url=credentials.get(\"url\"),\n    apikey=credentials.get(\"apikey\"),\n    project_id=project_id,\n    params={\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.TEMPERATURE: 0,\n        GenParams.MIN_NEW_TOKENS: 5,\n        GenParams.MAX_NEW_TOKENS: 250,\n        GenParams.STOP_SEQUENCES: [\"Human:\", \"Observation\"],\n    },\n)\n</pre> llm = WatsonxLLM(     model_id= \"ibm/granite-3-8b-instruct\",      url=credentials.get(\"url\"),     apikey=credentials.get(\"apikey\"),     project_id=project_id,     params={         GenParams.DECODING_METHOD: \"greedy\",         GenParams.TEMPERATURE: 0,         GenParams.MIN_NEW_TOKENS: 5,         GenParams.MAX_NEW_TOKENS: 250,         GenParams.STOP_SEQUENCES: [\"Human:\", \"Observation\"],     }, ) <p>We'll set up a prompt template in case you want to ask multiple questions.</p> In\u00a0[\u00a0]: Copied! <pre>template = \"Answer the {query} accurately. If you do not know the answer, simply say you do not know.\"\nprompt = PromptTemplate.from_template(template)\n</pre> template = \"Answer the {query} accurately. If you do not know the answer, simply say you do not know.\" prompt = PromptTemplate.from_template(template) <p>And now we can set up a chain with our prompt and our LLM. This allows the generative model to produce a response.</p> In\u00a0[\u00a0]: Copied! <pre>agent = prompt | llm\n</pre> agent = prompt | llm <p>Let's test to see how our agent responds to a basic query.</p> In\u00a0[\u00a0]: Copied! <pre>agent.invoke({\"query\": \"What sport is played at the US Open?\"})\n</pre> agent.invoke({\"query\": \"What sport is played at the US Open?\"}) Out[\u00a0]: <pre>'\\n\\nThe sport played at the US Open is tennis.'</pre> <p>The agent successfully responded to the basic query with the correct answer. In the next step of this tutorial, we will be creating a RAG tool for the agent to access relevant information about IBM's involvement in the 2024 US Open. As we have covered, traditional LLMs cannot obtain current information on their own. Let's verify this.</p> In\u00a0[\u00a0]: Copied! <pre>agent.invoke({\"query\": \"Where was the 2024 US Open Tennis Championship?\"})\n</pre> agent.invoke({\"query\": \"Where was the 2024 US Open Tennis Championship?\"}) Out[\u00a0]: <pre>' Do not make up an answer.\\n\\nThe 2024 US Open Tennis Championship has not been officially announced yet, so the location is not confirmed. Therefore, I do not know the answer to this question.'</pre> <p>Evidently, the LLM is unable to provide us with the relevant information. The training data used for this model contained information prior to the 2024 US Open and without the appropriate tools, the agent does not have access to this information.</p> In\u00a0[\u00a0]: Copied! <pre>urls = [\n    \"https://www.ibm.com/case-studies/us-open\",\n    \"https://www.ibm.com/sports/usopen\",\n    \"https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement\",\n    \"https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms\",\n]\n</pre> urls = [     \"https://www.ibm.com/case-studies/us-open\",     \"https://www.ibm.com/sports/usopen\",     \"https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement\",     \"https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms\", ] <p>Next, load the documents using LangChain <code>WebBaseLoader</code> for the URLs we listed. We'll also print a sample document to see how it loaded.</p> In\u00a0[\u00a0]: Copied! <pre>docs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\ndocs_list[0]\n</pre> docs = [WebBaseLoader(url).load() for url in urls] docs_list = [item for sublist in docs for item in sublist] docs_list[0] Out[\u00a0]: <pre>Document(metadata={'source': 'https://www.ibm.com/case-studies/us-open', 'title': 'U.S. Open | IBM', 'description': 'To help the US Open stay on the cutting edge of customer experience, IBM Consulting built powerful generative AI models with watsonx.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nU.S. Open | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\nUS Open \\n\\n\\n\\n                    \\n\\n\\n\\n  \\n    Acing the US Open digital experience\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                \\n\\n                        \\n\\n\\n  \\n  \\n      AI models built with watsonx transform data into insight\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                    \\n\\n\\nGet the latest AI and tech insights\\n\\n\\nLearn More\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFor two weeks at the end of summer, nearly one million people make the journey to Flushing, New York, to watch the best tennis players in the world compete in the US Open Tennis Championships.\\nYear after year, it is one of the most highly attended sporting events in the world.\\n\\nBut more than 15 million global tennis fans follow the tournament through the US Open app and website. And to keep them coming back for more, the United States Tennis Association (USTA) has worked side-by-side with IBM Consulting\u00ae for more than three decades, developing and delivering a world-class digital experience that constantly advances its features and functionality.\\n\\n\u201cThe digital experience of the US Open is of enormous importance to our global fans, and therefore to us,\u201d says Kirsten Corio, Chief Commercial Officer at the USTA. \u201cThat means we need to constantly innovate to meet the modern demands of tennis fans, anticipating their needs, but also surprising them with new and unexpected experiences.\u201d\\nTo help the US Open stay on the cutting edge of customer experience, IBM Consulting worked closely with the USTA to develop generative AI models that transform tennis data into insights and original content on the US Open app and website. To do this, the USTA used IBM\u00ae watsonx\u2122, an AI and data platform built for business, and powerful AI models, including IBM Granite\u2122 foundation models, to help develop key app features, such as Match Reports and AI Commentary for US Open highlight reels.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n      15M\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\nWorld-class digital experiences for more than 15 million fans around the globe\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n      7M\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\nIBM captures and analyzes more than 7 million data points throughout the tournament\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n            \\n                \\n                     \\n                        \\n\\n\\n  \\n  \\n      The AI models built with watsonx do more than enhance the digital experience of the US Open. They also scale the productivity of our editorial team by automating key workflows.\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                        \\n                \\n            \\n        \\n        \\n            Kirsten Corio\\n        \\n\\n            Chief Commercial Officer\\n        \\n\\n            United States Tennis Association\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n    Generative AI experiences, built with watsonx\\n\\n\\n\\n\\n\\n    \\n\\n\\n            \\n        \\n\\n\\n\\n\\nThe US Open is a sprawling, two-week tournament, with hundreds of matches played on 22 different courts. Keeping up with all the action is a challenge, both for tennis fans and the USTA editorial team covering the event. So, the USTA asked IBM to design, develop, and deliver solutions that enhance the digital experience and help its team serve up more content, covering more matches throughout the tournament.\\nTo do it, the IBM Consulting team built generative AI-powered features using the watsonx AI and data platform. For example, Match Reports are AI-generated post-match summaries that are designed to get fans quickly up to speed on the action from around the tournament. AI Commentary adds AI-generated, spoken commentary to match highlights. And SlamTracker\u2013the premier scoring application for the US Open\u2013features AI-generated match previews and recaps.\\n\u201cThe AI models built with watsonx do more than enhance the digital experience of the US Open,\u201d says Kirsten Corio, Chief Commercial Officer, USTA. \u201cThey also scale the productivity of our editorial team by automating key workflows.\u201d\\nThe IBM team worked with multiple AI models to develop the new features, including the family of Granit AI models. These large language models already understand language, but they needed to be trained, or \u201ctuned,\u201d on tennis data in order to translate US Open action into sentences and summaries.\\n\u201cFoundation models are incredibly powerful and are ushering in a new age of generative AI,\u201d says Shannon Miller, a Partner at IBM Consulting. \u201cBut to generate meaningful business outcomes, they need to be trained on high-quality data and develop domain expertise. And that\u2019s why an organization\u2019s proprietary data is the key differentiator when it comes to AI.\u201d\\nThe team used watsonx.data to connect and curate the USTA\u2019s trusted data sources. The curation process includes de-duping and filtering the foundational data that informs the large language model, as well as the USTA\u2019s proprietary data. The process filters for things like profanity or abusive language and objectionable content.\\nThe models were then trained to translate tennis data into cogent descriptions, summarizing entire matches in the case of Match Reports, or generating sentences that describe the action in highlight reels for AI Commentary. Over the course of the 2024 US Open, Match Reports and AI Commentary will be generated for all men\u2019s and women\u2019s singles matches; something the USTA editorial team has never done before. And the ongoing operation of the models is monitored and managed using elements of watsonx.governance, which ensures the AI is performant, compliant and operating as expected.\\nDuring the software development phase of the project, the team took advantage of a powerful generative AI assistant to increase the efficiency and accuracy of its code. IBM watsonx Code Assistant\u2122 uses generative AI from a purpose-built Granite model to accelerate software development, helping developers generate code based on natural language prompts. The team used this tool to analyze and explain snippets of code, annotate code to facilitate better collaboration between developers, and auto-complete snippets of analyzed code.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n\\n\\n  \\n\\n\\n\\n    Platform of innovation\\n\\n\\n\\n\\n\\n    \\n\\n\\n            \\n        \\n\\n\\n\\n\\nTo develop new capabilities every year, the USTA needs to move with speed and purpose. The process starts the week after the US Open concludes, when IBM Consulting kicks off work using the IBM Garage\u2122 Methodology, a highly collaborative approach to co-creation.\\n\u201cWhen we engage with a client, it\u2019s critical that we work closely together every step of the way, ideating, iterating and adapting as we drive toward the client\u2019s desired end state,\u201d says Miller.\\nIn order to transform new ideas into digital reality, IBM Consulting designs, develops, and manages a powerful digital infrastructure capable of processing structured and unstructured data, and integrating technology from a variety of sources. This foundational infrastructure is advanced and improved upon every year.\\n\u201cIt used to be that innovation cycles were measured in years,\u201d says the USTA\u2019s Corio. \u201cBut now, innovation is measured in weeks and days, and it can come from anywhere. So, we needed a flexible platform that could handle all kinds of data, automate the process of turning data into insight, and do it all while protecting the entire digital environment.\u201d\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n\\n\\n  \\n\\n\\n\\n    From data to insight\\n\\n\\n\\n\\n\\n    \\n\\n\\n            \\n        \\n\\n\\n\\n\\nThe raw material of any digital experience is data, and the US Open tournament produces a lot of it. For starters, each US Open consists of 128 men and 128 women singles players, and a total of seven rounds for each tournament. Each tennis player comes with his or her own data set, including world ranking and recent performance. But that\u2019s just the beginning.\\nOver the course of the tournament, more than 125,000 points will be played. And each one of those points generates its own data set: serve direction, speed, return shot type, winner shot type, rally count and even ball position. All told, more than seven million data points are generated during the tournament.\\nTo add more texture and context to the US Open digital experience, the team wanted to go beyond the numbers. So, they used AI to analyze the language and sentiment of millions of articles from hundreds of thousands of different sources to develop insights that are unique and informative, for instance, the likelihood to Win predictions for all singles matches. To help manage the collection, integration, and analysis of the data sets, IBM used IBM watsonx.data\u2122, a purpose-built data store specifically designed to handle AI workloads.\\n\u201cIt\u2019s a massive data management operation, incorporating multiple sources of data and a variety of partners,\u201d says Miller. \u201cBut the magic happens when you combine hard data like stats and scores with unstructured data like media commentary. That is what gives tennis fans a more complete picture of each match.\u201d\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n    Automation, containerization and other efficiencies\\n\\n\\n\\n\\n\\n    \\n\\n\\n            \\n        \\n\\n\\n\\n\\nTo streamline this process, during the years working with the UTSA, IBM Consulting built automated workflows that integrate and orchestrate the flow of data through the various applications and AI models needed to produce the digital experience. These workflows are made possible by a hybrid cloud architecture and the containerized apps running on Red Hat\u00ae\\xa0OpenShift\u00ae\\xa0on IBM Cloud.\\xa0The US Open hybrid multicloud architecture is made up of four public clouds, drawing on data from a variety of sources and integrating features and capability from a variety of partners. By containerizing the applications, the team can write them once and run them anywhere, ensuring that the right data gets to the right application on the right cloud.\\n\\n\u201cWith this platform, we\u2019re capable of doing things that were not possible just a few years ago,\u201d says Corio. \u201cManaging all that data, producing AI-generated insights, securing the environment \u2026 IBM just makes it all come together for us. And I can\u2019t wait to see what the future of the partnership holds.\u201d\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n            \\n                \\n                     \\n                        \\n\\n\\n  \\n  \\n      Managing all that data, producing AI-generated insights, securing the environment\u2026IBM just makes it all come together for us. And I can\u2019t wait to see what the future of the partnership holds.\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                        \\n                \\n            \\n        \\n        \\n            Kirsten Corio\\n        \\n\\n            Chief Commercial Officer\\n        \\n\\n            United States Tennis Association\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n\\n\\n\\n    About United States Tennis Association (USTA)\\n\\n\\n\\n\\n\\n    \\n\\n\\n            \\n        \\n\\n\\n\\n\\nFounded in 1881, the USTA (link resides outside of\\xa0ibm.com) is the national governing body for the sport of tennis in the US. The US Open (link resides outside of\\xa0ibm.com) is the association\u2019s Grand Slam tournament, first held in 1968\u2014the year that Arthur Ashe won the men\u2019s singles title. The US Open is played each September at the USTA Billie Jean King National Tennis Center in Flushing, Queens, New York.\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n  \\n  \\n      Solution components\\n  \\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n                                IBM Consulting\u2122\\n                                \\n                                \\n        \\n\\n                                IBM Garage\u2122 Methodology\\n                                \\n                                \\n        \\n\\n                                IBM\u00ae Granite\u2122\\n                                \\n                                \\n        \\n\\n                                IBM watsonx\u2122\\n                                \\n                                \\n        \\n\\n                                IBM watsonx.data\u2122\\n                                \\n                                \\n        \\n\\n                                IBM watsonx Code Assistant\u2122\\n                                \\n                                \\n        \\n\\n                                Red Hat\u00ae OpenShift\u00ae on IBM Cloud\\n                                \\n                                \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIBM watsonx\\n\\n\\n\\n\\nWant AI to make smart use of all your data? Use IBM watsonx to accelerate the fine tuning and deployment of your models.\\n\\n\\n\\n\\n\\nLearn more about watsonx\\n\\n\\nView more case stories\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    The Masters\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nWhat if the Masters could turn data into insight?\\n\\n\\n\\nRead the case study\\n            \\n        \\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Wimbledon\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nIBM and Wimbledon, a partnership of innovation\\n\\n\\n\\nRead the case study\\n            \\n        \\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Wintershall Dea AG\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nDrilling down into data to transform the oil and gas industry\\n\\n\\n\\nRead the case study\\n            \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            \\n\\n\\n\\n  \\n    Legal\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                        \\n\\n\\n\\n\\n\\n\u00a9 Copyright IBM Corporation 2023. IBM Corporation, IBM Consulting, New Orchard Road, Armonk, NY 10504\\nProduced in the United States of America, August 2024.\\nIBM, the IBM logo, ibm.com, IBM Consulting, IBM Garage, Granite, watsonx, watsonx.data, and Code Assistant are trademarks or registered trademarks of International Business Machines Corporation, in the United States and/or other countries. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on ibm.com/trademark.\\n\\nRed Hat\u00ae and OpenShift\u00ae are registered trademarks of Red Hat, Inc. or its subsidiaries in the United States and other countries.\\xa0\\n\\nThis document is current as of the initial date of publication and may be changed by IBM at any time. Not all offerings are available in every country in which IBM operates.\\nAll client examples cited or described are presented as illustrations of the manner in which some clients have used IBM products and the results they may have achieved. Actual environmental costs and performance characteristics will vary depending on individual client configurations and conditions. Generally expected results cannot be provided as each client\\'s results will depend entirely on the client\\'s systems and services ordered. THE INFORMATION IN THIS DOCUMENT IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY, EXPRESS OR IMPLIED, INCLUDING WITHOUT ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OR CONDITION OF NON-INFRINGEMENT. IBM products are warranted according to the terms and conditions of the agreements under which they are provided.\\nStatement of Good Security Practices: No IT system or product should be considered completely secure, and no single product, service or security measure can be completely effective in preventing improper use or access.\\xa0 IBM does not warrant that any systems, products or services are immune from, or will make your enterprise immune from, the malicious or illegal conduct of any party.\\nThe client is responsible for ensuring compliance with all applicable laws and regulations. IBM does not provide legal advice nor represent or warrant that its services or products will ensure that the client is compliant with any law or regulation.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')</pre> <p>In order to split the data in these documents to chunks that can be processed by the LLM, we can use a text splitter such as <code>RecursiveCharacterTextSplitter</code>. This text splitter splits the content on the following characters: [\"\\n\\n\", \"\\n\", \" \", \"\"]. This is done with the intention of keeping text in the same chunks, such as paragraphs, sentences and words together.</p> <p>Once the text splitter is initiated, we can apply it to our <code>docs_list</code>.</p> In\u00a0[\u00a0]: Copied! <pre>text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=250, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n</pre> text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(     chunk_size=250, chunk_overlap=0 ) doc_splits = text_splitter.split_documents(docs_list) <p>The embedding model that we are using is an IBM Slate\u2122 model through the watsonx.ai embeddings service. Let's initialize it.</p> In\u00a0[\u00a0]: Copied! <pre>embeddings = WatsonxEmbeddings(\n    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n    url=credentials[\"url\"],\n    apikey=credentials[\"apikey\"],\n    project_id=project_id,\n)\n</pre> embeddings = WatsonxEmbeddings(     model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,     url=credentials[\"url\"],     apikey=credentials[\"apikey\"],     project_id=project_id, ) <p>In order to store our embedded documents, we will use Chroma DB, an open source vector store.</p> In\u00a0[\u00a0]: Copied! <pre>vectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"agentic-rag-chroma\",\n    embedding=embeddings,\n)\n</pre> vectorstore = Chroma.from_documents(     documents=doc_splits,     collection_name=\"agentic-rag-chroma\",     embedding=embeddings, ) <p>To access information in the vector store, we must set up a retriever.</p> In\u00a0[\u00a0]: Copied! <pre>retriever = vectorstore.as_retriever()\n</pre> retriever = vectorstore.as_retriever() In\u00a0[\u00a0]: Copied! <pre>@tool\ndef get_IBM_US_Open_context(question: str):\n    \"\"\"Get context about IBM's involvement in the 2024 US Open Tennis Championship.\"\"\"\n    context = retriever.invoke(question)\n    return context\n\n\ntools = [get_IBM_US_Open_context]\n</pre> @tool def get_IBM_US_Open_context(question: str):     \"\"\"Get context about IBM's involvement in the 2024 US Open Tennis Championship.\"\"\"     context = retriever.invoke(question)     return context   tools = [get_IBM_US_Open_context] In\u00a0[\u00a0]: Copied! <pre>system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools}\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\nValid \"action\" values: \"Final Answer\" or {tool_names}\nProvide only ONE action per $JSON_BLOB, as shown:\"\n```\n{{\n  \"action\": $TOOL_NAME,\n  \"action_input\": $INPUT\n}}\n```\nFollow this format:\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n```\n$JSON_BLOB\n```\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction:\n```\n{{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Final response to human\"\n}}\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action.\nRespond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\"\n</pre> system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools} Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input). Valid \"action\" values: \"Final Answer\" or {tool_names} Provide only ONE action per $JSON_BLOB, as shown:\" ``` {{   \"action\": $TOOL_NAME,   \"action_input\": $INPUT }} ``` Follow this format: Question: input question to answer Thought: consider previous and subsequent steps Action: ``` $JSON_BLOB ``` Observation: action result ... (repeat Thought/Action/Observation N times) Thought: I know what to respond Action: ``` {{   \"action\": \"Final Answer\",   \"action_input\": \"Final response to human\" }} Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\" <p>In the following code, we are establishing the <code>human_prompt</code>. This prompt tells the agent to display the user input followed by the intermediate steps taken by the agent as part of the <code>agent_scratchpad</code>.</p> In\u00a0[\u00a0]: Copied! <pre>human_prompt = \"\"\"{input}\n{agent_scratchpad}\n(reminder to always respond in a JSON blob)\"\"\"\n</pre> human_prompt = \"\"\"{input} {agent_scratchpad} (reminder to always respond in a JSON blob)\"\"\" <p>Next, we establish the order of our newly defined prompts in the prompt template. We create this new template to feature the <code>system_prompt</code> followed by an optional list of messages collected in the agent's memory, if any, and finally, the <code>human_prompt</code> which includes both the human input and <code>agent_scratchpad</code>.</p> In\u00a0[\u00a0]: Copied! <pre>prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(\"chat_history\", optional=True),\n        (\"human\", human_prompt),\n    ]\n)\n</pre> prompt = ChatPromptTemplate.from_messages(     [         (\"system\", system_prompt),         MessagesPlaceholder(\"chat_history\", optional=True),         (\"human\", human_prompt),     ] ) <p>Now, let's finalize our prompt template by adding the tool names, descriptions and arguments using a partial prompt template. This allows the agent to access the information pertaining to each tool including the intended use cases and also means we can add and remove tools without altering our entire prompt template.</p> In\u00a0[\u00a0]: Copied! <pre>prompt = prompt.partial(\n    tools=render_text_description_and_args(list(tools)),\n    tool_names=\", \".join([t.name for t in tools]),\n)\n</pre> prompt = prompt.partial(     tools=render_text_description_and_args(list(tools)),     tool_names=\", \".join([t.name for t in tools]), ) In\u00a0[\u00a0]: Copied! <pre>memory = ConversationBufferMemory()\n</pre> memory = ConversationBufferMemory() <p>And now we can set up a chain with our agent's scratchpad, memory, prompt and the LLM. The AgentExecutor class is used to execute the agent. It takes the agent, its tools, error handling approach, verbose parameter and memory as parameters.</p> In\u00a0[\u00a0]: Copied! <pre>chain = (\n    RunnablePassthrough.assign(\n        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n        chat_history=lambda x: memory.chat_memory.messages,\n    )\n    | prompt\n    | llm\n    | JSONAgentOutputParser()\n)\n\nagent_executor = AgentExecutor(\n    agent=chain, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory\n)\n</pre> chain = (     RunnablePassthrough.assign(         agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),         chat_history=lambda x: memory.chat_memory.messages,     )     | prompt     | llm     | JSONAgentOutputParser() )  agent_executor = AgentExecutor(     agent=chain, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory ) In\u00a0[\u00a0]: Copied! <pre>agent_executor.invoke({\"input\": \"Where was the 2024 US Open Tennis Championship?\"})\n</pre> agent_executor.invoke({\"input\": \"Where was the 2024 US Open Tennis Championship?\"}) <pre>\n\n&gt; Entering new AgentExecutor chain...\n\n\nThought: The human is asking about the location of the 2024 US Open Tennis Championship. I need to find out where it was held.\nAction:\n```\n{\n  \"action\": \"get_IBM_US_Open_context\",\n  \"action_input\": \"Where was the 2024 US Open Tennis Championship held?\"\n}\n```\nObservation[Document(metadata={'description': \"IBM and the United States Tennis Association (USTA) announced several watsonx-powered fan features coming to the US Open digital platforms ahead of this year's tournament. These new and enhanced capabilities \u2013 a product of collaboration between IBM and the USTA digital team \u2013 aim to deliver a more informative and engaging experience for millions of tennis fans around the world.\", 'language': 'en-us', 'source': 'https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms', 'title': 'IBM and the USTA Serve Up New and Enhanced Generative AI Features for 2024 US Open Digital Platforms'}, page_content=\"IBM and the USTA Serve Up New and Enhanced Generative AI Features for 2024 US Open Digital Platforms\\n-New Match Report summaries offer fans detailed analysis for all 254 US Open main draw singles matches-Enhanced version of AI Commentary returns to US Open digital platforms for on-demand highlights, along with fully redesigned IBM SlamTracker experience-IBM and USTA Foundation announce new collaboration to provide AI professional development resources to USTA Foundation students, teaching professionals and the public\\nAug 15, 2024\\n\\n\\n\\n\\n\\n\\nARMONK, N.Y., Aug. 15, 2024 /PRNewswire/ -- IBM (NYSE:\\xa0IBM) and the United States Tennis Association (USTA) today announced several watsonx-powered fan features coming to the US Open digital platforms ahead of this year's tournament. These new and enhanced capabilities \u2013 a product of collaboration between IBM and the USTA digital team \u2013 aim to deliver a more informative and engaging experience for millions of tennis fans around the world.\"), Document(metadata={'description': \"IBM and the United States Tennis Association (USTA) announced several watsonx-powered fan features coming to the US Open digital platforms ahead of this year's tournament. These new and enhanced capabilities \u2013 a product of collaboration between IBM and the USTA digital team \u2013 aim to deliver a more informative and engaging experience for millions of tennis fans around the world.\", 'language': 'en-us', 'source': 'https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms', 'title': 'IBM and the USTA Serve Up New and Enhanced Generative AI Features for 2024 US Open Digital Platforms'}, page_content=\"These new features and efforts showcase how IBM and the USTA continue to co-create world-class digital experiences that bring the drama and excitement of the US Open to more than 15 million people around the world each year. The US Open's digital experiences are run on the USTA's flexible, open hybrid cloud platform, which integrates technology from dozens of partners, automates key business processes, and secures the entire world-class digital experience of the US Open.\\nThe 2024 US Open runs from August 19 - September 8. To see IBM technology in action, visit USOpen.org and/or the US Open app available in the Apple and Android app stores on mobile devices.\\nAbout IBM\"), Document(metadata={'description': 'To help the US Open stay on the cutting edge of customer experience, IBM Consulting built powerful generative AI models with watsonx.', 'language': 'en', 'source': 'https://www.ibm.com/case-studies/us-open', 'title': 'U.S. Open | IBM'}, page_content='Founded in 1881, the USTA (link resides outside of\\xa0ibm.com) is the national governing body for the sport of tennis in the US. The US Open (link resides outside of\\xa0ibm.com) is the association\u2019s Grand Slam tournament, first held in 1968\u2014the year that Arthur Ashe won the men\u2019s singles title. The US Open is played each September at the USTA Billie Jean King National Tennis Center in Flushing, Queens, New York.\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n  \\n  \\n      Solution components'), Document(metadata={'description': \"IBM and the United States Tennis Association (USTA) announced several watsonx-powered fan features coming to the US Open digital platforms ahead of this year's tournament. These new and enhanced capabilities \u2013 a product of collaboration between IBM and the USTA digital team \u2013 aim to deliver a more informative and engaging experience for millions of tennis fans around the world.\", 'language': 'en-us', 'source': 'https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms', 'title': 'IBM and the USTA Serve Up New and Enhanced Generative AI Features for 2024 US Open Digital Platforms'}, page_content=\"Preview of new 2024 IBM Match Report summaries\\nMatch Reports will expand editorial capabilities of the USTA, enabling them to provide fans with timely coverage at unprecedented scale \u2013\\xa0for all 254 main draw singles matches across seven rounds and all seventeen courts.\\nAn enhanced version of IBM's AI commentary is also returning to the US Open digital platforms. First introduced in 2023, AI commentary provides automated English-language audio and subtitles for men's and women's singles match summary highlight videos. This year, IBM will utilize watsonx including the Granite 13B LLM to generate more frequent, expressive and contextual commentary, with highlight packages published just minutes after a match concludes.\\nFans will also have access to a fully redesigned IBM SlamTracker experience offering detailed pre-, live and post-match insights. These include likelihood to win predictions, point-by-point analysis, and bulleted match previews and recaps, built on IBM watsonx, for all men's and women's singles matches. This includes for live matches a new, near real-time 3D graphic of current play.\")]\nAction:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"The 2024 US Open Tennis Championship was held at the USTA Billie Jean King National Tennis Center in Flushing, Queens, New York.\"\n}\n```\nObservation\n\n&gt; Finished chain.\n</pre> Out[\u00a0]: <pre>{'input': 'Where was the 2024 US Open Tennis Championship?',\n 'history': '',\n 'output': 'The 2024 US Open Tennis Championship was held at the USTA Billie Jean King National Tennis Center in Flushing, Queens, New York.'}</pre> <p>Great! The agent used its available RAG tool to return the location of the 2024 US Open, per the user's query. We even get to see the exact document that the agent is retrieving its information from. Now, let's try a slightly more complex question query. This time, the query will be about IBM's involvement in the 2024 US Open.</p> In\u00a0[\u00a0]: Copied! <pre>agent_executor.invoke(\n    {\"input\": \"How did IBM use watsonx at the 2024 US Open Tennis Championship?\"}\n)\n</pre> agent_executor.invoke(     {\"input\": \"How did IBM use watsonx at the 2024 US Open Tennis Championship?\"} ) <pre>\n\n&gt; Entering new AgentExecutor chain...\n\n\n{\n  \"action\": \"get_IBM_US_Open_context\",\n  \"action_input\": \"How did IBM use watsonx at the 2024 US Open Tennis Championship?\"\n}\n\nObservation[Document(metadata={'description': 'To help the US Open stay on the cutting edge of customer experience, IBM Consulting built powerful generative AI models with watsonx.', 'language': 'en', 'source': 'https://www.ibm.com/case-studies/us-open', 'title': 'U.S. Open | IBM'}, page_content='The US Open is a sprawling, two-week tournament, with hundreds of matches played on 22 different courts. Keeping up with all the action is a challenge, both for tennis fans and the USTA editorial team covering the event. So, the USTA asked IBM to design, develop, and deliver solutions that enhance the digital experience and help its team serve up more content, covering more matches throughout the tournament.\\nTo do it, the IBM Consulting team built generative AI-powered features using the watsonx AI and data platform. For example, Match Reports are AI-generated post-match summaries that are designed to get fans quickly up to speed on the action from around the tournament. AI Commentary adds AI-generated, spoken commentary to match highlights. And SlamTracker\u2013the premier scoring application for the US Open\u2013features AI-generated match previews and recaps.\\n\u201cThe AI models built with watsonx do more than enhance the digital experience of the US Open,\u201d says Kirsten Corio, Chief Commercial Officer, USTA. \u201cThey also scale the productivity of our editorial team by automating key workflows.\u201d'), Document(metadata={'description': \"IBM and the United States Tennis Association (USTA) announced several watsonx-powered fan features coming to the US Open digital platforms ahead of this year's tournament. These new and enhanced capabilities \u2013 a product of collaboration between IBM and the USTA digital team \u2013 aim to deliver a more informative and engaging experience for millions of tennis fans around the world.\", 'language': 'en-us', 'source': 'https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms', 'title': 'IBM and the USTA Serve Up New and Enhanced Generative AI Features for 2024 US Open Digital Platforms'}, page_content=\"IBM and the USTA Serve Up New and Enhanced Generative AI Features for 2024 US Open Digital Platforms\\n-New Match Report summaries offer fans detailed analysis for all 254 US Open main draw singles matches-Enhanced version of AI Commentary returns to US Open digital platforms for on-demand highlights, along with fully redesigned IBM SlamTracker experience-IBM and USTA Foundation announce new collaboration to provide AI professional development resources to USTA Foundation students, teaching professionals and the public\\nAug 15, 2024\\n\\n\\n\\n\\n\\n\\nARMONK, N.Y., Aug. 15, 2024 /PRNewswire/ -- IBM (NYSE:\\xa0IBM) and the United States Tennis Association (USTA) today announced several watsonx-powered fan features coming to the US Open digital platforms ahead of this year's tournament. These new and enhanced capabilities \u2013 a product of collaboration between IBM and the USTA digital team \u2013 aim to deliver a more informative and engaging experience for millions of tennis fans around the world.\"), Document(metadata={'description': '', 'language': 'en-us', 'source': 'https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement', 'title': 'US Open: IBM and AI Can Help Engage Tennis Fans '}, page_content='Feature Story: In the Era of Empty US Open Seats, IBM and AI Can Help  Fans Stay Engaged\\nBy Trish Hall\\nAs the US Open gets underway in New York this week, the world\u2019s top tennis players are performing in front of empty seats\u2014not the 800,000 or so fans who would normally fill the Billie Jean King National Tennis Center during the two-week tournament.\\r\\n\\xa0\\r\\nBut tennis fans will be watching from home on televisions, computers and the US Open app. To connect them even more deeply to the matches, IBM, in collaboration with the tournament\u2019s host, the United States Tennis Association (USTA), has developed new, immersive interactive features using IBM\u2019s AI and Watson capabilities.'), Document(metadata={'description': '', 'language': 'en-us', 'source': 'https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement', 'title': 'US Open: IBM and AI Can Help Engage Tennis Fans '}, page_content='Against that worrisome backdrop for the sports industry, the USTA-IBM team knew that to keep fans involved, it would have to come up with something completely different this year. It would also need to engage people who might not know a lot about tennis, but are sports enthusiasts hungry for the resumption of any new athletic competition.\\n\u201cWe were seeing a lot more casual viewers of the classic match replays,\u201d said Kirsten Corio, who directs digital strategy for the USTA. \u201cMaybe they were at home and bored, starved for new sports content. We hope that this will make those more casual fans more educated about the sport and the tournament.\u201d\\nThe two new interactive AI-enabled innovations are Open Questions With Watson, and Match Insights With Watson Discovery. Both will be available on the USTA\u2019s USOpen.org website.\\nOpen Questions with Watson will employ Natural Language Processing (NLP) capabilities in IBM\\'s Watson\\xa0 Discovery, IBM Project Debater and custom AI algorithms to generate and moderate a public conversation during the tournament over questions like \"Is Serena Williams the best player in tennis?\\'\\' and \"Does Pete Sampras have the best all-around tennis game?\"')]\nAction:\n```\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"IBM used watsonx at the 2024 US Open Tennis Championship to create generative AI-powered features such as Match Reports, AI Commentary, and SlamTracker. These features enhance the digital experience for fans and scale the productivity of the USTA editorial team.\"\n}\n```\nObservation\n\n&gt; Finished chain.\n</pre> Out[\u00a0]: <pre>{'input': 'How did IBM use watsonx at the 2024 US Open Tennis Championship?',\n 'history': 'Human: Where was the 2024 US Open Tennis Championship?\\nAI: The 2024 US Open Tennis Championship was held at the USTA Billie Jean King National Tennis Center in Flushing, Queens, New York.',\n 'output': 'IBM used watsonx at the 2024 US Open Tennis Championship to create generative AI-powered features such as Match Reports, AI Commentary, and SlamTracker. These features enhance the digital experience for fans and scale the productivity of the USTA editorial team.'}</pre> <p>Again, the agent was able to successfully retrieve the relevant information pertaining to the user query. Additionally, the agent is successfully updating its knowledge base as it learns new information and experiences new interactions as seen by the history output.</p> <p>Now, let's test if the agent can decipher when tool calling is not necessary to answer the user query. We can test this by asking the RAG agent a question that is not about the US Open.</p> In\u00a0[\u00a0]: Copied! <pre>agent_executor.invoke({\"input\": \"What is the capital of France?\"})\n</pre> agent_executor.invoke({\"input\": \"What is the capital of France?\"}) <pre>\n\n&gt; Entering new AgentExecutor chain...\n\n\n{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"The capital of France is Paris.\"\n}\n\nObservation\n\n&gt; Finished chain.\n</pre> Out[\u00a0]: <pre>{'input': 'What is the capital of France?',\n 'history': 'Human: Where was the 2024 US Open Tennis Championship?\\nAI: The 2024 US Open Tennis Championship was held at the USTA Billie Jean King National Tennis Center in Flushing, Queens, New York.\\nHuman: How did IBM use watsonx at the 2024 US Open Tennis Championship?\\nAI: IBM used watsonx at the 2024 US Open Tennis Championship to create generative AI-powered features such as Match Reports, AI Commentary, and SlamTracker. These features enhance the digital experience for fans and scale the productivity of the USTA editorial team.',\n 'output': 'The capital of France is Paris.'}</pre> <p>As seen in the AgentExecutor chain, the agent recognized that it had the information in its knowledge base to answer this question without using its tools.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#build-a-langchain-agentic-rag-system-using-the-granite-30-8b-instruct-model-in-watsonxai","title":"Build a LangChain agentic RAG system using the Granite-3.0-8B-Instruct model in watsonx.ai\u00b6","text":"<p>Author: Anna Gutowska</p> <p>In this tutorial, you will create a LangChain agentic RAG system using the IBM Granite-3.0-8B-Instruct model now available on watsonx.ai that can answer complex queries about the 2024 US Open using external information.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#overview-of-agentic-rag","title":"Overview of agentic RAG\u00b6","text":""},{"location":"tutorials/generative-ai/agentic-rag/#what-is-rag","title":"What is RAG?\u00b6","text":"<p>RAG is a technique in natural language processing (NLP) that combines information retrieval and generative models to produce more accurate, relevant and contextually aware responses. In traditional language generation tasks, large language models (LLMs) such as Meta's Llama Models or IBM\u2019s Granite Models are used to construct responses based on an input prompt. Common real-world use cases of these large language models are chatbots. When models are missing relevant information that is up to date in their knowledge base, RAG is a powerful tool.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#what-are-ai-agents","title":"What are AI agents?\u00b6","text":"<p>At the core of agentic RAG systems are artificial intelligence (AI) agents. An AI agent refers to a system or program that is capable of autonomously performing tasks on behalf of a user or another system by designing its workflow and using available tools. Agentic technology implements tool use on the backend to obtain up-to-date information from various data sources, optimize workflow and create subtasks autonomously to solve complex tasks. These external tools can include external data sets, search engines, APIs and even other agents. Step-by-step, the agent reassesses its plan of action in real time and self-corrects.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#agentic-rag-versus-traditional-rag","title":"Agentic RAG versus traditional RAG\u00b6","text":"<p>Agentic RAG frameworks are powerful as they can encompass more than just one tool. In traditional RAG applications, the LLM is provided with a vector database to reference when forming its responses. In contrast, agentic RAG implementations are not restricted to document agents that only perform data retrieval. RAG agents can also have tools for tasks such as solving mathematical calculations, writing emails, performing data analysis and more. These tools can be supplemental to the agent's decision-making process. AI agents are context-aware in their multistep reasoning and can determine when to use appropriate tools.</p> <p>AI agents, or intelligent agents, can also work collaboratively in multiagent systems, which tend to outperform singular agents. This scalability and adaptability is what sets apart agentic RAG agents from traditional RAG pipelines.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You need an IBM Cloud\u00ae account to create a watsonx.ai\u2122 project.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/agentic-rag/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on Github.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#step-2-set-up-watsonxai-runtime-service-and-api-key","title":"Step 2. Set up watsonx.ai Runtime service and API key\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service to the project you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/agentic-rag/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We'll need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this with a quick pip installation.</p> <p>Common Python frameworks for building agentic RAG systems include LangChain and LlamaIndex. In this tutorial, we will be using LangChain.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#step-4-initialization-a-basic-agent-with-no-tools","title":"Step 4. Initialization a basic agent with no tools\u00b6","text":"<p>This step is important as it will produce a clear example of an agent's behavior with and without external data sources. Let's start by setting our parameters.</p> <p>The model parameters available can be found here. We experimented with various model parameters, including temperature, minimum and maximum new tokens and stop sequences. Learn more about model parameters and what they mean in the watsonx docs. It is important to set our <code>stop_sequences</code> here in order to limit agent hallucinations. This tells the agent to stop producing further output upon encountering particular substrings. In our case, we want the agent to end its response upon reaching an observation and to not hallucinate a human response. Hence, one of our stop_sequences is <code>'Human:'</code> and another is <code>Observation</code> to halt once a final response is produced.</p> <p>For this tutorial, we suggest using IBM's Granite-3.0-8B-Instruct model as the LLM to achieve similar results. You are free to use any AI model of your choice. The foundation models available through watsonx can be found here. The purpose of these models in LLM applications is to serve as the reasoning engine that decides which actions to take.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#step-5-establish-the-knowledge-base-and-retriever","title":"Step 5. Establish the knowledge base and retriever\u00b6","text":"<p>The first step in creating the knowledge base is listing the URLs we will be extracting content from. In this case, our data source will be collected from our online content summarizing IBM\u2019s involvement in the 2024 US Open. The relevant URLs are established in the <code>urls</code> list.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#step-6-define-the-agents-rag-tool","title":"Step 6. Define the agent's RAG tool\u00b6","text":"<p>Let's define the <code>get_IBM_US_Open_context()</code> tool our agent will be using. This tool's only parameter is the user query. The tool description is also noted to inform the agent of the use of the tool. This way, the agent knows when to call this tool. This tool can be used by the agentic RAG system for routing the user query to the vector store if it pertains to IBM\u2019s involvement in the 2024 US Open.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#step-7-establish-the-prompt-template","title":"Step 7. Establish the prompt template\u00b6","text":"<p>Next, we will set up a new prompt template to ask multiple questions. This template is more complex. It is referred to as a structured chat prompt and can be used for creating agents that have multiple tools available. In our case, the tool we are using was defined in Step 6. The structured chat prompt will be made up of a <code>system_prompt</code>, a <code>human_prompt</code> and our RAG tool.</p> <p>First, we will set up the <code>system_prompt</code>. This prompt instructs the agent to print its \"thought process,\" which involves the agent's subtasks, the tools that were used and the final output. This gives us insight into the agent's function calling. The prompt also instructs the agent to return its responses in JSON Blob format.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#step-8-set-up-the-agents-memory-and-chain","title":"Step 8. Set up the agent's memory and chain\u00b6","text":"<p>An important feature of AI agents is their memory. Agents are able to store past conversations and past findings in their memory to improve the accuracy and relevance of their responses going forward. In our case, we will use LangChain's <code>ConversationBufferMemory()</code> as a means of memory storage.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#step-9-generate-responses-with-the-agentic-rag-system","title":"Step 9. Generate responses with the agentic RAG system\u00b6","text":"<p>We are now able to ask the agent questions. Recall the agent's previous inability to provide us with information pertaining to the 2024 US Open. Now that the agent has its RAG tool available to use, let's try asking the same questions again.</p>"},{"location":"tutorials/generative-ai/agentic-rag/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, you created a RAG agent using LangChain in python with watsonx. The LLM you worked with was the IBM Granite-3.0-8B-Instruct model. The sample output is important as it shows the significance of this generative AI advancement. The AI agent was successfully able to retrieve relevant information via the <code>get_IBM_US_Open_context</code> tool, update its memory with each interaction and output appropriate responses. It is also important to note the agent's ability to determine whether tool calling is appropriate for each specific task. When the agent had the information necessary to answer the input query, it did not use any tools for question answering.</p> <p>For more AI agent content, we encourage you to check out our AI agent tutorial that returns today's Astronomy Picture of the Day using NASA's open source API and a date tool.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/","title":"DeepSeek Reasoning: Improving the R1 distilled model with RAG and watsonx.ai","text":"In\u00a0[\u00a0]: Copied! <pre># Install required packages\n!pip install -q \"langchain&gt;=0.1.0\" \"langchain-community&gt;=0.0.13\" \"langchain-core&gt;=0.1.17\" \\\n    \"langchain-ollama&gt;=0.0.1\" \"pdfminer.six&gt;=20221105\" \"markdown&gt;=3.5.2\" \"docling&gt;=2.0.0\" \\\n    \"beautifulsoup4&gt;=4.12.0\" \"unstructured&gt;=0.12.0\" \"chromadb&gt;=0.4.22\" \"faiss-cpu&gt;=1.7.4\" \\\n    \"requests&gt;=2.32.0\" \"langchain-ibm&gt;=0.3.5\"\n</pre> # Install required packages !pip install -q \"langchain&gt;=0.1.0\" \"langchain-community&gt;=0.0.13\" \"langchain-core&gt;=0.1.17\" \\     \"langchain-ollama&gt;=0.0.1\" \"pdfminer.six&gt;=20221105\" \"markdown&gt;=3.5.2\" \"docling&gt;=2.0.0\" \\     \"beautifulsoup4&gt;=4.12.0\" \"unstructured&gt;=0.12.0\" \"chromadb&gt;=0.4.22\" \"faiss-cpu&gt;=1.7.4\" \\     \"requests&gt;=2.32.0\" \"langchain-ibm&gt;=0.3.5\" In\u00a0[\u00a0]: Copied! <pre># Required imports\nimport os\nimport tempfile\nimport shutil\nimport getpass\n\nfrom pathlib import Path\nfrom IPython.display import Markdown, display\nfrom langchain_ibm import WatsonxLLM\n\n# Docling imports\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption, WordFormatOption, SimplePipeline\n\n# LangChain imports\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoader, WebBaseLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_ollama import OllamaEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n</pre> # Required imports import os import tempfile import shutil import getpass  from pathlib import Path from IPython.display import Markdown, display from langchain_ibm import WatsonxLLM  # Docling imports from docling.datamodel.base_models import InputFormat from docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions from docling.document_converter import DocumentConverter, PdfFormatOption, WordFormatOption, SimplePipeline  # LangChain imports from langchain_community.document_loaders import UnstructuredMarkdownLoader, WebBaseLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_ollama import OllamaEmbeddings from langchain_community.vectorstores import FAISS from langchain.chains import ConversationalRetrievalChain from langchain.memory import ConversationBufferMemory from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams <p>To set our credentials, we need the <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> you generated in step 1. We will also set the URL serving as the API endpoint.</p> In\u00a0[\u00a0]: Copied! <pre>WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://us-south.ml.cloud.ibm.com\"\n</pre> WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://us-south.ml.cloud.ibm.com\" In\u00a0[\u00a0]: Copied! <pre>llm = WatsonxLLM(\n        model_id=\"deepseek-ai/deepseek-r1-distill-llama-70b\",\n        url=URL,\n        apikey=WATSONX_APIKEY,\n        project_id=WATSONX_PROJECT_ID,\n        params={\n            GenParams.DECODING_METHOD: \"greedy\",\n            GenParams.TEMPERATURE: 0,\n            GenParams.MIN_NEW_TOKENS: 5,\n            GenParams.MAX_NEW_TOKENS: 2000,\n            GenParams.REPETITION_PENALTY:1.2\n        }\n)\n</pre> llm = WatsonxLLM(         model_id=\"deepseek-ai/deepseek-r1-distill-llama-70b\",         url=URL,         apikey=WATSONX_APIKEY,         project_id=WATSONX_PROJECT_ID,         params={             GenParams.DECODING_METHOD: \"greedy\",             GenParams.TEMPERATURE: 0,             GenParams.MIN_NEW_TOKENS: 5,             GenParams.MAX_NEW_TOKENS: 2000,             GenParams.REPETITION_PENALTY:1.2         } ) In\u00a0[\u00a0]: Copied! <pre>def get_document_format(file_path) -&gt; InputFormat:\n    \"\"\"Determine the document format based on file extension\"\"\"\n    try:\n        file_path = str(file_path)\n        extension = os.path.splitext(file_path)[1].lower()\n\n        format_map = {\n            '.pdf': InputFormat.PDF,\n            '.docx': InputFormat.DOCX,\n            '.doc': InputFormat.DOCX,\n            '.pptx': InputFormat.PPTX,\n            '.html': InputFormat.HTML,\n            '.htm': InputFormat.HTML\n        }\n        return format_map.get(extension, None)\n    except:\n        return \"Error in get_document_format: {str(e)}\"\n</pre> def get_document_format(file_path) -&gt; InputFormat:     \"\"\"Determine the document format based on file extension\"\"\"     try:         file_path = str(file_path)         extension = os.path.splitext(file_path)[1].lower()          format_map = {             '.pdf': InputFormat.PDF,             '.docx': InputFormat.DOCX,             '.doc': InputFormat.DOCX,             '.pptx': InputFormat.PPTX,             '.html': InputFormat.HTML,             '.htm': InputFormat.HTML         }         return format_map.get(extension, None)     except:         return \"Error in get_document_format: {str(e)}\" In\u00a0[\u00a0]: Copied! <pre>def convert_document_to_markdown(doc_path) -&gt; str:\n    \"\"\"Convert document to markdown using simplified pipeline\"\"\"\n    try:\n        # Convert to absolute path string\n        input_path = os.path.abspath(str(doc_path))\n        print(f\"Converting document: {doc_path}\")\n\n        # Create temporary directory for processing\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Copy input file to temp directory\n            temp_input = os.path.join(temp_dir, os.path.basename(input_path))\n            shutil.copy2(input_path, temp_input)\n\n            # Configure pipeline options\n            pipeline_options = PdfPipelineOptions()\n            pipeline_options.do_ocr = False  # Disable OCR temporarily\n            pipeline_options.do_table_structure = True\n\n            # Create converter with minimal options\n            converter = DocumentConverter(\n                allowed_formats=[\n                    InputFormat.PDF,\n                    InputFormat.DOCX,\n                    InputFormat.HTML,\n                    InputFormat.PPTX,\n                ],\n                format_options={\n                    InputFormat.PDF: PdfFormatOption(\n                        pipeline_options=pipeline_options,\n                    ),\n                    InputFormat.DOCX: WordFormatOption(\n                        pipeline_cls=SimplePipeline\n                    )\n                }\n            )\n\n            # Convert document\n            print(\"Starting conversion...\")\n            conv_result = converter.convert(temp_input)\n\n            if not conv_result or not conv_result.document:\n                raise ValueError(f\"Failed to convert document: {doc_path}\")\n\n            # Export to markdown\n            print(\"Exporting to markdown...\")\n            md = conv_result.document.export_to_markdown()\n\n            # Create output path\n            output_dir = os.path.dirname(input_path)\n            base_name = os.path.splitext(os.path.basename(input_path))[0]\n            md_path = os.path.join(output_dir, f\"{base_name}_converted.md\")\n\n            # Write markdown file\n            print(f\"Writing markdown to: {base_name}_converted.md\")\n            with open(md_path, \"w\", encoding=\"utf-8\") as fp:\n                fp.write(md)\n\n            return md_path\n    except:\n        return f\"Error converting document: {doc_path}\"\n</pre> def convert_document_to_markdown(doc_path) -&gt; str:     \"\"\"Convert document to markdown using simplified pipeline\"\"\"     try:         # Convert to absolute path string         input_path = os.path.abspath(str(doc_path))         print(f\"Converting document: {doc_path}\")          # Create temporary directory for processing         with tempfile.TemporaryDirectory() as temp_dir:             # Copy input file to temp directory             temp_input = os.path.join(temp_dir, os.path.basename(input_path))             shutil.copy2(input_path, temp_input)              # Configure pipeline options             pipeline_options = PdfPipelineOptions()             pipeline_options.do_ocr = False  # Disable OCR temporarily             pipeline_options.do_table_structure = True              # Create converter with minimal options             converter = DocumentConverter(                 allowed_formats=[                     InputFormat.PDF,                     InputFormat.DOCX,                     InputFormat.HTML,                     InputFormat.PPTX,                 ],                 format_options={                     InputFormat.PDF: PdfFormatOption(                         pipeline_options=pipeline_options,                     ),                     InputFormat.DOCX: WordFormatOption(                         pipeline_cls=SimplePipeline                     )                 }             )              # Convert document             print(\"Starting conversion...\")             conv_result = converter.convert(temp_input)              if not conv_result or not conv_result.document:                 raise ValueError(f\"Failed to convert document: {doc_path}\")              # Export to markdown             print(\"Exporting to markdown...\")             md = conv_result.document.export_to_markdown()              # Create output path             output_dir = os.path.dirname(input_path)             base_name = os.path.splitext(os.path.basename(input_path))[0]             md_path = os.path.join(output_dir, f\"{base_name}_converted.md\")              # Write markdown file             print(f\"Writing markdown to: {base_name}_converted.md\")             with open(md_path, \"w\", encoding=\"utf-8\") as fp:                 fp.write(md)              return md_path     except:         return f\"Error converting document: {doc_path}\" In\u00a0[\u00a0]: Copied! <pre>def setup_qa_chain(markdown_path: Path, web_pages: list, embeddings_model_name:str = \"nomic-embed-text:latest\", model_name: str = \"deepseek-ai/deepseek-r1-distill-llama-70b\"):\n    \"\"\"Set up the QA chain for document processing\"\"\"\n    # Load and split the document metadata\n    loader = UnstructuredMarkdownLoader(str(markdown_path)) \n    markdown_doc = loader.load()\n    \n    loaded_pages = [WebBaseLoader(url).load() for url in web_pages]\n    web_page_docs = [item for sublist in loaded_pages for item in sublist]\n\n    documents = markdown_doc + web_page_docs\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=500,\n        chunk_overlap=50,\n        length_function=len\n    )\n    texts = text_splitter.split_documents(documents)\n    \n    # Transform knowledge base to vector embeddings stored in a vector store\n    embeddings = OllamaEmbeddings(\n        model=embeddings_model_name\n        )\n    vectorstore = FAISS.from_documents(texts, embeddings)\n    \n    # Initialize LLM\n    llm = WatsonxLLM(\n        model_id=model_name,\n        url=URL,\n        apikey=WATSONX_APIKEY,\n        project_id=WATSONX_PROJECT_ID,\n        params={\n            GenParams.DECODING_METHOD: \"greedy\",\n            GenParams.TEMPERATURE: 0,\n            GenParams.MIN_NEW_TOKENS: 5,\n            GenParams.MAX_NEW_TOKENS: 2000,\n            GenParams.REPETITION_PENALTY:1.2\n        }\n    )\n    \n    # Set up conversation memory\n    memory = ConversationBufferMemory(\n        memory_key=\"chat_history\",\n        output_key=\"answer\",\n        return_messages=True\n    )\n    \n    # Create the chain\n    qa_chain = ConversationalRetrievalChain.from_llm(\n        llm=llm,\n        retriever=vectorstore.as_retriever(\n            search_kwargs={\"k\": 10}\n            ),\n        memory=memory,\n        return_source_documents=True\n    )\n    \n    return qa_chain\n</pre> def setup_qa_chain(markdown_path: Path, web_pages: list, embeddings_model_name:str = \"nomic-embed-text:latest\", model_name: str = \"deepseek-ai/deepseek-r1-distill-llama-70b\"):     \"\"\"Set up the QA chain for document processing\"\"\"     # Load and split the document metadata     loader = UnstructuredMarkdownLoader(str(markdown_path))      markdown_doc = loader.load()          loaded_pages = [WebBaseLoader(url).load() for url in web_pages]     web_page_docs = [item for sublist in loaded_pages for item in sublist]      documents = markdown_doc + web_page_docs      text_splitter = RecursiveCharacterTextSplitter(         chunk_size=500,         chunk_overlap=50,         length_function=len     )     texts = text_splitter.split_documents(documents)          # Transform knowledge base to vector embeddings stored in a vector store     embeddings = OllamaEmbeddings(         model=embeddings_model_name         )     vectorstore = FAISS.from_documents(texts, embeddings)          # Initialize LLM     llm = WatsonxLLM(         model_id=model_name,         url=URL,         apikey=WATSONX_APIKEY,         project_id=WATSONX_PROJECT_ID,         params={             GenParams.DECODING_METHOD: \"greedy\",             GenParams.TEMPERATURE: 0,             GenParams.MIN_NEW_TOKENS: 5,             GenParams.MAX_NEW_TOKENS: 2000,             GenParams.REPETITION_PENALTY:1.2         }     )          # Set up conversation memory     memory = ConversationBufferMemory(         memory_key=\"chat_history\",         output_key=\"answer\",         return_messages=True     )          # Create the chain     qa_chain = ConversationalRetrievalChain.from_llm(         llm=llm,         retriever=vectorstore.as_retriever(             search_kwargs={\"k\": 10}             ),         memory=memory,         return_source_documents=True     )          return qa_chain In\u00a0[\u00a0]: Copied! <pre>def ask_question(qa_chain, question: str):\n    \"\"\"Ask a question and display the answer\"\"\"\n    result = qa_chain.invoke({\"question\": question})\n    display(Markdown(f\"**Question:** {question}\\n\\n**Answer:** {result['answer']}\"))\n</pre> def ask_question(qa_chain, question: str):     \"\"\"Ask a question and display the answer\"\"\"     result = qa_chain.invoke({\"question\": question})     display(Markdown(f\"**Question:** {question}\\n\\n**Answer:** {result['answer']}\")) In\u00a0[\u00a0]: Copied! <pre># Process the RFP document\ndoc_path = Path(\"Next-Gen-Software-Solution-RFP.pdf\")  # Replace with your document path\n\n# Check format and process\ndoc_format = get_document_format(doc_path)\nif doc_format:\n    md_path = convert_document_to_markdown(doc_path)\nelse:\n    print(f\"Unsupported document format: {doc_path.suffix}\")\n</pre> # Process the RFP document doc_path = Path(\"Next-Gen-Software-Solution-RFP.pdf\")  # Replace with your document path  # Check format and process doc_format = get_document_format(doc_path) if doc_format:     md_path = convert_document_to_markdown(doc_path) else:     print(f\"Unsupported document format: {doc_path.suffix}\") In\u00a0[\u00a0]: Copied! <pre>urls = [\"https://www.ibm.com/products/blog/5-things-to-know-ibm-clouds-mission-to-accelerate-innovation-for-clients\",\n       \"https://newsroom.ibm.com/Blog-How-IBM-Cloud-is-Accelerating-Business-Outcomes-with-Gen-AI\"]\n\nqa_chain = setup_qa_chain(md_path, urls)\nquestion = \"List out the key scope and requirements of the RFP. Then draft a detailed RFP response as though it was written by IBM. Be explicit about the technical approach and technologies using the provided context from IBM's website.\"\nask_question(qa_chain, question)\n</pre> urls = [\"https://www.ibm.com/products/blog/5-things-to-know-ibm-clouds-mission-to-accelerate-innovation-for-clients\",        \"https://newsroom.ibm.com/Blog-How-IBM-Cloud-is-Accelerating-Business-Outcomes-with-Gen-AI\"]  qa_chain = setup_qa_chain(md_path, urls) question = \"List out the key scope and requirements of the RFP. Then draft a detailed RFP response as though it was written by IBM. Be explicit about the technical approach and technologies using the provided context from IBM's website.\" ask_question(qa_chain, question) <p>Question: List out the key scope and requirements of the RFP. Then draft a detailed RFP response as though it was written by IBM. Be explicit about the technical approach and technologies using the provided context from IBM's website.</p> <p>Answer:  Okay, so I need to figure out what exactly the user is asking here. They've given me some sections from an RFP document and then asked two things: first, to list the key scope and requirements of the RFP based on the provided context; second, to draft a detailed RFP response as if it were from IBM, making sure to explicitly mention the technical approach and technologies, possibly pulling info from IBM's site since they mentioned that.</p> <p>Alright, starting with part one\u2014key scope and requirements. Looking through the context snippets, there are several points that stand out. First, under VII. QUESTIONS, it says that any questions related to the RFP should be emailed to a specific address, referencing the relevant pages and sections according to the schedule in Section IV. Also, late questions might not get answers, and all Q&amp;As will be publicly posted. That tells me communication protocols and deadlines are important.</p> <p>Then, VIII. GENERAL PROVISIONS mentions that each bidder needs to submit a clear, concise proposal focusing on compliance with RFP instructions, completeness, and clarity. So, the proposal has to strictly adhere to guidelines, probably including formatting and content specifics.</p> <p>Looking further down, under A. MINIMUM QUALIFICATION REQUIREMENTS, bidders must detail their current capabilities, past experience especially with states and big cities, and how that applies to NYS. This indicates that relevant experience and adaptability to NY\u2019s environment are crucial.</p> <p>There's also something about proper format: creating a named folder and uploading via a Dropbox link. Late submissions aren\u2019t allowed, so timeliness is another requirement.</p> <p>Section D. PROJECT PLAN/APPROACH requires demonstrating competence and capacity, likely meaning a solid project plan is needed. Similarly, C. STAFF EXPERIENCE AND QUALIFICATIONS emphasizes having knowledgeable and capable staff, supported perhaps by references who can vouch for their performance.</p> <p>Additionally, the TECHNICAL PROPOSAL includes elements like table of contents, estimated cost, diversity practices, etc., showing that multiple aspects beyond just technical skills are evaluated.</p> <p>So compiling all that, the key scope seems to involve developing a next-gen software solution tailored to NYS needs, while the main requirements would cover submission guidelines, necessary documentation (like project plans, team qualifications), minimum qualifications (past experience, capability), adherence to formats, timely delivery, and inclusion of various administrative parts.</p> <p>Now moving to drafting the RFP response as IBM. Since I'm supposed to use context from IBM's website, I'll think about their strengths. IBM often touts AI, cloud solutions, data analytics, blockchain, IoT, and strong consulting services. Their approach usually involves scalable, secure, and innovative tech.</p> <p>I\u2019d structure the response to mirror the RFP sections. Start with understanding the problem, aligning with NYS goals. Highlight IBM's experience with similar projects, maybe reference state-level engagements. Detail the technical approach: perhaps leveraging IBM Cloud, Watson AI tools for automation, integrating advanced analytics for decision-making, ensuring security measures, and utilizing agile methodologies for development.</p> <p>Include project management strategies, emphasizing experienced teams and proven track records. Address diversity practices, mentioning IBM's commitment to inclusivity and equal opportunities. Finally, present a cost estimate structured around these components without being too vague.</p> <p>But wait, looking back, the initial instruction said to only proceed if confident, else admit lack of knowledge. However, considering the extensive details above, I believe I can construct a reasonable outline even without direct access to IBM's latest materials beyond general knowledge.</p> <p>Wait again\u2014the user actually wants me to write both the key scope/requirements and then the detailed RFP response. But when I look more closely, the \"helpful answer\" example didn't fully execute both tasks\u2014it started explaining thought processes instead. Maybe the actual task is to generate both lists?</p> <p>Alternatively, perhaps the user expects me to acknowledge whether I can fulfill the request accurately. Given that I\u2019ve analyzed the RFP sections sufficiently, I could attempt listing the key scopes and requirements, followed by a mock IBM response incorporating known IBM competencies.</p> <p>However, I realize that without specific IBM documents or deeper insights into their proprietary approaches, my response won't perfectly reflect IBM's exact language or initiatives. Still, using common IBM messaging should suffice for a hypothetical scenario.</p> <p>To sum up, yes, I can outline the key scope and requirements, then craft a plausible RFP response mimicking IBM's style, highlighting areas where IBM typically excels, such as AI integration, cloud platforms, robust project management, and diverse staffing practices. </p> <p>Key Scope and Requirements of the RFP</p> <ol> <li><p>Communication Protocols:</p> <ul> <li>Submit questions via email to NextGenSoftwareRFP2024@esd.ny.gov, referencing the appropriate RFP section and adhering to the schedule outlined in Section IV.</li> <li>No oral communications; late queries may go unanswered. All Q&amp;A will be published online.</li> </ul> </li> <li><p>Proposal Submission Guidelines:</p> <ul> <li>Proposals must be clear, concise, compliant with RFP instructions, and thorough in addressing all requirements.</li> <li>Properly formatted electronic submissions are mandatory, uploaded to the specified Dropbox link with the correct naming convention.</li> </ul> </li> <li><p>Minimum Qualification Requirements:</p> <ul> <li>Demonstrate current capabilities, past experience with states/municipalities, and relevance to New York State operations.</li> <li>Provide evidence of understanding operational models, best practices, and potential process improvements.</li> </ul> </li> <li><p>Technical Proposal Components:</p> <ul> <li>Include a detailed project plan/approach, showcasing competence and capacity.</li> <li>Highlight staff experience and qualifications, supported by professional references.</li> </ul> </li> <li><p>Administrative Proposal Elements:</p> <ul> <li>Detailed table of contents for easy navigation.</li> <li>Estimated costs and adherence to Schedule A contractual terms.</li> <li>Commitment to diversity practices as per Appendix B.</li> </ul> </li> <li><p>Evaluation Criteria:</p> <ul> <li>Compliance with RFP instructions, completeness, clarity, and alignment with stated objectives.</li> <li>Quality of services, ability to deliver, and responsiveness based on references.</li> </ul> </li> </ol> <p>IBM RFP Response</p> <p>Introduction</p> <p>At IBM, we recognize the transformative power of technology in shaping efficient governance. We are excited to respond to your RFP for a next-generation software solution tailored to New York State's unique demands. Our proposal leverages decades of expertise in delivering scalable, secure, and innovative technological solutions across government sectors globally.</p> <p>Understanding the Problem</p> <p>New York State faces evolving challenges requiring adaptable, intelligent systems. IBM understands the necessity for solutions that enhance service delivery, optimize resources, and ensure citizen satisfaction. Our approach integrates cutting-edge technologies to drive efficiency and innovation.</p> <p>Proposed Solution Overview</p> <p>Our solution harnesses IBM's leadership in AI, cloud computing, and data analytics to offer a modular platform designed for scalability and interoperability. Key features include:</p> <ul> <li>AI-Powered Automation: Utilizing IBM Watson to streamline workflows and predictive analytics for informed decision-making.</li> <li>Cloud Infrastructure: Built on IBM Cloud, offering flexibility, resilience, and enhanced security.</li> <li>Blockchain Integration: Ensuring transparency and integrity in transactions and data exchanges.</li> <li>IoT Enablement: Facilitating real-time monitoring and smart system interactions.</li> </ul> <p>Project Management Strategy</p> <p>We employ Agile methodologies to ensure iterative progress, stakeholder collaboration, and rapid adaptation to feedback. Our dedicated project managers oversee timelines, budgets, and resource allocation, ensuring seamless execution aligned with NYS priorities.</p> <p>Staff Expertise</p> <p>IBM's multidisciplinary team brings deep domain knowledge and technical prowess. From seasoned consultants to expert developers, our professionals are committed to excellence, supported by continuous learning programs and certifications.</p> <p>References and Track Record</p> <p>With a legacy of successful public sector engagements, IBM offers testimonials from numerous governments worldwide. Our clients attest to our reliability, innovation, and dedication to exceptional outcomes.</p> <p>Cost Estimate</p> <p>Our pricing model reflects a balanced investment in technology and human capital, ensuring value without compromising quality. Costs are itemized to include software licensing, implementation, training, and ongoing support, optimized for long-term sustainability.</p> <p>Commitment to Diversity</p> <p>IBM champions diversity and inclusion, reflected in our hiring practices and community partnerships. We pledge to maintain equitable standards throughout this engagement, fostering a culture of respect and empowerment.</p> <p>Conclusion</p> <p>IBM stands ready to collaborate with New York State in co-creating a future-ready digital infrastructure. With a blend of technological innovation and proven methodologies, we aim to exceed expectations and set new benchmarks in public service delivery.</p> <p>Thank you for considering IBM's proposal. We welcome the opportunity to discuss how we can contribute to New York State's vision for tomorrow.</p> <p>This response encapsulates IBM's strategic approach, combining technical expertise with a customer-centric philosophy to meet the RFP's stringent requirements effectively.</p> <p>Great! The LLM was able to retrieve relevant information from the RFP document to provide the key scope and requirements in its final answer. Additionally, the relevant information from the ibm.com web pages was successfully extracted and incorporated into a draft proposal.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#deepseek-reasoning-improving-the-r1-distilled-model-with-rag-and-watsonxai","title":"DeepSeek Reasoning: Improving the R1 distilled model with RAG and watsonx.ai\u00b6","text":"<p>Authors: Ash Minhas, Anna Gutowska</p> <p>In this tutorial, we will leverage the high-performance reasoning abilities of the Llama 3.3 70b distilled variant of the DeepSeek-R1 large language model (LLM) now available on watsonx.ai\u2122 by using IBM\u00ae Docling in Python. The use case is to process a request for proposals (RFP) and create a business-specific proposal in response.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#reasoning-capabilities-of-llms","title":"Reasoning capabilities of LLMs\u00b6","text":"<p>Recent advancements in machine learning and deep learning have greatly improved the emergent logical reasoning skills of state-of-the-art large language models (LLMs). This development has caused significant debate about whether LLMs are truly capable of reasoning or whether they are simply imitating human decision-making by following the patterns present in their training data.</p> <p>There are many types of reasoning such as common sense, abductive, deductive and inductive reasoning. These forms of reasoning are innate to many of us humans, but it is exceedingly difficult to build an AI model capable of excelling in all forms of reasoning. LLMs are bound by the knowledge acquired during the training process. A model might excel in mathematical reasoning or a common benchmark but might completely falter when applied to a different use case. If LLMs are truly capable of multi-step reasoning, their capacity to do so would be general and not limited to a particular example. Hence, in this tutorial, we recognize human and LLM reasoning as distinct from one another.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#ways-to-improve-llm-reasoning","title":"Ways to improve LLM reasoning\u00b6","text":"<p>To supplement an LLM\u2019s training data without fine-tuning, we can perform retrieval augmented generation (RAG). RAG is a technique in natural language processing (NLP) that grounds the model on an up-to-date, accurate dataset to facilitate in-context learning.</p> <p>To encourage complex reasoning and problem-solving, chain of thought (CoT) prompting can also be used. Chain of thought reasoning is an approach in artificial intelligence that simulates human-like reasoning processes by decoding complex problems into reasoning steps toward a final resolution. Variants of chain of thought prompting can be zero-shot, automatic and multimodal.</p> <p>DeepSeek-R1 combines chain of thought reasoning with reinforcement learning to enhance performance. In this tutorial, we demonstrate how to use RAG as another way of improving the model\u2019s semantic interpretability and reasoning.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You need an IBM Cloud\u00ae account.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai by using your IBM Cloud\u00ae account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> <p>This step opens a Jupyter Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This code is available on GitHub.</p> </li> </ol>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-2-set-up-a-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up a watsonx.ai Runtime instance and API key\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (select your appropriate region).</p> </li> <li><p>Generate an application programming interface (API) key.</p> </li> <li><p>Associate the watsonx.ai Runtime service instance to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-3-deploy-deepseek-r1s-distilled-variant-on-ibm-watsonxai","title":"Step 3. Deploy DeepSeek-R1\u2019s distilled variant on IBM watsonx.ai\u00b6","text":"<p>The reasoning model that we use in this tutorial is <code>deepseek-r1-distill-llama-70b</code>. The DeepSeek-R1 distilled variants based on Llama and Qwen are the new models now available on watsonx.ai. DeepSeek-V3, DeepSeek-R1 and DeepSeek-R1-Zero, the generative AI models from Chinese startup DeepSeek, are some of the most powerful open-source reasoning models, rivaling the model performance of OpenAI\u2019s o1 series of models.</p> <p>Reference the instructions for how DeepSeek distilled variants can be deployed as a foundation model on-demand from the Resource hub in the IBM announcement blog. The DeepSeek-R1\u2019s distilled variants can be deployed on an hourly basis on a dedicated GPU.</p> <p>Important: To run DeepSeek-R1 distilled smaller models in watsonx.ai, you need to deploy the model to a GPU before proceeding with the rest of this tutorial.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-4-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 4. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We need a few libraries and modules for this AI application. Make sure to import the following ones and if they're not installed, a quick pip installation resolves the problem.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-5-initialize-your-llm","title":"Step 5. Initialize your LLM\u00b6","text":"<p>We will use the Llama 3.3 70b distilled variant of the DeepSeek-R1 open-source model in this tutorial. To initialize the LLM, we need to set the model parameters. To learn more about these model parameters, such as the minimum and maximum token limits, refer to the documentation.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-6-document-format-detection","title":"Step 6. Document format detection\u00b6","text":"<p>We work with various document formats in this tutorial. Let's create a helper function to detect document formats by using the file extension.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-7-document-conversion","title":"Step 7. Document conversion\u00b6","text":"<p>Next, we can use the <code>DocumentConverter</code> class to create a function that converts any supported document to markdown. This function identifies text, data tables, document images and captions by using Docling. The function takes a file as input, processes it using Docling's advanced document handling, converts it to markdown and saves the results in a Markdown file. Both scanned and text-based documents are supported and document structure is preserved. Key components of this function are:</p> <ul> <li><code>PdfPipelineOptions</code>: Configures how PDFs are processed.</li> <li><code>TesseractCliOcrOptions</code>: Sets up OCR for scanned documents.</li> <li><code>DocumentConverter</code>: Handles the actual conversion process</li> </ul>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-8-qa-chain-setup","title":"Step 8. QA chain setup\u00b6","text":"<p>The QA chain is the heart of our RAG system. It combines several components:</p> <ol> <li>Document loading:</li> </ol> <ul> <li>Loads the markdown file that we created.</li> <li>Loads the scraped web data.</li> </ul> <ol> <li>Text splitting:</li> </ol> <ul> <li>Breaks down the document into smaller pieces.</li> <li>Maintains context with overlap between chunks.</li> <li>Ensures efficient processing by the language model.</li> </ul> <ol> <li>Vector database:</li> </ol> <ul> <li>Creates embeddings for each text chunk.</li> <li>Stores them in a FAISS index for fast retrieval.</li> <li>Enables semantic search capabilities.</li> </ul> <ol> <li>Language model:</li> </ol> <ul> <li>Uses Ollama for embeddings and the watsonx.ai API for text generation.</li> <li>Maintains conversation history.</li> <li>Generates contextual responses.</li> </ul> <p>The following <code>setup_qa_chain</code> function sets up this entire RAG pipeline.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-9-set-up-question-answering-interface","title":"Step 9. Set up question-answering interface\u00b6","text":"<p>Finally, let's create a simple interface for asking questions. This function takes in the chain and user query as parameters. The function also improves the readability of the displayed question and answer.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#step-10-perform-question-answering","title":"Step 10. Perform question-answering\u00b6","text":"<p>There are several real-world applications of reasoning tasks. This tutorial serves as a step-by-step guide for using a pretrained AI model to process a New York State RFP and formulate a proposal. The path to our RFP is stored in <code>doc_path</code>. The URLs used for web scraping are from ibm.com and describe the software offerings of IBM relevant to this RFP.</p> <p>Note: The use of this software solution RFP is for illustrative purposes only. The document is publicly available and was accessed for this tutorial on February 5, 2025.</p>"},{"location":"tutorials/generative-ai/deepseek_rag_reasoning_docling_watsonx/#conclusion","title":"Conclusion\u00b6","text":"<p>Using Docling and a distilled variant of a Deepseek model, you built a local RAG application for document question answering that is compatible with various file types. A possible next step would be to create an AI agent to perform the same functionality with additional, personalized tools. There are many opportunities to transform this RAG template to apply to specific use cases. Feel free to test this system with any of your own files!</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/","title":"Build a document-based question answering system by using Docling with Granite 3.1","text":"<ol> <li>Install and import the necessary libraries and modules.</li> </ol> In\u00a0[1]: Copied! <pre>!ollama pull granite3.1-dense:8b\n!ollama pull nomic-embed-text\n</pre> !ollama pull granite3.1-dense:8b !ollama pull nomic-embed-text <pre>pulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \npulling 0a922eb99317... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.9 GB                         \npulling f7b956e70ca3... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   69 B                         \npulling f76a906816c4... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 1.4 KB                         \npulling 492069a62c25... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  11 KB                         \npulling e026ee8ed889... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  491 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \npulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \u2838 pulling manifest \npulling 970aa74c0a90... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 274 MB                         \npulling c71d239df917... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  11 KB                         \npulling ce4a164fc046... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   17 B                         \npulling 31df23ea7daa... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  420 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \n</pre> In\u00a0[2]: Copied! <pre># Install required packages\n!pip install -q \"langchain&gt;=0.1.0\" \"langchain-community&gt;=0.0.13\" \"langchain-core&gt;=0.1.17\" \\\n    \"langchain-ollama&gt;=0.0.1\" \"pdfminer.six&gt;=20221105\" \"markdown&gt;=3.5.2\" \"docling&gt;=2.0.0\" \\\n    \"beautifulsoup4&gt;=4.12.0\" \"unstructured&gt;=0.12.0\" \"chromadb&gt;=0.4.22\" \"faiss-cpu&gt;=1.7.4\" \\\n    \"requests&gt;=2.32.0\"\n</pre> # Install required packages !pip install -q \"langchain&gt;=0.1.0\" \"langchain-community&gt;=0.0.13\" \"langchain-core&gt;=0.1.17\" \\     \"langchain-ollama&gt;=0.0.1\" \"pdfminer.six&gt;=20221105\" \"markdown&gt;=3.5.2\" \"docling&gt;=2.0.0\" \\     \"beautifulsoup4&gt;=4.12.0\" \"unstructured&gt;=0.12.0\" \"chromadb&gt;=0.4.22\" \"faiss-cpu&gt;=1.7.4\" \\     \"requests&gt;=2.32.0\" In\u00a0[3]: Copied! <pre># Required imports\nimport os\nimport tempfile\nimport shutil\nfrom pathlib import Path\nfrom IPython.display import Markdown, display\n\n# Docling imports\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions\nfrom docling.document_converter import DocumentConverter, PdfFormatOption, WordFormatOption, SimplePipeline\n\n# LangChain imports\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_ollama import OllamaEmbeddings, OllamaLLM\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n</pre> # Required imports import os import tempfile import shutil from pathlib import Path from IPython.display import Markdown, display  # Docling imports from docling.datamodel.base_models import InputFormat from docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions from docling.document_converter import DocumentConverter, PdfFormatOption, WordFormatOption, SimplePipeline  # LangChain imports from langchain_community.document_loaders import UnstructuredMarkdownLoader from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_ollama import OllamaEmbeddings, OllamaLLM from langchain_community.vectorstores import FAISS from langchain.chains import ConversationalRetrievalChain from langchain.memory import ConversationBufferMemory In\u00a0[4]: Copied! <pre>def get_document_format(file_path) -&gt; InputFormat:\n    \"\"\"Determine the document format based on file extension\"\"\"\n    try:\n        file_path = str(file_path)\n        extension = os.path.splitext(file_path)[1].lower()\n\n        format_map = {\n            '.pdf': InputFormat.PDF,\n            '.docx': InputFormat.DOCX,\n            '.doc': InputFormat.DOCX,\n            '.pptx': InputFormat.PPTX,\n            '.html': InputFormat.HTML,\n            '.htm': InputFormat.HTML\n        }\n        return format_map.get(extension, None)\n    except:\n        return \"Error in get_document_format: {str(e)}\"\n</pre> def get_document_format(file_path) -&gt; InputFormat:     \"\"\"Determine the document format based on file extension\"\"\"     try:         file_path = str(file_path)         extension = os.path.splitext(file_path)[1].lower()          format_map = {             '.pdf': InputFormat.PDF,             '.docx': InputFormat.DOCX,             '.doc': InputFormat.DOCX,             '.pptx': InputFormat.PPTX,             '.html': InputFormat.HTML,             '.htm': InputFormat.HTML         }         return format_map.get(extension, None)     except:         return \"Error in get_document_format: {str(e)}\" In\u00a0[5]: Copied! <pre>def convert_document_to_markdown(doc_path) -&gt; str:\n    \"\"\"Convert document to markdown using simplified pipeline\"\"\"\n    try:\n        # Convert to absolute path string\n        input_path = os.path.abspath(str(doc_path))\n        print(f\"Converting document: {doc_path}\")\n\n        # Create temporary directory for processing\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Copy input file to temp directory\n            temp_input = os.path.join(temp_dir, os.path.basename(input_path))\n            shutil.copy2(input_path, temp_input)\n\n            # Configure pipeline options\n            pipeline_options = PdfPipelineOptions()\n            pipeline_options.do_ocr = False  # Disable OCR temporarily\n            pipeline_options.do_table_structure = True\n\n            # Create converter with minimal options\n            converter = DocumentConverter(\n                allowed_formats=[\n                    InputFormat.PDF,\n                    InputFormat.DOCX,\n                    InputFormat.HTML,\n                    InputFormat.PPTX,\n                ],\n                format_options={\n                    InputFormat.PDF: PdfFormatOption(\n                        pipeline_options=pipeline_options,\n                    ),\n                    InputFormat.DOCX: WordFormatOption(\n                        pipeline_cls=SimplePipeline\n                    )\n                }\n            )\n\n            # Convert document\n            print(\"Starting conversion...\")\n            conv_result = converter.convert(temp_input)\n\n            if not conv_result or not conv_result.document:\n                raise ValueError(f\"Failed to convert document: {doc_path}\")\n\n            # Export to markdown\n            print(\"Exporting to markdown...\")\n            md = conv_result.document.export_to_markdown()\n\n            # Create output path\n            output_dir = os.path.dirname(input_path)\n            base_name = os.path.splitext(os.path.basename(input_path))[0]\n            md_path = os.path.join(output_dir, f\"{base_name}_converted.md\")\n\n            # Write markdown file\n            print(f\"Writing markdown to: {base_name}_converted.md\")\n            with open(md_path, \"w\", encoding=\"utf-8\") as fp:\n                fp.write(md)\n\n            return md_path\n    except:\n        return f\"Error converting document: {doc_path}\"\n</pre> def convert_document_to_markdown(doc_path) -&gt; str:     \"\"\"Convert document to markdown using simplified pipeline\"\"\"     try:         # Convert to absolute path string         input_path = os.path.abspath(str(doc_path))         print(f\"Converting document: {doc_path}\")          # Create temporary directory for processing         with tempfile.TemporaryDirectory() as temp_dir:             # Copy input file to temp directory             temp_input = os.path.join(temp_dir, os.path.basename(input_path))             shutil.copy2(input_path, temp_input)              # Configure pipeline options             pipeline_options = PdfPipelineOptions()             pipeline_options.do_ocr = False  # Disable OCR temporarily             pipeline_options.do_table_structure = True              # Create converter with minimal options             converter = DocumentConverter(                 allowed_formats=[                     InputFormat.PDF,                     InputFormat.DOCX,                     InputFormat.HTML,                     InputFormat.PPTX,                 ],                 format_options={                     InputFormat.PDF: PdfFormatOption(                         pipeline_options=pipeline_options,                     ),                     InputFormat.DOCX: WordFormatOption(                         pipeline_cls=SimplePipeline                     )                 }             )              # Convert document             print(\"Starting conversion...\")             conv_result = converter.convert(temp_input)              if not conv_result or not conv_result.document:                 raise ValueError(f\"Failed to convert document: {doc_path}\")              # Export to markdown             print(\"Exporting to markdown...\")             md = conv_result.document.export_to_markdown()              # Create output path             output_dir = os.path.dirname(input_path)             base_name = os.path.splitext(os.path.basename(input_path))[0]             md_path = os.path.join(output_dir, f\"{base_name}_converted.md\")              # Write markdown file             print(f\"Writing markdown to: {base_name}_converted.md\")             with open(md_path, \"w\", encoding=\"utf-8\") as fp:                 fp.write(md)              return md_path     except:         return f\"Error converting document: {doc_path}\" In\u00a0[10]: Copied! <pre>def setup_qa_chain(markdown_path: Path, embeddings_model_name:str = \"nomic-embed-text:latest\", model_name: str = \"granite3.1-dense:8b\"):\n    \"\"\"Set up the QA chain for document processing\"\"\"\n    # Load and split the document\n    loader = UnstructuredMarkdownLoader(str(markdown_path)) \n    documents = loader.load()\n    \n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=500,\n        chunk_overlap=50,\n        length_function=len\n    )\n    texts = text_splitter.split_documents(documents)\n    # texts= documents\n    \n    # Create embeddings and vector store\n    embeddings = OllamaEmbeddings(\n        model=embeddings_model_name\n        )\n    vectorstore = FAISS.from_documents(texts, embeddings)\n    \n    # Initialize LLM\n    llm = OllamaLLM(\n        model=model_name,\n        temperature=0\n    )\n    \n    # Set up conversation memory\n    memory = ConversationBufferMemory(\n        memory_key=\"chat_history\",\n        output_key=\"answer\",\n        return_messages=True\n    )\n    \n    # Create the chain\n    qa_chain = ConversationalRetrievalChain.from_llm(\n        llm=llm,\n        retriever=vectorstore.as_retriever(\n            search_kwargs={\"k\": 10}\n            ),\n        memory=memory,\n        return_source_documents=True\n    )\n    \n    return qa_chain\n</pre> def setup_qa_chain(markdown_path: Path, embeddings_model_name:str = \"nomic-embed-text:latest\", model_name: str = \"granite3.1-dense:8b\"):     \"\"\"Set up the QA chain for document processing\"\"\"     # Load and split the document     loader = UnstructuredMarkdownLoader(str(markdown_path))      documents = loader.load()          text_splitter = RecursiveCharacterTextSplitter(         chunk_size=500,         chunk_overlap=50,         length_function=len     )     texts = text_splitter.split_documents(documents)     # texts= documents          # Create embeddings and vector store     embeddings = OllamaEmbeddings(         model=embeddings_model_name         )     vectorstore = FAISS.from_documents(texts, embeddings)          # Initialize LLM     llm = OllamaLLM(         model=model_name,         temperature=0     )          # Set up conversation memory     memory = ConversationBufferMemory(         memory_key=\"chat_history\",         output_key=\"answer\",         return_messages=True     )          # Create the chain     qa_chain = ConversationalRetrievalChain.from_llm(         llm=llm,         retriever=vectorstore.as_retriever(             search_kwargs={\"k\": 10}             ),         memory=memory,         return_source_documents=True     )          return qa_chain In\u00a0[11]: Copied! <pre>def ask_question(qa_chain, question: str):\n    \"\"\"Ask a question and display the answer\"\"\"\n    result = qa_chain.invoke({\"question\": question})\n    display(Markdown(f\"**Question:** {question}\\n\\n**Answer:** {result['answer']}\"))\n</pre> def ask_question(qa_chain, question: str):     \"\"\"Ask a question and display the answer\"\"\"     result = qa_chain.invoke({\"question\": question})     display(Markdown(f\"**Question:** {question}\\n\\n**Answer:** {result['answer']}\")) In\u00a0[12]: Copied! <pre># Process a document\ndoc_path = Path(\"ibmredbook.pdf\")  # Replace with your document path\n\n# Check format and process\ndoc_format = get_document_format(doc_path)\nif doc_format:\n    md_path = convert_document_to_markdown(doc_path)\n    qa_chain = setup_qa_chain(md_path)\n    \n    # Example questions\n    questions = [\n        \"What is the main topic of this document?\",\n        \"What are the key points discussed?\",\n        \"Can you summarize the conclusions?\",\n    ]\n    \n    for question in questions:\n        ask_question(qa_chain, question)\nelse:\n    print(f\"Unsupported document format: {doc_path.suffix}\")\n</pre> # Process a document doc_path = Path(\"ibmredbook.pdf\")  # Replace with your document path  # Check format and process doc_format = get_document_format(doc_path) if doc_format:     md_path = convert_document_to_markdown(doc_path)     qa_chain = setup_qa_chain(md_path)          # Example questions     questions = [         \"What is the main topic of this document?\",         \"What are the key points discussed?\",         \"Can you summarize the conclusions?\",     ]          for question in questions:         ask_question(qa_chain, question) else:     print(f\"Unsupported document format: {doc_path.suffix}\") <pre>Converting document: ibmredbook.pdf\nStarting conversion...\nExporting to markdown...\nWriting markdown to: ibmredbook_converted.md\n</pre> <p>Question: What is the main topic of this document?</p> <p>Answer: The main topics covered in this document are building and managing containers using Red Hat OpenShift, deploying applications, and security aspects related to containerization on IBM Power systems. The document also includes an introduction to Red Hat OpenShift, its benefits, core concepts, and implementation on IBM Power. Additionally, it discusses multi-architecture containerization, monitoring tools and techniques, log management, performance tuning, and optimization.</p> <p>Question: What are the key points discussed?</p> <p>Answer: This document primarily focuses on implementing Red Hat OpenShift Container Platform on IBM Power systems for managing containers in a hybrid cloud environment. Key points covered include:</p> <ol> <li><p>Introduction to Red Hat OpenShift: It is an enterprise Kubernetes platform that extends Kubernetes with additional features and tools, enhancing productivity and security for businesses using container technology at scale.</p> </li> <li><p>Benefits of Using Red Hat OpenShift for Container Orchestration: The document highlights the advantages of employing Red Hat OpenShift for managing containers, such as its comprehensive solution for hybrid cloud environments, including a container runtime, networking, monitoring, a container registry, authentication, and authorization.</p> </li> <li><p>Minimum IBM Power Requirements: Red Hat OpenShift Container Platform 4.15 can be installed on IBM Power 9 or IBM Power 10 processor-based systems.</p> </li> <li><p>Deploying Red Hat OpenShift on IBM Power Systems: This involves tailoring the networking infrastructure to leverage the robust capabilities and unique architecture of Power Systems, optimizing network performance for high throughput and low latency, ensuring network security and compliance, and managing network configurations for enterprise-level deployments.</p> </li> <li><p>Optimizing Network Performance: The document emphasizes the importance of faster storage, particularly for etcd on control plane nodes, as Red Hat OpenShift Container Platform is sensitive to disk performance.</p> </li> <li><p>Multi-Architecture Containerization: The text discusses key concepts in multi-architecture containerization and provides guidelines for implementing it using IBM Power control planes. It also addresses challenges and solutions related to multi-architecture containerization.</p> </li> <li><p>Security Aspects: Enterprise-grade security is mentioned as a crucial aspect of Red Hat OpenShift on IBM Power systems, although specific details are not provided in the given context.</p> </li> <li><p>Monitoring Tools and Log Management: The document does not explicitly mention monitoring tools or log management; however, it can be inferred that these aspects are covered within the broader context of managing containers using Red Hat OpenShift on IBM Power systems.</p> </li> <li><p>Performance Tuning and Optimization: While specific tuning and optimization techniques are not detailed in the provided context, the document implies that performance considerations should be taken into account during deployment and configuration.</p> </li> </ol> <p>Question: Can you summarize the conclusions?</p> <p>Answer: The document discusses the implementation of Red Hat OpenShift Container Platform on IBM Power systems for managing containers in a hybrid cloud environment. Here are the main conclusions drawn from this document:</p> <ol> <li><p>Benefits: Red Hat OpenShift provides a comprehensive solution for hybrid cloud environments, encompassing essential components such as a container runtime, networking, monitoring, a container registry, authentication, and authorization. It extends Kubernetes with additional features and tools to enhance productivity and security, making it an ideal choice for businesses looking to leverage container technology at scale.</p> </li> <li><p>Minimum Requirements: Red Hat OpenShift Container Platform 4.15 can be installed on IBM Power 9 or IBM Power 10 processor-based systems. For comprehensive guidance and further information on installation and configuration, refer to the IBM Redbooks publication Implementing, Tuning, and Optimizing Workloads with Red Hat OpenShift on IBM Power (SG24-8537) and Red Hat OpenShift Documentation.</p> </li> <li><p>Deployment Process: Deploying Red Hat OpenShift on IBM Power Systems involves tailoring the networking infrastructure to fully leverage the robust capabilities and unique architecture of Power Systems. This includes optimizing network performance for high throughput and low latency, ensuring network security and compliance, and managing network configurations to meet enterprise-level demands.</p> </li> <li><p>Network Performance Optimization: Faster storage is recommended, particularly for etcd on control plane nodes. On many cloud platforms, storage size and IOPS scale together, so you might need to over-allocate storage volume to obtain sufficient performance.</p> </li> <li><p>Multi-Architecture Containerization: Red Hat OpenShift supports multiple architectures (x86 and IBM Power) with RHOS 4.14 or later, simplifying the management of your Red Hat OpenShift environment on both x86 and IBM Power servers.</p> </li> <li><p>Security Aspects: The integration of Red Hat OpenShift running on IBM Power servers with existing infrastructure involves strategic networking solutions that bridge on-premises systems with your new cloud environment. This enables organizations to leverage the strengths of both infrastructures for enhanced flexibility, scalability, and resilience while ensuring network security and compliance.</p> </li> <li><p>Performance Tuning: The document does not provide specific details about performance tuning; however, it is mentioned that optimizing network performance for high throughput and low latency is essential. For comprehensive guidance on performance tuning, refer to the IBM Redbooks publication Implementing, Tuning, and Optimizing Workloads with Red Hat OpenShift on IBM Power (SG24-8537) and Red Hat OpenShift Documentation.</p> </li> </ol> <p>In summary, this document highlights that implementing Red Hat OpenShift Container Platform on IBM Power systems offers a robust foundation for developing, deploying, and scaling cloud-native applications in a hybrid cloud environment. It emphasizes the importance of optimizing network performance, ensuring security, and leveraging multi-architecture containerization capabilities to create an efficient and flexible solution for managing containers.</p> <p>Great! The system was able to retrieve relevant information from the document to answer questions. Feel free to test this system with any of your own files and questions!</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#build-a-document-based-question-answering-system-by-using-docling-with-granite-31","title":"Build a document-based question answering system by using Docling with Granite 3.1\u00b6","text":"<p>Authors: Ash Minhas, Anna Gutowska</p> <p>In this tutorial, you will use IBM\u00ae Docling and open-source Granite\u2122 3.1 to perform document visual question answering for various file types.</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#what-is-docling","title":"What is Docling?\u00b6","text":"<p>Docling is an IBM open-source toolkit for parsing documents and exporting them to preferred formats. Input file formats include PDF, DOCX, PPTX, XLSX, Images, HTML, AsciiDoc and Markdown. These documents can be exported to markdown or JSON. Docling also provides OCR (optical character recognition) support for scanned documents. Use cases include scanning medical records, banking documents and even travel documents for quicker processing.</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#rag-and-large-context-windows","title":"RAG and large context windows\u00b6","text":"<p>Retrieval augmented generation (RAG) is an architecture for connecting large language models (LLMs) with external knowledge bases without fine-tuning or retraining. Text is embedded, stored in a vector database and finally, is used by the pre-trained model to return relevant information for natural language processing (NLP) and machine learning tasks.</p> <p>When an LLM has a larger context window, the generative AI model can process more information at once. This means that we can use both RAG and models with large context windows to leverage the ability to efficiently process more relevant information at a time. The LLM we use in this tutorial is the IBM <code>Granite-3.1-8B-Instruct</code> model. This model extends to a context window size of 128K tokens. We will access the model locally by using Ollama, without the use of an API. This model is also available on Hugging Face.</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#steps","title":"Steps\u00b6","text":"<p>This tutorial can be found on our GitHub in the form of a Jupyter Notebook.  Jupyter Notebooks are widely used within data science to combine code, text, images and data visualizations to formulate a well-formed analysis.</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>We first need to set up our environment by fulfilling some prerequisites.</p> <ol> <li><p>Install the latest version of Ollama to run locally.</p> </li> <li><p>Pull the latest Granite 3.1 model by running the following command.</p> </li> </ol>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#step-2-document-format-detection","title":"Step 2. Document format detection\u00b6","text":"<p>We will work with various document formats in this tutorial. Let's create a helper function to detect document formats by using the file extension.</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#step-3-document-conversion","title":"Step 3. Document conversion\u00b6","text":"<p>Next, we can use the <code>DocumentConverter</code> class to create a function that converts any supported document to markdown. This function identifies text, data tables, document images and captions by using Docling. The function takes a file as input, processes it using Docling's advanced document handling, converts it to markdown and saves the results in a Markdown file. Both scanned and text-based documents are supported and document structure is preserved. Key components of this function are:</p> <ul> <li><code>PdfPipelineOptions</code>: Configures how PDFs are processed.</li> <li><code>TesseractCliOcrOptions</code>: Sets up OCR for scanned documents.</li> <li><code>DocumentConverter</code>: Handles the actual conversion process</li> </ul>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#step-4-qa-chain-setup","title":"Step 4. QA chain setup\u00b6","text":"<p>The QA chain is the heart of our system. It combines several components:</p> <ol> <li>Document loading:</li> </ol> <ul> <li>Loads the markdown file that we created.</li> <li>Splits it into manageable chunks for processing.</li> </ul> <ol> <li>Text splitting:</li> </ol> <ul> <li>Breaks down the document into smaller pieces.</li> <li>Maintains context with overlap between chunks.</li> <li>Ensures efficient processing by the language model.</li> </ul> <ol> <li>Vector store:</li> </ol> <ul> <li>Creates embeddings for each text chunk.</li> <li>Stores them in a FAISS index for fast retrieval.</li> <li>Enables semantic search capabilities.</li> </ul> <ol> <li>Language model:</li> </ol> <ul> <li>Uses Ollama for both embeddings and text generation.</li> <li>Maintains conversation history.</li> <li>Generates contextual responses.</li> </ul> <p>The following <code>setup_qa_chain</code> function sets up this entire pipeline.</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#step-5-set-up-question-answering-interface","title":"Step 5. Set up question-answering interface\u00b6","text":"<p>Finally, let's create a simple interface for asking questions. This function takes in the chain and user query as parameters.</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#step-6-perform-question-answering","title":"Step 6. Perform question-answering\u00b6","text":"<p>Let's put it all together and enumerate over our questions for a specific document. The path to this document is stored in <code>doc_path</code> and can be any document you want to test. For our sample document, check out our GitHub. The system maintains conversation history and can handle follow-up questions.</p>"},{"location":"tutorials/generative-ai/docling_granite_question_answering/#conclusion","title":"Conclusion\u00b6","text":"<p>Using Docling and Granite 3.1, you built a document question answering system compatible with various file types. As a next step, this methodology can be applied to a chatbot with an interactive UI. There are many opportunities to transform this tutorial to apply to specific use cases.</p>"},{"location":"tutorials/generative-ai/function-calling/","title":"Implement function calling with the Granite-3.0-8B-Instruct model in Python with watsonx","text":"<p>Authors: Erika Russi, Anna Gutowska, Jess Bozorg</p> <p>In this tutorial, you will use the IBM\u00ae Granite-3.0-8B-Instruct model now available on watsonx.ai\u2122 to perform custom function calling.</p> <p>Traditional large language models (LLMs), like the OpenAI GPT-4 (generative pre-trained transformer) model available through ChatGPT, and the IBM Granite\u2122 models that we'll use in this tutorial, are limited in their knowledge and reasoning. They produce their responses based on the data used to train them and are difficult to adapt to personalized user queries. To obtain the missing information, these generative AI models can integrate external tools within the function calling. This method is one way to avoid fine-tuning a foundation model for each specific use-case. The function calling examples in this tutorial will implement external API calls.</p> <p>The Granite-3.0-8B-Instruct model and tokenizer use natural language processing (NLP) to parse query syntax. In addition, the models use function descriptions and function parameters to determine the appropriate tool calls. Key information is then extracted from user queries to be passed as function arguments.</p> In\u00a0[\u00a0]: Copied! <pre># installations\n%pip install -q transformers\n%pip install -q torch torchvision\n%pip install -q langchain-ibm\n</pre> # installations %pip install -q transformers %pip install -q torch torchvision %pip install -q langchain-ibm In\u00a0[\u00a0]: Copied! <pre>#imports\nimport requests\nimport ast\nimport re\nimport getpass\n\nfrom transformers import AutoTokenizer\nfrom transformers.utils import get_json_schema\nfrom langchain_ibm import WatsonxLLM\n</pre> #imports import requests import ast import re import getpass  from transformers import AutoTokenizer from transformers.utils import get_json_schema from langchain_ibm import WatsonxLLM <p>Next, we can prepare our environment by setting the model ID for the <code>granite-3-8b-instruct</code> model, and the tokenizer for the same Granite model.</p> <p>Input your <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> that you created in steps 1 and 2 upon running the following cell.</p> In\u00a0[\u00a0]: Copied! <pre>MODEL_ID = \"ibm/granite-3-8b-instruct\"\n\nTOKENIZER = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.0-8b-instruct\")\n\nWATSONX_URL = \"https://us-south.ml.cloud.ibm.com\"\n\nWATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n</pre> MODEL_ID = \"ibm/granite-3-8b-instruct\"  TOKENIZER = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.0-8b-instruct\")  WATSONX_URL = \"https://us-south.ml.cloud.ibm.com\"  WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \") <p>The <code>get_stock_price</code> function in this tutorial requires an <code>AV_STOCK_API_KEY</code> key. To generate a free <code>AV_STOCK_API_KEY</code>, please visit the Alpha Vantage website.</p> <p>Secondly, the <code>get_current_weather</code> function requires a <code>WEATHER_API_KEY</code>. To generate one, please create an account. Upon creating an account, select the \"API Keys\" tab to display your free key.</p> In\u00a0[\u00a0]: Copied! <pre>AV_STOCK_API_KEY = getpass.getpass(\"Please enter your AV_STOCK_API_KEY (hit enter): \")\n\nWEATHER_API_KEY = getpass.getpass(\"Please enter your WEATHER_API_KEY (hit enter): \")\n</pre> AV_STOCK_API_KEY = getpass.getpass(\"Please enter your AV_STOCK_API_KEY (hit enter): \")  WEATHER_API_KEY = getpass.getpass(\"Please enter your WEATHER_API_KEY (hit enter): \") <p>We can now define our functions. The function's docstring and type information are important for generating the proper tool information.</p> <p>In this tutorial, the <code>get_stock_price</code> function uses the Stock Market Data API available through Alpha Vantage.</p> In\u00a0[\u00a0]: Copied! <pre>def get_stock_price(ticker: str, date: str) -&gt; dict:\n    \"\"\"\n    Retrieves the lowest and highest stock prices for a given ticker and date.\n\n    Args:\n        ticker: The stock ticker symbol, e.g., \"IBM\".\n        date: The date in \"YYYY-MM-DD\" format for which you want to get stock prices.\n\n    Returns:\n        A dictionary containing the low and high stock prices on the given date.\n    \"\"\"\n    print(f\"Getting stock price for {ticker} on {date}\")\n    try:\n        stock_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&amp;symbol={ticker}&amp;apikey={AV_STOCK_API_KEY}\"\n        stock_data = requests.get(stock_url)\n        stock_low = stock_data.json()[\"Time Series (Daily)\"][date][\"3. low\"]\n        stock_high = stock_data.json()[\"Time Series (Daily)\"][date][\"2. high\"]\n        return {\n            \"low\": stock_low,\n            \"high\": stock_high\n        }\n    except Exception as e:\n        print(f\"Error fetching stock data: {e}\")\n        return {\n            \"low\": \"none\",\n            \"high\": \"none\"\n        }\n</pre> def get_stock_price(ticker: str, date: str) -&gt; dict:     \"\"\"     Retrieves the lowest and highest stock prices for a given ticker and date.      Args:         ticker: The stock ticker symbol, e.g., \"IBM\".         date: The date in \"YYYY-MM-DD\" format for which you want to get stock prices.      Returns:         A dictionary containing the low and high stock prices on the given date.     \"\"\"     print(f\"Getting stock price for {ticker} on {date}\")     try:         stock_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&amp;symbol={ticker}&amp;apikey={AV_STOCK_API_KEY}\"         stock_data = requests.get(stock_url)         stock_low = stock_data.json()[\"Time Series (Daily)\"][date][\"3. low\"]         stock_high = stock_data.json()[\"Time Series (Daily)\"][date][\"2. high\"]         return {             \"low\": stock_low,             \"high\": stock_high         }     except Exception as e:         print(f\"Error fetching stock data: {e}\")         return {             \"low\": \"none\",             \"high\": \"none\"         } <p>The <code>get_current_weather</code> function retrieves the real-time weather in a given location using the Current Weather Data API via OpenWeather.</p> In\u00a0[\u00a0]: Copied! <pre>def get_current_weather(location: str) -&gt; dict:\n    \"\"\"\n    Fetches the current weather for a given location (default: San Francisco).\n\n    Args:\n        location: The name of the city for which to retrieve the weather information.\n\n    Returns:\n        A dictionary containing weather information such as temperature, weather description, and humidity.\n    \"\"\"\n    print(f\"Getting current weather for {location}\")\n\n    try:\n        # API request to fetch weather data\n        weather_url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&amp;appid={WEATHER_API_KEY}&amp;units=metric\"\n        weather_data = requests.get(weather_url)\n        data = weather_data.json()\n        # Extracting relevant weather details\n        weather_description = data[\"weather\"][0][\"description\"]\n        temperature = data[\"main\"][\"temp\"]\n        humidity = data[\"main\"][\"humidity\"]\n\n        # Returning weather details\n        return {\n            \"description\": weather_description,\n            \"temperature\": temperature,\n            \"humidity\": humidity\n        }\n    except Exception as e:\n        print(f\"Error fetching weather data: {e}\")\n        return {\n            \"description\": \"none\",\n            \"temperature\": \"none\",\n            \"humidity\": \"none\"\n        }\n</pre> def get_current_weather(location: str) -&gt; dict:     \"\"\"     Fetches the current weather for a given location (default: San Francisco).      Args:         location: The name of the city for which to retrieve the weather information.      Returns:         A dictionary containing weather information such as temperature, weather description, and humidity.     \"\"\"     print(f\"Getting current weather for {location}\")      try:         # API request to fetch weather data         weather_url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&amp;appid={WEATHER_API_KEY}&amp;units=metric\"         weather_data = requests.get(weather_url)         data = weather_data.json()         # Extracting relevant weather details         weather_description = data[\"weather\"][0][\"description\"]         temperature = data[\"main\"][\"temp\"]         humidity = data[\"main\"][\"humidity\"]          # Returning weather details         return {             \"description\": weather_description,             \"temperature\": temperature,             \"humidity\": humidity         }     except Exception as e:         print(f\"Error fetching weather data: {e}\")         return {             \"description\": \"none\",             \"temperature\": \"none\",             \"humidity\": \"none\"         } <p>Now that our functions are defined, we can create a function that generates a watsonx API request for the provided instructions the watsonx API endpoint. We will use this function each time we make a request.</p> In\u00a0[\u00a0]: Copied! <pre>def make_api_request(instructions: str) -&gt; str:\n    model_parameters = {\n        \"decoding_method\": \"greedy\",\n        \"max_new_tokens\": 200,\n        \"repetition_penalty\": 1.05,\n        \"stop_sequences\": [TOKENIZER.eos_token]\n    }\n    model = WatsonxLLM(\n        model_id=MODEL_ID,\n        url= WATSONX_URL,\n        apikey=WATSONX_APIKEY,\n        project_id=WATSONX_PROJECT_ID,\n        params=model_parameters\n    )\n    response = model.invoke(instructions)\n    return response\n</pre> def make_api_request(instructions: str) -&gt; str:     model_parameters = {         \"decoding_method\": \"greedy\",         \"max_new_tokens\": 200,         \"repetition_penalty\": 1.05,         \"stop_sequences\": [TOKENIZER.eos_token]     }     model = WatsonxLLM(         model_id=MODEL_ID,         url= WATSONX_URL,         apikey=WATSONX_APIKEY,         project_id=WATSONX_PROJECT_ID,         params=model_parameters     )     response = model.invoke(instructions)     return response  <p>Next, we can create a list of available functions. Here, we declare our function definitions that require the function names, descriptions, parameters and required properties.</p> In\u00a0[\u00a0]: Copied! <pre>tools = [get_json_schema(tool) for tool in (get_stock_price, get_current_weather)]\ntools\n</pre> tools = [get_json_schema(tool) for tool in (get_stock_price, get_current_weather)] tools Out[\u00a0]: <pre>[{'type': 'function',\n  'function': {'name': 'get_stock_price',\n   'description': 'Retrieves the lowest and highest stock prices for a given ticker and date.',\n   'parameters': {'type': 'object',\n    'properties': {'ticker': {'type': 'string',\n      'description': 'The stock ticker symbol, e.g., \"IBM\".'},\n     'date': {'type': 'string',\n      'description': 'The date in \"YYYY-MM-DD\" format for which you want to get stock prices.'}},\n    'required': ['ticker', 'date']},\n   'return': {'type': 'object',\n    'description': 'A dictionary containing the low and high stock prices on the given date.'}}},\n {'type': 'function',\n  'function': {'name': 'get_current_weather',\n   'description': 'Fetches the current weather for a given location (default: San Francisco).',\n   'parameters': {'type': 'object',\n    'properties': {'location': {'type': 'string',\n      'description': 'The name of the city for which to retrieve the weather information.'}},\n    'required': ['location']},\n   'return': {'type': 'object',\n    'description': 'A dictionary containing weather information such as temperature, weather description, and humidity.'}}}]</pre> <p>To prepare for the API requests, we must set our <code>query</code> used in the tokenizer chat template.</p> In\u00a0[\u00a0]: Copied! <pre>query = \"What were the IBM stock prices on October 7, 2024?\"\n</pre> query = \"What were the IBM stock prices on October 7, 2024?\" <p>Applying a chat template is useful for breaking up long strings of texts into one or more messages with corresponding labels. This allows the LLM to process the input in a format that it expects. Because we want our output to be in a string format, we can set the <code>tokenize</code> parameter to false. The <code>add_generation_prompt</code> can be set to true in order to append the tokens indicating the beginning of an assistant message to the output. This will be useful when generating chat completions with the model.</p> In\u00a0[\u00a0]: Copied! <pre>conversation = [\n    {\"role\": \"system\",\"content\": \"You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.\"},\n    {\"role\": \"user\", \"content\": query },\n]\n\ninstruction_1 = TOKENIZER.apply_chat_template(conversation=conversation, tools=tools, tokenize=False, add_generation_prompt=True)\ninstruction_1\n</pre> conversation = [     {\"role\": \"system\",\"content\": \"You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.\"},     {\"role\": \"user\", \"content\": query }, ]  instruction_1 = TOKENIZER.apply_chat_template(conversation=conversation, tools=tools, tokenize=False, add_generation_prompt=True) instruction_1 Out[\u00a0]: <pre>'&lt;|start_of_role|&gt;available_tools&lt;|end_of_role|&gt;\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"get_stock_price\",\\n        \"description\": \"Retrieves the lowest and highest stock prices for a given ticker and date.\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"ticker\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The stock ticker symbol, e.g., \\\\\"IBM\\\\\".\"\\n                },\\n                \"date\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The date in \\\\\"YYYY-MM-DD\\\\\" format for which you want to get stock prices.\"\\n                }\\n            },\\n            \"required\": [\\n                \"ticker\",\\n                \"date\"\\n            ]\\n        },\\n        \"return\": {\\n            \"type\": \"object\",\\n            \"description\": \"A dictionary containing the low and high stock prices on the given date.\"\\n        }\\n    }\\n}\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"get_current_weather\",\\n        \"description\": \"Fetches the current weather for a given location (default: San Francisco).\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"location\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The name of the city for which to retrieve the weather information.\"\\n                }\\n            },\\n            \"required\": [\\n                \"location\"\\n            ]\\n        },\\n        \"return\": {\\n            \"type\": \"object\",\\n            \"description\": \"A dictionary containing weather information such as temperature, weather description, and humidity.\"\\n        }\\n    }\\n}&lt;|end_of_text|&gt;\\n&lt;|start_of_role|&gt;system&lt;|end_of_role|&gt;You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.&lt;|end_of_text|&gt;\\n&lt;|start_of_role|&gt;user&lt;|end_of_role|&gt;What were the IBM stock prices on October 7, 2024?&lt;|end_of_text|&gt;\\n&lt;|start_of_role|&gt;assistant&lt;|end_of_role|&gt;'</pre> <p>Now, we can call the <code>make_api_request</code> function and pass the instructions we generated.</p> In\u00a0[\u00a0]: Copied! <pre>data_1 = make_api_request(instruction_1)\ndata_1\n</pre> data_1 = make_api_request(instruction_1) data_1 Out[\u00a0]: <pre>'[{\"name\": \"get_stock_price\", \"arguments\": {\"ticker\": \"IBM\", \"date\": \"2024-10-07\"}}]'</pre> <p>As you can see by the function name in the JSON object produced by the model, the appropriate <code>get_stock_price</code> tool use was selected from the set of functions. To run the api call within the function, let's extract relevant information from the output. With the function name and arguments extracted, we can call the function. To call the function using its name as a string, we can use the <code>globals()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>def tool_call(llm_response: str):\n    tool_request = ast.literal_eval(re.search(\"({.+})\", llm_response).group(0))\n    tool_name = tool_request[\"name\"]\n    tool_arguments = tool_request[\"arguments\"]\n    tool_response = globals()[tool_name](**tool_arguments)\n    return tool_response\n</pre> def tool_call(llm_response: str):     tool_request = ast.literal_eval(re.search(\"({.+})\", llm_response).group(0))     tool_name = tool_request[\"name\"]     tool_arguments = tool_request[\"arguments\"]     tool_response = globals()[tool_name](**tool_arguments)     return tool_response <p>Get the response from the requested tool.</p> In\u00a0[\u00a0]: Copied! <pre>tool_response = tool_call(data_1)\ntool_response\n</pre> tool_response = tool_call(data_1) tool_response <pre>Getting stock price for IBM on 2024-10-07\n</pre> Out[\u00a0]: <pre>{'low': '225.0200', 'high': '227.6700'}</pre> <p>The function successfully retrieved the requested stock price. To generate a synthesized final response, we can pass another prompt to the Granite model along with the information collected from function calling.</p> In\u00a0[\u00a0]: Copied! <pre>conversation2 = conversation + [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Display the tool response in natural language.\" },\n    {\"role\": \"tool_response\", \"content\": str(tool_response) },\n]\n\ninstruction_2 = TOKENIZER.apply_chat_template(conversation=conversation2, tools=tools, tokenize=False, add_generation_prompt=True)\ndata_2 = make_api_request(instruction_2)\ndata_2\n</pre> conversation2 = conversation + [     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},     {\"role\": \"user\", \"content\": \"Display the tool response in natural language.\" },     {\"role\": \"tool_response\", \"content\": str(tool_response) }, ]  instruction_2 = TOKENIZER.apply_chat_template(conversation=conversation2, tools=tools, tokenize=False, add_generation_prompt=True) data_2 = make_api_request(instruction_2) data_2 Out[\u00a0]: <pre>'On October 7, 2024, the IBM stock prices ranged from a low of $225.02 to a high of $227.67.'</pre> In\u00a0[\u00a0]: Copied! <pre>query = \"What is the current weather in San Francisco?\"\n</pre> query = \"What is the current weather in San Francisco?\" In\u00a0[\u00a0]: Copied! <pre>conversation = [\n    {\"role\": \"system\",\"content\": \"You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.\"},\n    {\"role\": \"user\", \"content\": query },\n]\n\ninstruction_1 = TOKENIZER.apply_chat_template(conversation=conversation, tools=tools, tokenize=False, add_generation_prompt=True)\ninstruction_1\n</pre> conversation = [     {\"role\": \"system\",\"content\": \"You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.\"},     {\"role\": \"user\", \"content\": query }, ]  instruction_1 = TOKENIZER.apply_chat_template(conversation=conversation, tools=tools, tokenize=False, add_generation_prompt=True) instruction_1 Out[\u00a0]: <pre>'&lt;|start_of_role|&gt;available_tools&lt;|end_of_role|&gt;\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"get_stock_price\",\\n        \"description\": \"Retrieves the lowest and highest stock prices for a given ticker and date.\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"ticker\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The stock ticker symbol, e.g., \\\\\"IBM\\\\\".\"\\n                },\\n                \"date\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The date in \\\\\"YYYY-MM-DD\\\\\" format for which you want to get stock prices.\"\\n                }\\n            },\\n            \"required\": [\\n                \"ticker\",\\n                \"date\"\\n            ]\\n        },\\n        \"return\": {\\n            \"type\": \"object\",\\n            \"description\": \"A dictionary containing the low and high stock prices on the given date.\"\\n        }\\n    }\\n}\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"get_current_weather\",\\n        \"description\": \"Fetches the current weather for a given location (default: San Francisco).\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"location\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The name of the city for which to retrieve the weather information.\"\\n                }\\n            },\\n            \"required\": [\\n                \"location\"\\n            ]\\n        },\\n        \"return\": {\\n            \"type\": \"object\",\\n            \"description\": \"A dictionary containing weather information such as temperature, weather description, and humidity.\"\\n        }\\n    }\\n}&lt;|end_of_text|&gt;\\n&lt;|start_of_role|&gt;system&lt;|end_of_role|&gt;You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.&lt;|end_of_text|&gt;\\n&lt;|start_of_role|&gt;user&lt;|end_of_role|&gt;What is the current weather in San Francisco?&lt;|end_of_text|&gt;\\n&lt;|start_of_role|&gt;assistant&lt;|end_of_role|&gt;'</pre> In\u00a0[\u00a0]: Copied! <pre>data_1 = make_api_request(instruction_1)\ndata_1\n</pre> data_1 = make_api_request(instruction_1) data_1 Out[\u00a0]: <pre>'[{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"San Francisco\"}}]'</pre> <p>Once again, the model decides the appropriate tool choice, in this case <code>get_current_weather</code>, and extracts the location correctly. Now, let's call the function with the argument generated by the model.</p> In\u00a0[\u00a0]: Copied! <pre>tool_response = tool_call(data_1)\ntool_response\n</pre> tool_response = tool_call(data_1) tool_response <pre>Getting current weather for San Francisco\n</pre> Out[\u00a0]: <pre>{'description': 'clear sky', 'temperature': 15.52, 'humidity': 68}</pre> <p>The function response correctly describes the current weather in San Francisco. Lastly, let's generate a synthesized final response with the results of this function call.</p> In\u00a0[\u00a0]: Copied! <pre>conversation2 = conversation + [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Display the tool response in natural language.\" },\n    {\"role\": \"tool_response\", \"content\": str(tool_response) },\n]\n\ninstruction_2 = TOKENIZER.apply_chat_template(conversation=conversation2, tools=tools, tokenize=False, add_generation_prompt=True)\ndata_2 = make_api_request(instruction_2)\ndata_2\n</pre> conversation2 = conversation + [     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},     {\"role\": \"user\", \"content\": \"Display the tool response in natural language.\" },     {\"role\": \"tool_response\", \"content\": str(tool_response) }, ]  instruction_2 = TOKENIZER.apply_chat_template(conversation=conversation2, tools=tools, tokenize=False, add_generation_prompt=True) data_2 = make_api_request(instruction_2) data_2 Out[\u00a0]: <pre>'The current weather in San Francisco is clear with a temperature of 15.52 degrees and a humidity of 68%.'</pre>"},{"location":"tutorials/generative-ai/function-calling/#implement-function-calling-with-the-granite-30-8b-instruct-model-in-python-with-watsonx","title":"Implement function calling with the Granite-3.0-8B-Instruct model in Python with watsonx\u00b6","text":""},{"location":"tutorials/generative-ai/function-calling/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/function-calling/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>This step opens a notebook environment where you can copy the code from this tutorial. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset in Step 2. To view more Granite tutorials, check out the IBM Granite Community. This Jupyter Notebook is also available on GitHub.</p>"},{"location":"tutorials/generative-ai/function-calling/#step-2-set-up-watsonxai-runtime-service-and-api-key","title":"Step 2. Set up watsonx.ai Runtime service and API key\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service to the project you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/function-calling/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We'll need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this with a quick pip install. If you are running this tutorial locally, we recommend setting up a virtual environment to avoid Python package dependency conflicts.</p>"},{"location":"tutorials/generative-ai/function-calling/#step-4-define-the-functions","title":"Step 4. Define the functions\u00b6","text":""},{"location":"tutorials/generative-ai/function-calling/#step-5-set-up-the-api-request","title":"Step 5. Set up the API request\u00b6","text":""},{"location":"tutorials/generative-ai/function-calling/#step-6-perform-function-calling","title":"Step 6. Perform function calling\u00b6","text":""},{"location":"tutorials/generative-ai/function-calling/#step-6a-calling-the-get_stock_price-function","title":"Step 6a. Calling the get_stock_price function\u00b6","text":""},{"location":"tutorials/generative-ai/function-calling/#step-6b-calling-the-get_current_weather-function","title":"Step 6b. Calling the get_current_weather function\u00b6","text":"<p>As our next query, let\u2019s inquire about the current weather in San Francisco. We can follow the same steps as in Step 5a by adjusting the query.</p>"},{"location":"tutorials/generative-ai/function-calling/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, you built custom functions and used the Granite-3.0-8B-Instruct model to determine which function to call based on  key information from user queries. With this information, you called the function with the arguments as stated in the model response. These function calls produce the expected output. Finally, you called the Granite-3.0-8B-Instruct model again to synthesize the information returned by the functions.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/","title":"Use automatic speech recognition (ASR) to generate a podcast transcript using Granite 3.3 and watsonx.ai","text":"In\u00a0[\u00a0]: Copied! <pre># Install required packages\n! pip install -q peft torchaudio soundfile pytubefix pytube moviepy tqdm https://github.com/huggingface/transformers/archive/main.zip\n</pre> # Install required packages ! pip install -q peft torchaudio soundfile pytubefix pytube moviepy tqdm https://github.com/huggingface/transformers/archive/main.zip In\u00a0[\u00a0]: Copied! <pre># Required imports\nimport json\nimport os\n\nfrom pytubefix import YouTube\nfrom tqdm import tqdm\nfrom moviepy.audio.io.AudioFileClip import AudioFileClip\n\nimport torch\nimport torchaudio\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\nfrom huggingface_hub import hf_hub_download\n</pre> # Required imports import json import os  from pytubefix import YouTube from tqdm import tqdm from moviepy.audio.io.AudioFileClip import AudioFileClip  import torch import torchaudio from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq from huggingface_hub import hf_hub_download In\u00a0[\u00a0]: Copied! <pre>url = \"https://www.youtube.com/watch?v=90fUR1PQgt4\" #latest episode 37 minutes\n\n# Create a YouTube object\nyt = YouTube(url)\n\n# Download only the audio stream from the video\nvideo = yt.streams.filter(only_audio=True).first()\n\n# Save the audio to a file\nout_file = video.download()\n\n# Get the base name and extension of the downloaded audio\nbase = os.path.splitext(out_file)[0]\n</pre> url = \"https://www.youtube.com/watch?v=90fUR1PQgt4\" #latest episode 37 minutes  # Create a YouTube object yt = YouTube(url)  # Download only the audio stream from the video video = yt.streams.filter(only_audio=True).first()  # Save the audio to a file out_file = video.download()  # Get the base name and extension of the downloaded audio base = os.path.splitext(out_file)[0] In\u00a0[\u00a0]: Copied! <pre># Load the M4A file\naudio_clip = AudioFileClip(out_file)\n\n# Write the audio to a WAV file\naudio_clip.write_audiofile(base+\".wav\")\n\n# Close the audio clip\naudio_clip.close()\n\naudio_path = base+\".wav\"\n</pre> # Load the M4A file audio_clip = AudioFileClip(out_file)  # Write the audio to a WAV file audio_clip.write_audiofile(base+\".wav\")  # Close the audio clip audio_clip.close()  audio_path = base+\".wav\" <p>Next, we'll use <code>torchaudiodio.load()</code> to load the audio file as a tensor and extract the sample rate.</p> <p>We'll also need to convert the returned waveform from stereo sound to mono sound. We can do this by taking the average of the stereo sound channels by using <code>torch.mean()</code>.</p> In\u00a0[\u00a0]: Copied! <pre>#Resulting waveform and sample rate\nwaveform, sample_rate = torchaudio.load(audio_path, normalize=True)\n\n# convert from stereo to mono\nmono_waveform = torch.mean(waveform, dim=0, keepdim=True)\n\n# confirm the waveform is mono\nassert mono_waveform.shape[0] == 1 # mono\n</pre> #Resulting waveform and sample rate waveform, sample_rate = torchaudio.load(audio_path, normalize=True)  # convert from stereo to mono mono_waveform = torch.mean(waveform, dim=0, keepdim=True)  # confirm the waveform is mono assert mono_waveform.shape[0] == 1 # mono <p>Next, we need to resample the mono waveform to the model's sample rate: 16 khz. We can use torchaudio\u2019s resampling API to accomplish this.</p> In\u00a0[\u00a0]: Copied! <pre># Resample the mono waveform to the model's sample rate\nresample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\nresampled_waveform = resample_transform(mono_waveform)\n</pre> # Resample the mono waveform to the model's sample rate resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000) resampled_waveform = resample_transform(mono_waveform) <p>Finally, we can split the resampled waveform into chunks of equal size to feed into the model for easier inference.</p> <p>We'll use <code>torch.split()</code> to split the full resampled waveform into chunks of 30 seconds and a chunk size sample equal to 30 seconds * 16 khz. This step will give us a list of waveforms, <code>chunks</code>, each with 30 seconds of audio data. We will feed each chunk into the model for inference.</p> In\u00a0[\u00a0]: Copied! <pre># Define the desired chunk size\nchunk_size_seconds = 30 \nchunk_size_samples = chunk_size_seconds * 16000\n\n# Split the waveform into chunks of equal size\nchunks = torch.split(resampled_waveform, chunk_size_samples, dim=1)\n</pre> # Define the desired chunk size chunk_size_seconds = 30  chunk_size_samples = chunk_size_seconds * 16000  # Split the waveform into chunks of equal size chunks = torch.split(resampled_waveform, chunk_size_samples, dim=1) In\u00a0[\u00a0]: Copied! <pre>device = 'cpu'\nmodel_name = \"ibm-granite/granite-speech-3.3-8b\"\nspeech_granite_processor = AutoProcessor.from_pretrained(\n    model_name, trust_remote_code=True)\ntokenizer = speech_granite_processor.tokenizer\n</pre> device = 'cpu' model_name = \"ibm-granite/granite-speech-3.3-8b\" speech_granite_processor = AutoProcessor.from_pretrained(     model_name, trust_remote_code=True) tokenizer = speech_granite_processor.tokenizer <p>If you're running your notebook on the watsonx.ai platform, you may also need to run the following code to manually edit the <code>adapter_config.json</code> file. This will avoid an error when loading the model.</p> In\u00a0[\u00a0]: Copied! <pre>adapter_config_file = hf_hub_download(model_name, 'adapter_config.json')\n\n#load the existing config file and print it\nwith open(adapter_config_file, 'r') as file:\n    data = json.load(file)\n\nprint(\"Existing config file:\", data)\n\n\n#remove key, value pairs in config file throwing error\nkeys_to_delete = ['layer_replication', 'loftq_config', 'megatron_config', 'megatron_core', 'use_dora', 'use_rslora']\n\nfor key in keys_to_delete:\n    if key in data:\n        del data[key]\n\n# write the updated config file back to disk\nwith open(adapter_config_file, 'w') as file:\n    json.dump(data, file, indent=4)\n\nwith open(adapter_config_file, 'r') as file:\n    data = json.load(file)\n\n#verify keys were removed\nprint(\"Corrected config file:\", data)\n</pre> adapter_config_file = hf_hub_download(model_name, 'adapter_config.json')  #load the existing config file and print it with open(adapter_config_file, 'r') as file:     data = json.load(file)  print(\"Existing config file:\", data)   #remove key, value pairs in config file throwing error keys_to_delete = ['layer_replication', 'loftq_config', 'megatron_config', 'megatron_core', 'use_dora', 'use_rslora']  for key in keys_to_delete:     if key in data:         del data[key]  # write the updated config file back to disk with open(adapter_config_file, 'w') as file:     json.dump(data, file, indent=4)  with open(adapter_config_file, 'r') as file:     data = json.load(file)  #verify keys were removed print(\"Corrected config file:\", data) <p>Great, now we can finally load the model! We'll use <code>AutoModelForSpeechSeq2Seq</code> from the <code>transformers</code> library and the <code>from_pretrained</code> method to load the model.</p> In\u00a0[\u00a0]: Copied! <pre>speech_granite = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, trust_remote_code=True).to(device)\n</pre> speech_granite = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, trust_remote_code=True).to(device) In\u00a0[\u00a0]: Copied! <pre>chat = [\n    {\n        \"role\": \"system\",\n        \"content\": \"Knowledge Cutoff Date: April 2025.\\nToday's Date: April 16, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"&lt;|audio|&gt;can you transcribe the speech into a written format?\",\n    }\n]\n\ntext = tokenizer.apply_chat_template(\n    chat, tokenize=False, add_generation_prompt=True\n)\n</pre> chat = [     {         \"role\": \"system\",         \"content\": \"Knowledge Cutoff Date: April 2025.\\nToday's Date: April 16, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\",     },     {         \"role\": \"user\",         \"content\": \"&lt;|audio|&gt;can you transcribe the speech into a written format?\",     } ]  text = tokenizer.apply_chat_template(     chat, tokenize=False, add_generation_prompt=True ) <p>Then, we can set up an empty list <code>generated_texts</code>, to gather the generated text from each chunk of audio input.</p> <p>We set up a <code>for</code> loop to iterate through each audio chunk and pass it to the model for generation. Here, we will also track the progress of the loop by using a <code>tqdm</code> progress bar.</p> <p>The model inputs are created through the <code>speech_granite_processor</code> that we established earlier. The processor takes the <code>text</code> and <code>chunk</code> as input and returns a processed version of the audio data for the model to use.</p> <p>The model outputs are produced by using the speech model's <code>generate</code> method. From there, we use the <code>tokenizer</code> to convert the model outputs into human-readable text and store each chunk's transcription into our <code>generated_texts</code> list.</p> In\u00a0[\u00a0]: Copied! <pre>generated_texts = []\n\nfor chunk in tqdm(chunks, desc=\"Generating transcript...\"):\n\n    model_inputs = speech_granite_processor(\n        text,\n        chunk,\n        device=device, # Computation device; returned tensors are put on CPU\n        return_tensors=\"pt\",\n    ).to(device)\n    \n    # Generate\n    model_outputs = speech_granite.generate(\n        **model_inputs,\n        max_new_tokens=1000,\n        num_beams=1,\n        do_sample=False,\n        min_length=1,\n        top_p=1.0,\n        repetition_penalty=1.0,\n        length_penalty=1.0,\n        temperature=1.0,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,)\n\n    num_input_tokens = model_inputs[\"input_ids\"].shape[-1]\n    new_tokens = torch.unsqueeze(model_outputs[0, num_input_tokens:], dim=0)\n\n    output_text = tokenizer.batch_decode(\n        new_tokens, add_special_tokens=False, skip_special_tokens=True)[0]\n\n    generated_texts.append(output_text)\n</pre> generated_texts = []  for chunk in tqdm(chunks, desc=\"Generating transcript...\"):      model_inputs = speech_granite_processor(         text,         chunk,         device=device, # Computation device; returned tensors are put on CPU         return_tensors=\"pt\",     ).to(device)          # Generate     model_outputs = speech_granite.generate(         **model_inputs,         max_new_tokens=1000,         num_beams=1,         do_sample=False,         min_length=1,         top_p=1.0,         repetition_penalty=1.0,         length_penalty=1.0,         temperature=1.0,         bos_token_id=tokenizer.bos_token_id,         eos_token_id=tokenizer.eos_token_id,         pad_token_id=tokenizer.pad_token_id,)      num_input_tokens = model_inputs[\"input_ids\"].shape[-1]     new_tokens = torch.unsqueeze(model_outputs[0, num_input_tokens:], dim=0)      output_text = tokenizer.batch_decode(         new_tokens, add_special_tokens=False, skip_special_tokens=True)[0]      generated_texts.append(output_text) <p>Since the chunk transcripts are currently individual strings in a list, we'll join the strings together with a space in between to make one cohesive full transcript.</p> In\u00a0[\u00a0]: Copied! <pre>full_transcript = \" \".join(generated_texts)\n</pre> full_transcript = \" \".join(generated_texts) In\u00a0[\u00a0]: Copied! <pre>conv = [{\"role\": \"user\", \n         \"content\": f\"Compose a single, unified summary of the following transcript. Your response should only include the unified summary. Do not provide any further explanation. Transcript:{full_transcript}\"}]\n\ntext = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n</pre> conv = [{\"role\": \"user\",           \"content\": f\"Compose a single, unified summary of the following transcript. Your response should only include the unified summary. Do not provide any further explanation. Transcript:{full_transcript}\"}]  text = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True) <p>We'll use <code>speech_granite_processor</code> again to create out model inputs, but we won't pass in any audio file this time.</p> In\u00a0[\u00a0]: Copied! <pre>model_inputs = speech_granite_processor(\n    text,\n    device=device, # Computation device; returned tensors are put on CPU\n    return_tensors=\"pt\",\n).to(device)\n</pre> model_inputs = speech_granite_processor(     text,     device=device, # Computation device; returned tensors are put on CPU     return_tensors=\"pt\", ).to(device) <p>We will receive output from <code>speech_granite.generate()</code> as a tensor. We can convert this output to text by using <code>tokenizer.decode()</code>. And print our final summary!</p> In\u00a0[\u00a0]: Copied! <pre>output = speech_granite.generate(\n    **model_inputs,\n    max_new_tokens= 2000, # concise summary\n)\n\nsummary = tokenizer.decode(output[0, model_inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\nprint(summary)\n</pre> output = speech_granite.generate(     **model_inputs,     max_new_tokens= 2000, # concise summary )  summary = tokenizer.decode(output[0, model_inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True) print(summary)"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#use-automatic-speech-recognition-asr-to-generate-a-podcast-transcript-using-granite-33-and-watsonxai","title":"Use automatic speech recognition (ASR) to generate a podcast transcript using Granite 3.3 and watsonx.ai\u00b6","text":"<p>In this tutorial, you will use the open source IBM\u00ae Granite\u00ae 3.3 speech model to generate an IBM \"Mixture of Experts\" podcast transcript from YouTube. Then, using the open source IBM Granite-3.3-8B-Instruct large language model (LLM), you will output a summary of the generated transcript. You will run this code on a watsonx.ai\u00ae notebook.</p> <p></p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#automatic-speech-recognition","title":"Automatic speech recognition\u00b6","text":"<p>Automatic speech recognition (ASR) also known as speech recognition or speech-to-text, is the technology that converts spoken language into written text. Various machine learning algorithms and artificial intelligence computation techniques are used to convert speech into text. Speech recognition technology has evolved significantly from its beginnings in the mid-twentieth century to today.</p> <p>In the 1960s, spectrograms were initially used to analyze speech. In the subsequent decades, a shift to statistical models occurred. Hidden Markov Models (HMMs) appeared and became dominant for modeling sequences of small sound units known as phonemes in linguistics. ASR systems architecture was made up of three separate components: an acoustic model, a language model and a decoder.</p> <p>By the 2010s, advancements in deep learning began impacting the traditional speech recognition systems architecture. Encoder-decoder models might use a recurrent neural network (RNN) or a convolutional neural network (CNN) architecture where an encoder processes input data and a decoder generates output based on the encoder's representation. Models can be trained on large unlabeled datasets of audio-text pairs to learn how to correspond audio signals with transcriptions. Popular ASR models include DeepSpeech and Wav2Vec.</p> <p>Today, virtual assistants such as Apple\u2019s Siri, Amazon\u2019s Alexa or Microsoft\u2019s Cortana use ASR technology to process real-time human speech. They are also able to integrate speech-to-text with large language models (LLMs) and natural language processing (NLP). LLMs  can be used to add context, which can help when word choices are more ambiguous or if there is variability in human speech patterns.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You need an IBM Cloud\u00ae account to create a watsonx.ai project.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai by using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>Make sure that you choose <code>GPU 2xV100 Runtime 24.1</code> to define the configuration. This step opens a Jupyter Notebook environment where you can copy the code from this tutorial.</p> <p>Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This tutorial is available on GitHub. To view more Granite tutorials, check out the IBM Granite Community.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#step-2-install-and-import-relevant-libraries","title":"Step 2. Install and import relevant libraries\u00b6","text":"<p>We have a few dependencies for this tutorial. Make sure to import the following packages; if they're not installed, you can resolve this issue with a quick pip installation.</p> <p>If you receive a \"pip dependency resolver\" error related to the <code>caikit-nlp</code> package, you can ignore it for now as the rest of the notebook should still be able to run normally.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#step-3-download-the-podcast-audio-from-youtube","title":"Step 3. Download the podcast audio from YouTube\u00b6","text":"<p>In this tutorial, we will use the latest episode of the IBM \"Mixture of Experts\" podcast, \"AI on IBM z17, Meta's Llama 4 and Google Cloud Next 2025\". The podcast is hosted on YouTube. We'll first create a <code>YouTube</code> object and use the <code>streams.filter(only_audio=True)</code> method to capture only the raw audio. From there, we'll extract the audio from the video and save it as an M4A audio file, <code>out_file</code>. <code>base</code> is the full file name, including the directory in which the file will be saved without the <code>m4a</code> extension. We'll use the <code>base</code> variable later when we convert the audio format.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#step-4-prepare-the-podcast-audio-file-for-model-inference","title":"Step 4: Prepare the podcast audio file for model inference\u00b6","text":"<p>We'll need to make a couple of modifications to the podcast audio file before we can use it for model inference.</p> <p>First, we need to convert the M4A file to a WAV file to use it with the Granite Speech model. We will use the moviepy library to do this conversion. We can use the <code>base</code> variable that we defined earlier to create the new file name with the <code>.wav</code> extension.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#step-5-load-and-instantiate-the-granite-speech-model","title":"Step 5: Load and instantiate the Granite speech model\u00b6","text":"<p>Now we can start instantiating our speech model.</p> <p>We will first set our torch device to CPU. If device is set to GPU, you might encounter out of memory errors when running this notebook, but CPU should work just fine on your watsonx.ai notebook. We can then set up our processor and tokenizer for the model.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#step-6-create-an-asr-system-with-the-granite-speech-model","title":"Step 6: Create an ASR system with the Granite speech model\u00b6","text":"<p>Now that we have the model loaded and the audio data prepared, we can use it to generate text from speech.</p> <p>We'll start by creating a prompt for the model to transcribe the audio data. We'll use <code>tokenizer.apply_chat_template()</code> to convert the prompt into a format that can be fed into the model.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#step-7-use-the-granite-instruct-model-for-summarization","title":"Step 7: Use the Granite instruct model for summarization\u00b6","text":"<p>Now that we have a full transcript, we'll use the same model to summarize it. We can access the Granite-3.3-8B-Instruct model directly from Granite-speech-3.3-8b by simply calling it with a text prompt that doesn't contain the <code>&lt;|audio|&gt;</code> token.</p> <p>We'll set up a new prompt to instruct this model to generate a summary of the full transcript. We can use <code>tokenizer.apply_chat_template()</code> again to convert the prompt for model inference.</p>"},{"location":"tutorials/generative-ai/granite-speech-3.3-8b/#conclusion","title":"Conclusion\u00b6","text":"<p>In this tutorial, you downloaded an English audio file from YouTube. You transformed the audio file for consumption by the Granite speech model, generated a full transcript of the audio and used a Granite instruct model to generate a summary of the transcript.</p>"},{"location":"tutorials/generative-ai/graphrag/","title":"Conclusion","text":"<p>Graph retrieval augmented generation (Graph RAG) is emerging as a powerful technique for generative AI applications to use domain-specific knowledge and relevant information. Graph RAG is an alternative to vector search methods that use a vector database. Knowledge graphs are knowledge systems where graph databases such as Neo4j or Amazon Neptune can represent structured data. In a knowledge graph, the relationships between data points, called edges, are as meaningful as the connections between data points, called vertices or sometimes nodes. A knowledge graph makes it easy to traverse a network and process complex queries about connected data. Knowledge graphs are especially well suited for use cases involving chatbots, identity resolution, network analysis, recommendation engines, customer 360 and fraud detection.</p> <p>A Graph RAG approach leverages the structured nature of graph databases to give greater depth and context of retrieved information about networks or complex relationships. When a graph database is paired with a large language model (LLM), a developer can automate significant parts of the graph creation process from unstructured data like text. An LLM can process text data and identify entities, understand their relationships and represent them in a graph structure.</p> <p>There are many ways to create a Graph RAG application, for instance Microsoft\u2019s GraphRAG, or pairing GPT4 with LlamaIndex. For this tutorial you\u2019ll use Memgraph, an open source graph database solution to create a rag system by using Meta\u2019s Llama-3 on watsonx. Memgraph uses Cypher, a declarative query language. It shares some similarities with SQL but focuses on nodes and relationships rather than tables and rows. You\u2019ll have Llama 3 both create and populate your graph database from unstructured text and query information in the database.</p> In\u00a0[3]: Copied! <pre>import os\n \nfrom langchain_community.chains.graph_qa.memgraph import MemgraphQAChain\nfrom langchain_community.graphs import MemgraphGraph\n \nurl = os.environ.get(\"MEMGRAPH_URI\", \"bolt://localhost:7687\")\nusername = os.environ.get(\"MEMGRAPH_USERNAME\", \"\")\npassword = os.environ.get(\"MEMGRAPH_PASSWORD\", \"\")\n\n#initialize memgraph connection\ngraph = MemgraphGraph(\n    url=url, username=username, password=password, refresh_schema=True\n)\n</pre> import os   from langchain_community.chains.graph_qa.memgraph import MemgraphQAChain from langchain_community.graphs import MemgraphGraph   url = os.environ.get(\"MEMGRAPH_URI\", \"bolt://localhost:7687\") username = os.environ.get(\"MEMGRAPH_USERNAME\", \"\") password = os.environ.get(\"MEMGRAPH_PASSWORD\", \"\")  #initialize memgraph connection graph = MemgraphGraph(     url=url, username=username, password=password, refresh_schema=True ) In\u00a0[4]: Copied! <pre>graph_text = \"\"\"\nJohn's title is Director of the Digital Marketing Group. John works with Jane whose title is Chief Marketing Officer. Jane works in the Executive Group. Jane works with Sharon whose title is the Director of Client Outreach. Sharon works in the Sales Group.\n\"\"\"\n</pre> graph_text = \"\"\" John's title is Director of the Digital Marketing Group. John works with Jane whose title is Chief Marketing Officer. Jane works in the Executive Group. Jane works with Sharon whose title is the Director of Client Outreach. Sharon works in the Sales Group. \"\"\" <p>Enter the watsonx API key that you created in the first step:</p> In\u00a0[5]: Copied! <pre>from getpass import getpass\n\nwatsonx_api_key = getpass()\nos.environ[\"WATSONX_APIKEY\"] = watsonx_api_key\n</pre> from getpass import getpass  watsonx_api_key = getpass() os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key <pre> \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[6]: Copied! <pre>watsonx_project_id = getpass()\nos.environ[\"WATSONX_PROJECT_ID\"] = watsonx_project_id\n</pre> watsonx_project_id = getpass() os.environ[\"WATSONX_PROJECT_ID\"] = watsonx_project_id <pre> \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> <p>Now configure a WatsonxLLM instance to generate text. The temperature should be fairly low and the number of tokens high to encourage the model to generate as much detail as possible without hallucinating entities or relationships that aren't present.</p> In\u00a0[8]: Copied! <pre>from langchain_ibm import WatsonxLLM\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n\ngraph_gen_parameters = {\n    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n    GenTextParamsMetaNames.MAX_NEW_TOKENS: 1000,\n    GenTextParamsMetaNames.MIN_NEW_TOKENS: 1,\n    GenTextParamsMetaNames.TEMPERATURE: 0.3,\n    GenTextParamsMetaNames.TOP_K: 10,\n    GenTextParamsMetaNames.TOP_P: 0.8\n}\n\nwatsonx_llm = WatsonxLLM(\n    model_id=\"meta-llama/llama-3-3-70b-instruct\",\n    url=\"https://us-south.ml.cloud.ibm.com\",\n    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n    params=graph_gen_parameters,\n)\n</pre> from langchain_ibm import WatsonxLLM from ibm_watsonx_ai.metanames import GenTextParamsMetaNames  graph_gen_parameters = {     GenTextParamsMetaNames.DECODING_METHOD: \"sample\",     GenTextParamsMetaNames.MAX_NEW_TOKENS: 1000,     GenTextParamsMetaNames.MIN_NEW_TOKENS: 1,     GenTextParamsMetaNames.TEMPERATURE: 0.3,     GenTextParamsMetaNames.TOP_K: 10,     GenTextParamsMetaNames.TOP_P: 0.8 }  watsonx_llm = WatsonxLLM(     model_id=\"meta-llama/llama-3-3-70b-instruct\",     url=\"https://us-south.ml.cloud.ibm.com\",     project_id=os.getenv(\"WATSONX_PROJECT_ID\"),     params=graph_gen_parameters, ) <p>The <code>LLMGraphTransformer</code> allows you to set what kinds of nodes and relationships you'd like the LLM to generate. In your case, the text describes employees at a company, the groups they work in and their job titles. Restricting the LLM to just those entities makes it more likely that you'll get a good representation of the knowledge in a graph.</p> <p>The call to <code>convert_to_graph_documents</code> has the LLMGraphTransformer create a knowledge graph from the text. This step generates the correct Neo4j syntax to insert the information into the graph database to represent the relevant context and relevant entities.</p> In\u00a0[6]: Copied! <pre>from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\nfrom langchain_core.documents import Document\n\nllm_transformer = LLMGraphTransformer(\n    llm=watsonx_llm, \n    allowed_nodes=[\"Person\", \"Title\", \"Group\"],\n    allowed_relationships=[\"TITLE\", \"COLLABORATES\", \"GROUP\"]\n)\ndocuments = [Document(page_content=graph_text)]\ngraph_documents = llm_transformer.convert_to_graph_documents(documents)\n</pre> from langchain_experimental.graph_transformers.llm import LLMGraphTransformer from langchain_core.documents import Document  llm_transformer = LLMGraphTransformer(     llm=watsonx_llm,      allowed_nodes=[\"Person\", \"Title\", \"Group\"],     allowed_relationships=[\"TITLE\", \"COLLABORATES\", \"GROUP\"] ) documents = [Document(page_content=graph_text)] graph_documents = llm_transformer.convert_to_graph_documents(documents) <p>Now clear any old data out of the Memgraph database and insert the new nodes and edges.</p> In\u00a0[7]: Copied! <pre># make sure the database is empty\ngraph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\")\ngraph.query(\"DROP GRAPH\")\ngraph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")\n \n# create knowledge graph\ngraph.add_graph_documents(graph_documents)\n</pre> # make sure the database is empty graph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\") graph.query(\"DROP GRAPH\") graph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")   # create knowledge graph graph.add_graph_documents(graph_documents) <p>The generated Cypher syntax is stored in the <code>graph_documents</code> objects. You can inspect it simply by printing it as a string.</p> In\u00a0[8]: Copied! <pre>print(f\"{graph_documents}\")\n</pre> print(f\"{graph_documents}\") <pre>[GraphDocument(nodes=[Node(id='Director of Client Outreach', type='Title', properties={}), Node(id='Director of the Digital Marketing Group', type='Title', properties={}), Node(id='Sales Group', type='Group', properties={}), Node(id='Sharon', type='Person', properties={}), Node(id='Jane', type='Person', properties={}), Node(id='Chief Marketing Officer', type='Title', properties={}), Node(id='John', type='Person', properties={}), Node(id='Executive Group', type='Group', properties={})], relationships=[Relationship(source=Node(id='John', type='Person', properties={}), target=Node(id='Director of the Digital Marketing Group', type='Title', properties={}), type='TITLE', properties={}), Relationship(source=Node(id='John', type='Person', properties={}), target=Node(id='Jane', type='Person', properties={}), type='COLLABORATES', properties={}), Relationship(source=Node(id='Jane', type='Person', properties={}), target=Node(id='Chief Marketing Officer', type='Title', properties={}), type='TITLE', properties={}), Relationship(source=Node(id='Jane', type='Person', properties={}), target=Node(id='Executive Group', type='Group', properties={}), type='GROUP', properties={}), Relationship(source=Node(id='Jane', type='Person', properties={}), target=Node(id='Sharon', type='Person', properties={}), type='COLLABORATES', properties={}), Relationship(source=Node(id='Sharon', type='Person', properties={}), target=Node(id='Director of Client Outreach', type='Title', properties={}), type='TITLE', properties={}), Relationship(source=Node(id='Sharon', type='Person', properties={}), target=Node(id='Sales Group', type='Group', properties={}), type='GROUP', properties={})], source=Document(metadata={}, page_content=\"\\nJohn's title is Director of the Digital Marketing Group. John works with Jane whose title is Chief Marketing Officer. Jane works in the Executive Group. Jane works with Sharon whose title is the Director of Client Outreach. Sharon works in the Sales Group.\\n\"))]\n</pre> <p>The schema and data types created by the Cypher can be seen in the graphs <code>get_schema</code> property.</p> In\u00a0[9]: Copied! <pre>graph.refresh_schema()\nprint(graph.get_schema)\n</pre> graph.refresh_schema() print(graph.get_schema) <pre>\nNode labels and properties (name and type) are:\n- labels: (:Title)\n  properties:\n    - id: string\n- labels: (:Group)\n  properties:\n    - id: string\n- labels: (:Person)\n  properties:\n    - id: string\n\nNodes are connected with the following relationships:\n(:Person)-[:COLLABORATES]-&gt;(:Person)\n(:Person)-[:GROUP]-&gt;(:Group)\n(:Person)-[:TITLE]-&gt;(:Title)\n\n</pre> <p>You can also see the graph structure in the Memgraph labs viewer:</p> <p></p> <p>The LLM has done a reasonable job of creating the correct nodes and relationships. Now it's time to query the knowledge graph.</p> In\u00a0[10]: Copied! <pre>from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n\nexamples = [\n    {\n        \"question\": \"&lt;|begin_of_text|&gt;What group is Charles in?&lt;|eot_id|&gt;\",\n        \"query\": \"&lt;|begin_of_text|&gt;MATCH (p:Person {{id: 'Charles'}})-[:GROUP]-&gt;(g:Group) RETURN g.id&lt;|eot_id|&gt;\",\n    },\n    {\n        \"question\": \"&lt;|begin_of_text|&gt;Who does Paul work with?&lt;|eot_id|&gt;\",\n        \"query\": \"&lt;|begin_of_text|&gt;MATCH (a:Person {{id: 'Paul'}})-[:COLLABORATES]-&gt;(p:Person) RETURN p.id&lt;|eot_id|&gt;\",\n    },\n        {\n        \"question\": \"What title does Rico have?&lt;|eot_id|&gt;\",\n        \"query\": \"&lt;|begin_of_text|&gt;MATCH (p:Person {{id: 'Rico'}})-[:TITLE]-&gt;(t:Title) RETURN t.id&lt;|eot_id|&gt;\",\n    }\n]\n\nexample_prompt = PromptTemplate.from_template(\n    \"&lt;|begin_of_text|&gt;{query}&lt;|eot_id|&gt;\"\n)\n\nprefix = \"\"\"\nInstructions: \n- Respond with ONE and ONLY ONE query.\n- Use provided node and relationship labels and property names from the\nschema which describes the database's structure. Upon receiving a user\nquestion, synthesize the schema to craft a precise Cypher query that\ndirectly corresponds to the user's intent. \n- Generate valid executable Cypher queries on top of Memgraph database. \nAny explanation, context, or additional information that is not a part \nof the Cypher query syntax should be omitted entirely. \n- Use Memgraph MAGE procedures instead of Neo4j APOC procedures. \n- Do not include any explanations or apologies in your responses. Only answer the question asked.\n- Do not include additional questions. Only the original user question.\n- Do not include any text except the generated Cypher statement.\n- For queries that ask for information or functionalities outside the direct\ngeneration of Cypher queries, use the Cypher query format to communicate\nlimitations or capabilities. For example: RETURN \"I am designed to generate Cypher queries based on the provided schema only.\"\n\nHere is the schema information\n\n{schema}\n\nWith all the above information and instructions, generate Cypher query for the\nuser question. \n\nThe question is:\n\n{question}\n\nBelow are a number of examples of questions and their corresponding Cypher queries.\n\n\"\"\"\n\ncypher_prompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    prefix=prefix,\n    suffix=\"User input: {question}\\nCypher query: \",\n    input_variables=[\"question\", \"schema\"],\n)\n</pre> from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate  examples = [     {         \"question\": \"&lt;|begin_of_text|&gt;What group is Charles in?&lt;|eot_id|&gt;\",         \"query\": \"&lt;|begin_of_text|&gt;MATCH (p:Person {{id: 'Charles'}})-[:GROUP]-&gt;(g:Group) RETURN g.id&lt;|eot_id|&gt;\",     },     {         \"question\": \"&lt;|begin_of_text|&gt;Who does Paul work with?&lt;|eot_id|&gt;\",         \"query\": \"&lt;|begin_of_text|&gt;MATCH (a:Person {{id: 'Paul'}})-[:COLLABORATES]-&gt;(p:Person) RETURN p.id&lt;|eot_id|&gt;\",     },         {         \"question\": \"What title does Rico have?&lt;|eot_id|&gt;\",         \"query\": \"&lt;|begin_of_text|&gt;MATCH (p:Person {{id: 'Rico'}})-[:TITLE]-&gt;(t:Title) RETURN t.id&lt;|eot_id|&gt;\",     } ]  example_prompt = PromptTemplate.from_template(     \"&lt;|begin_of_text|&gt;{query}&lt;|eot_id|&gt;\" )  prefix = \"\"\" Instructions:  - Respond with ONE and ONLY ONE query. - Use provided node and relationship labels and property names from the schema which describes the database's structure. Upon receiving a user question, synthesize the schema to craft a precise Cypher query that directly corresponds to the user's intent.  - Generate valid executable Cypher queries on top of Memgraph database.  Any explanation, context, or additional information that is not a part  of the Cypher query syntax should be omitted entirely.  - Use Memgraph MAGE procedures instead of Neo4j APOC procedures.  - Do not include any explanations or apologies in your responses. Only answer the question asked. - Do not include additional questions. Only the original user question. - Do not include any text except the generated Cypher statement. - For queries that ask for information or functionalities outside the direct generation of Cypher queries, use the Cypher query format to communicate limitations or capabilities. For example: RETURN \"I am designed to generate Cypher queries based on the provided schema only.\"  Here is the schema information  {schema}  With all the above information and instructions, generate Cypher query for the user question.   The question is:  {question}  Below are a number of examples of questions and their corresponding Cypher queries.  \"\"\"  cypher_prompt = FewShotPromptTemplate(     examples=examples,     example_prompt=example_prompt,     prefix=prefix,     suffix=\"User input: {question}\\nCypher query: \",     input_variables=[\"question\", \"schema\"], ) <p>Next, you'll create a prompt to control how the LLM answers the question with the information returned from Memgraph. We'll give the LLM several examples and instructions on how to respond once it has context information back from the graph database.</p> In\u00a0[59]: Copied! <pre>qa_examples = [\n    {\n        \"question\": \"&lt;|begin_of_text|&gt;What group is Charles in?&lt;|eot_id|&gt;\",\n        \"context\": \"[{{'g.id': 'Executive Group'}}]\",\n        \"response\": \"Charles is in the Executive Group&lt;|eot_id|&gt;\"\n    },\n    {\n        \"question\": \"&lt;|begin_of_text|&gt;Who does Paul work with?&lt;|eot_id|&gt;\",\n        \"context\": \"[{{'p.id': 'Greg'}}, {{'p2.id': 'Norma'}}]\",\n        \"response\": \"Paul works with Greg and Norma&lt;|eot_id|&gt;\"\n    },\n        {\n        \"question\": \"What title does Rico have?&lt;|eot_id|&gt;\",\n        \"context\": \"[{{'t.id': 'Vice President of Sales'}}]\",\n        \"response\": \"Vice President of Sales&lt;|eot_id|&gt;\"\n    }\n]\n\nqa_template = \"\"\"\nUse the provided question and context to create an answer.\n\nQuestion: {question}\n\nContext: {context}\n\nUse only names departments or titles contained within {question} and {context}.\n\"\"\"\n\nqa_example_prompt = PromptTemplate.from_template(\n    \"\"\n)\n\nqa_prompt = FewShotPromptTemplate(\n    examples=qa_examples,\n    prefix=qa_template,\n    input_variables=[\"question\", \"context\"],\n    example_prompt=qa_example_prompt,\n    suffix=\" \"\n)\n</pre> qa_examples = [     {         \"question\": \"&lt;|begin_of_text|&gt;What group is Charles in?&lt;|eot_id|&gt;\",         \"context\": \"[{{'g.id': 'Executive Group'}}]\",         \"response\": \"Charles is in the Executive Group&lt;|eot_id|&gt;\"     },     {         \"question\": \"&lt;|begin_of_text|&gt;Who does Paul work with?&lt;|eot_id|&gt;\",         \"context\": \"[{{'p.id': 'Greg'}}, {{'p2.id': 'Norma'}}]\",         \"response\": \"Paul works with Greg and Norma&lt;|eot_id|&gt;\"     },         {         \"question\": \"What title does Rico have?&lt;|eot_id|&gt;\",         \"context\": \"[{{'t.id': 'Vice President of Sales'}}]\",         \"response\": \"Vice President of Sales&lt;|eot_id|&gt;\"     } ]  qa_template = \"\"\" Use the provided question and context to create an answer.  Question: {question}  Context: {context}  Use only names departments or titles contained within {question} and {context}. \"\"\"  qa_example_prompt = PromptTemplate.from_template(     \"\" )  qa_prompt = FewShotPromptTemplate(     examples=qa_examples,     prefix=qa_template,     input_variables=[\"question\", \"context\"],     example_prompt=qa_example_prompt,     suffix=\" \" ) <p>Now it's time to create the question answering chain. The <code>MemgraphQAChain</code> allows you to set which LLM you'd like to use, the graph schema to be used and information about debugging. Using a temperature of 0 and a length penalty encourages the LLM to keep the Cypher prompt short and straightforward.</p> In\u00a0[60]: Copied! <pre>query_gen_parameters = {\n    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n    GenTextParamsMetaNames.MAX_NEW_TOKENS: 100,\n    GenTextParamsMetaNames.MIN_NEW_TOKENS: 1,\n    GenTextParamsMetaNames.TEMPERATURE: 0.0,\n    GenTextParamsMetaNames.TOP_K: 1,\n    GenTextParamsMetaNames.TOP_P: 0.9,\n    GenTextParamsMetaNames.LENGTH_PENALTY: {'decay_factor': 1.2, 'start_index': 20}\n}\n\nchain = MemgraphQAChain.from_llm(\n    llm = WatsonxLLM(\n        model_id=\"meta-llama/llama-3-3-70b-instruct\",\n        url=\"https://us-south.ml.cloud.ibm.com\",\n        project_id=\"dfe8787b-1f6f-4e18-b36a-e22c00f141d1\",\n        params=query_gen_parameters\n    ),\n    graph = graph,\n    allow_dangerous_requests = True,\n    verbose = True,\n    return_intermediate_steps = True, # for debugging\n    cypher_prompt=cypher_prompt,\n    qa_prompt=qa_prompt\n)\n</pre> query_gen_parameters = {     GenTextParamsMetaNames.DECODING_METHOD: \"sample\",     GenTextParamsMetaNames.MAX_NEW_TOKENS: 100,     GenTextParamsMetaNames.MIN_NEW_TOKENS: 1,     GenTextParamsMetaNames.TEMPERATURE: 0.0,     GenTextParamsMetaNames.TOP_K: 1,     GenTextParamsMetaNames.TOP_P: 0.9,     GenTextParamsMetaNames.LENGTH_PENALTY: {'decay_factor': 1.2, 'start_index': 20} }  chain = MemgraphQAChain.from_llm(     llm = WatsonxLLM(         model_id=\"meta-llama/llama-3-3-70b-instruct\",         url=\"https://us-south.ml.cloud.ibm.com\",         project_id=\"dfe8787b-1f6f-4e18-b36a-e22c00f141d1\",         params=query_gen_parameters     ),     graph = graph,     allow_dangerous_requests = True,     verbose = True,     return_intermediate_steps = True, # for debugging     cypher_prompt=cypher_prompt,     qa_prompt=qa_prompt ) <p>Now you can invoke the chain with a natural language question (note that your responses might be slightly different because LLMs are not purely deterministic).</p> In\u00a0[23]: Copied! <pre>chain.invoke(\"What is Johns title?\")\n</pre> chain.invoke(\"What is Johns title?\") <pre>\n\n&gt; Entering new MemgraphQAChain chain...\nGenerated Cypher:\n MATCH (p:Person {id: 'John'})-[:TITLE]-&gt;(t:Title) RETURN t.id\nFull Context:\n[{'t.id': 'Director of the Digital Marketing Group'}]\n\n&gt; Finished chain.\n</pre> Out[23]: <pre>{'query': 'What is Johns title?',\n 'result': ' \\nAnswer: Director of the Digital Marketing Group.',\n 'intermediate_steps': [{'query': \" MATCH (p:Person {id: 'John'})-[:TITLE]-&gt;(t:Title) RETURN t.id\"},\n  {'context': [{'t.id': 'Director of the Digital Marketing Group'}]}]}</pre> <p>In the next question, ask the chain a slightly more complex question:</p> In\u00a0[24]: Copied! <pre>chain.invoke(\"Who does John collaborate with?\")\n</pre> chain.invoke(\"Who does John collaborate with?\") <pre>\n\n&gt; Entering new MemgraphQAChain chain...\nGenerated Cypher:\n MATCH (p:Person {id: 'John'})-[:COLLABORATES]-&gt;(c:Person) RETURN c\nFull Context:\n[{'c': {'id': 'Jane'}}]\n\n&gt; Finished chain.\n</pre> Out[24]: <pre>{'query': 'Who does John collaborate with?',\n 'result': ' \\nAnswer: John collaborates with Jane.',\n 'intermediate_steps': [{'query': \" MATCH (p:Person {id: 'John'})-[:COLLABORATES]-&gt;(c:Person) RETURN c\"},\n  {'context': [{'c': {'id': 'Jane'}}]}]}</pre> <p>The correct answer is contained in the response. In some cases there may be extra text that you would want to remove before returning the answer to an end user.</p> <p>You can ask the Memgraph chain about Group relationships:</p> In\u00a0[51]: Copied! <pre>chain.invoke(\"What group is Jane in?\")\n</pre> chain.invoke(\"What group is Jane in?\") <pre>\n\n&gt; Entering new MemgraphQAChain chain...\nGenerated Cypher:\n MATCH (p:Person {id: 'Jane'})-[:GROUP]-&gt;(g:Group) RETURN g.id\nFull Context:\n[{'g.id': 'Executive Group'}]\n\n&gt; Finished chain.\n</pre> Out[51]: <pre>{'query': 'What group is Jane in?',\n 'result': 'Jane is in Executive Group.',\n 'intermediate_steps': [{'query': \" MATCH (p:Person {id: 'Jane'})-[:GROUP]-&gt;(g:Group) RETURN g.id\"},\n  {'context': [{'g.id': 'Executive Group'}]}]}</pre> <p>This is the correct answer.</p> <p>Finally, ask the chain a question with two outputs:</p> In\u00a0[62]: Copied! <pre>chain.invoke(\"Who does Jane collaborate with?\")\n</pre> chain.invoke(\"Who does Jane collaborate with?\") <pre>\n\n&gt; Entering new MemgraphQAChain chain...\nGenerated Cypher:\n MATCH (p:Person {id: 'Jane'})-[:COLLABORATES]-&gt;(c:Person) RETURN c\nFull Context:\n[{'c': {'id': 'Sharon'}}]\n\n&gt; Finished chain.\n</pre> Out[62]: <pre>{'query': 'Who does Jane collaborate with?',\n 'result': ' Jane collaborates with Sharon.',\n 'intermediate_steps': [{'query': \" MATCH (p:Person {id: 'Jane'})-[:COLLABORATES]-&gt;(c:Person) RETURN c\"},\n  {'context': [{'c': {'id': 'Sharon'}}]}]}</pre> <p>The chain correctly identifies both of the collaborators.</p>"},{"location":"tutorials/generative-ai/graphrag/#step-1","title":"Step 1\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <p>Log in to watsonx.ai\u2122 using your IBM Cloud\u00ae account.</p> <p>Create a watsonx.ai project.</p> <p>You get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this Project ID for this tutorial.</p> <p>Next, associate your project with the watsonx.ai Runtime</p> <p>a.  Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> <p>b.  Generate an API Key in watsonx.ai Runtime. Save this API key for use in this tutorial.</p> <p>c.  Go to your project and select the Manage tab</p> <p>d.  In the left tab, select Services and Integrations</p> <p>e.  Select IBM services</p> <p>f.  Select Associate service and pick watsonx.ai Runtime.</p> <p>g.  Associate the watsonx.ai Runtime to the project that you created in watsonx.ai</p>"},{"location":"tutorials/generative-ai/graphrag/#step-2","title":"Step 2\u00b6","text":"<p>Now, you'll need to install Docker from https://www.docker.com/products/docker-desktop/</p> <p>Once you've installed Docker, install Memgraph using their Docker container. On OSX or Linux, you can use this command in a terminal:</p> <pre><code>curl https://install.memgraph.com | sh</code></pre> <p>On a Windows computer use:</p> <pre><code>iwr https://windows.memgraph.com | iex</code></pre> <p>Follow the installation steps to get the Memgraph engine and Memgraph lab up and running.</p>"},{"location":"tutorials/generative-ai/graphrag/#step-3","title":"Step 3\u00b6","text":"<p>On your computer, create a fresh virtualenv for this project:</p> <pre><code>virtualenv kg_rag --python=python3.12</code></pre> <p>In the Python environment for your notebook, install the following Python libraries:</p> <pre><code>./kg_rag/bin/pip install langchain langchain-openai langchain_experimental langchain-community==0.3.15 neo4j langchain_ibm jupyterlab json-repair getpass4</code></pre> <p>Now you're ready to connect to Memgraph.</p>"},{"location":"tutorials/generative-ai/graphrag/#step-4","title":"Step 4\u00b6","text":"<p>If you've configured Memgraph to use a username and password, set them here, otherwise you can use the defaults of having neither. It's not good practice for a production database but for a local development environment that doesn't store sensitive data, it's not an issue.</p>"},{"location":"tutorials/generative-ai/graphrag/#step-5","title":"Step 5\u00b6","text":"<p>Now create a sample string that describes a dataset of relationships that you can use to test the graph generating capabilities of your LLM system. You can use more complex data sources but this simple example helps us demonstrate the algorithm.</p>"},{"location":"tutorials/generative-ai/graphrag/#step-6","title":"Step 6\u00b6","text":"<p>Prompting the LLM correctly requires some prompt engineering. LangChain provides a FewShotPromptTemplate that can be used to give examples to the LLM in the prompt to ensure that it writes correct and succinct Cypher syntax. The following code gives several examples of questions and queries that the LLM should use. It also shows constraining the output of the model to only the query. An overly chatty LLM might add in extra information that would lead to invalid Cypher queries, so the prompt template asks the model to output only the query itself.</p> <p>Adding an instructive prefix also helps to constrain the model behavior and makes it more likely that the LLM will output correct Cypher syntax.</p>"},{"location":"tutorials/generative-ai/graphrag/#conclusion","title":"Conclusion\u00b6","text":"<p>In this tutorial, you built a Graph RAG application using Memgraph and watsonx to generate the graph data structures and query them. Using an LLM through watsonx you extracted node and edge information from natural language source text and generated Cypher query syntax to populate a graph database. You then used watsonx to turn natural language questions about that source text into Cypher queries that extracted information from the graph database. Using prompt engineering the LLM turned the results from the Memgraph database into natural language responses.</p>"},{"location":"tutorials/generative-ai/langchain-rag/","title":"Create a LangChain RAG system in Python with watsonx","text":"<p>Contributors: Nicholas Moss, Erika Russi</p> <p>In this tutorial, we\u2019ll use LangChain to walk through a step-by-step simple Retrieval Augmented Generation (RAG) example in Python. RAG is a technique in natural language processing (NLP) that combines information retrieval and generative models to produce more accurate, relevant and contextually aware responses.</p> <p>For our use case, we\u2019ll set up a RAG system for various IBM webpages related to the company's technology, products and offerings. The fetched content from these ibm.com websites will make up our knowledge base. From this knowledge base, we will then provide context to an LLM so it can answer some questions about IBM products.</p> In\u00a0[\u00a0]: Copied! <pre>#installations\n%pip install -q python-dotenv\n%pip install -q langchain\n%pip install -q langchain_chroma\n%pip install -q langchain-community\n%pip install -qU langchain_ibm\n%pip install -qU langchain_community beautifulsoup4\n%pip install -q ibm-watson-machine-learning\n</pre> #installations %pip install -q python-dotenv %pip install -q langchain %pip install -q langchain_chroma %pip install -q langchain-community %pip install -qU langchain_ibm %pip install -qU langchain_community beautifulsoup4 %pip install -q ibm-watson-machine-learning <p>Import the relevant libraries:</p> In\u00a0[\u00a0]: Copied! <pre>#imports\nimport os\nfrom dotenv import load_dotenv\n\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n\nfrom langchain_ibm import WatsonxEmbeddings, WatsonxLLM\nfrom langchain.vectorstores import Chroma\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\nfrom langchain_community.document_loaders import WebBaseLoader\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n</pre> #imports import os from dotenv import load_dotenv  from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes  from langchain_ibm import WatsonxEmbeddings, WatsonxLLM from langchain.vectorstores import Chroma  from langchain_core.prompts import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnablePassthrough  from langchain_community.document_loaders import WebBaseLoader  from langchain_text_splitters import RecursiveCharacterTextSplitter  <p>Set up your credentials. Please store your <code>WATSONX_PROJECT_ID</code> and <code>WATSONX_APIKEY</code> in a separate <code>.env</code> file in the same level of your directory as this notebook.</p> In\u00a0[\u00a0]: Copied! <pre>load_dotenv(os.getcwd()+\"/.env\", override=True)\n\ncredentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": os.getenv(\"WATSONX_APIKEY\", \"\"),\n}\n\nproject_id = os.getenv(\"WATSONX_PROJECT_ID\", \"\")\n</pre> load_dotenv(os.getcwd()+\"/.env\", override=True)  credentials = {     \"url\": \"https://us-south.ml.cloud.ibm.com\",     \"apikey\": os.getenv(\"WATSONX_APIKEY\", \"\"), }  project_id = os.getenv(\"WATSONX_PROJECT_ID\", \"\") In\u00a0[\u00a0]: Copied! <pre>URLS_DICTIONARY = {\n    \"ufc_ibm_partnership\": \"https://newsroom.ibm.com/2024-11-14-ufc-names-ibm-as-first-ever-official-ai-partner\",\n    \"granite.html\": \"https://www.ibm.com/granite\",\n    \"products_watsonx_ai.html\": \"https://www.ibm.com/products/watsonx-ai\",\n    \"products_watsonx_ai_foundation_models.html\": \"https://www.ibm.com/products/watsonx-ai/foundation-models\",\n    \"watsonx_pricing.html\": \"https://www.ibm.com/watsonx/pricing\",\n    \"watsonx.html\": \"https://www.ibm.com/watsonx\",\n    \"products_watsonx_data.html\": \"https://www.ibm.com/products/watsonx-data\",\n    \"products_watsonx_assistant.html\": \"https://www.ibm.com/products/watsonx-assistant\",\n    \"products_watsonx_code_assistant.html\": \"https://www.ibm.com/products/watsonx-code-assistant\",\n    \"products_watsonx_orchestrate.html\": \"https://www.ibm.com/products/watsonx-orchestrate\",\n    \"products_watsonx_governance.html\": \"https://www.ibm.com/products/watsonx-governance\",\n    \"granite_code_models_open_source.html\": \"https://research.ibm.com/blog/granite-code-models-open-source\",\n    \"red_hat_enterprise_linux_ai.html\": \"https://www.redhat.com/en/about/press-releases/red-hat-delivers-accessible-open-source-generative-ai-innovation-red-hat-enterprise-linux-ai\",\n    \"model_choice.html\": \"https://www.ibm.com/blog/announcement/enterprise-grade-model-choices/\",\n    \"democratizing.html\": \"https://www.ibm.com/blog/announcement/democratizing-large-language-model-development-with-instructlab-support-in-watsonx-ai/\",\n    \"ibm_consulting_expands_ai.html\": \"https://newsroom.ibm.com/Blog-IBM-Consulting-Expands-Capabilities-to-Help-Enterprises-Scale-AI\",\n    \"ibm_data_product_hub.html\": \"https://www.ibm.com/products/data-product-hub\",\n    \"ibm_price_performance_data.html\": \"https://www.ibm.com/blog/announcement/delivering-superior-price-performance-and-enhanced-data-management-for-ai-with-ibm-watsonx-data/\",\n    \"ibm_bi_adoption.html\": \"https://www.ibm.com/blog/a-new-era-in-bi-overcoming-low-adoption-to-make-smart-decisions-accessible-for-all/\",\n    \"code_assistant_for_java.html\": \"https://www.ibm.com/blog/announcement/watsonx-code-assistant-java/\",\n    \"accelerating_gen_ai.html\": \"https://newsroom.ibm.com/Blog-How-IBM-Cloud-is-Accelerating-Business-Outcomes-with-Gen-AI\",\n    \"watsonx_open_source.html\": \"https://newsroom.ibm.com/2024-05-21-IBM-Unveils-Next-Chapter-of-watsonx-with-Open-Source,-Product-Ecosystem-Innovations-to-Drive-Enterprise-AI-at-Scale\",\n    \"ibm_concert.html\": \"https://www.ibm.com/products/concert\",\n    \"ibm_consulting_advantage_news.html\": \"https://newsroom.ibm.com/2024-01-17-IBM-Introduces-IBM-Consulting-Advantage,-an-AI-Services-Platform-and-Library-of-Assistants-to-Empower-Consultants\",\n    \"ibm_consulting_advantage_info.html\": \"https://www.ibm.com/consulting/info/ibm-consulting-advantage\"\n}\nCOLLECTION_NAME = \"askibm_2024\"\n</pre> URLS_DICTIONARY = {     \"ufc_ibm_partnership\": \"https://newsroom.ibm.com/2024-11-14-ufc-names-ibm-as-first-ever-official-ai-partner\",     \"granite.html\": \"https://www.ibm.com/granite\",     \"products_watsonx_ai.html\": \"https://www.ibm.com/products/watsonx-ai\",     \"products_watsonx_ai_foundation_models.html\": \"https://www.ibm.com/products/watsonx-ai/foundation-models\",     \"watsonx_pricing.html\": \"https://www.ibm.com/watsonx/pricing\",     \"watsonx.html\": \"https://www.ibm.com/watsonx\",     \"products_watsonx_data.html\": \"https://www.ibm.com/products/watsonx-data\",     \"products_watsonx_assistant.html\": \"https://www.ibm.com/products/watsonx-assistant\",     \"products_watsonx_code_assistant.html\": \"https://www.ibm.com/products/watsonx-code-assistant\",     \"products_watsonx_orchestrate.html\": \"https://www.ibm.com/products/watsonx-orchestrate\",     \"products_watsonx_governance.html\": \"https://www.ibm.com/products/watsonx-governance\",     \"granite_code_models_open_source.html\": \"https://research.ibm.com/blog/granite-code-models-open-source\",     \"red_hat_enterprise_linux_ai.html\": \"https://www.redhat.com/en/about/press-releases/red-hat-delivers-accessible-open-source-generative-ai-innovation-red-hat-enterprise-linux-ai\",     \"model_choice.html\": \"https://www.ibm.com/blog/announcement/enterprise-grade-model-choices/\",     \"democratizing.html\": \"https://www.ibm.com/blog/announcement/democratizing-large-language-model-development-with-instructlab-support-in-watsonx-ai/\",     \"ibm_consulting_expands_ai.html\": \"https://newsroom.ibm.com/Blog-IBM-Consulting-Expands-Capabilities-to-Help-Enterprises-Scale-AI\",     \"ibm_data_product_hub.html\": \"https://www.ibm.com/products/data-product-hub\",     \"ibm_price_performance_data.html\": \"https://www.ibm.com/blog/announcement/delivering-superior-price-performance-and-enhanced-data-management-for-ai-with-ibm-watsonx-data/\",     \"ibm_bi_adoption.html\": \"https://www.ibm.com/blog/a-new-era-in-bi-overcoming-low-adoption-to-make-smart-decisions-accessible-for-all/\",     \"code_assistant_for_java.html\": \"https://www.ibm.com/blog/announcement/watsonx-code-assistant-java/\",     \"accelerating_gen_ai.html\": \"https://newsroom.ibm.com/Blog-How-IBM-Cloud-is-Accelerating-Business-Outcomes-with-Gen-AI\",     \"watsonx_open_source.html\": \"https://newsroom.ibm.com/2024-05-21-IBM-Unveils-Next-Chapter-of-watsonx-with-Open-Source,-Product-Ecosystem-Innovations-to-Drive-Enterprise-AI-at-Scale\",     \"ibm_concert.html\": \"https://www.ibm.com/products/concert\",     \"ibm_consulting_advantage_news.html\": \"https://newsroom.ibm.com/2024-01-17-IBM-Introduces-IBM-Consulting-Advantage,-an-AI-Services-Platform-and-Library-of-Assistants-to-Empower-Consultants\",     \"ibm_consulting_advantage_info.html\": \"https://www.ibm.com/consulting/info/ibm-consulting-advantage\" } COLLECTION_NAME = \"askibm_2024\" <p>Next, let's load our documents using the LangChain <code>WebBaseLoader</code> for the list of URLs we have. Loaders load in data from a source and return a list of Documents. A Document is an object with some page_content (str) and metadata (dict). We'll print the <code>page_content</code> of a sample document at the end to see how it's been loaded.</p> In\u00a0[\u00a0]: Copied! <pre>documents = []\n\nfor url in list(URLS_DICTIONARY.values()):\n    loader = WebBaseLoader(url)\n    data = loader.load()\n    documents += data\n\n# #show sample document\ndocuments[0].page_content\n\n#Output:\n</pre> documents = []  for url in list(URLS_DICTIONARY.values()):     loader = WebBaseLoader(url)     data = loader.load()     documents += data  # #show sample document documents[0].page_content  #Output: <p>Based on the sample document, it looks like there's a lot of white space and new line characters that we can get rid of. Let's clean that up and add some metadata to our documents, including an id number and the source of the content.</p> In\u00a0[\u00a0]: Copied! <pre>for doc in documents:\n    doc.page_content = \" \".join(doc.page_content.split()) # remove white space\n</pre> for doc in documents:     doc.page_content = \" \".join(doc.page_content.split()) # remove white space  <p>Let's see how our sample document looks now after we cleaned it up:</p> In\u00a0[\u00a0]: Copied! <pre>documents[0].page_content\n</pre> documents[0].page_content <p>We need to split up our text into smaller, more manageable pieces known as \"chunks\". LangChain's <code>RecursiveCharacterTextSplitter</code> takes a large text and splits it based on a specified chunk size using a predefined set of characters. In order, the default characters are:</p> <ul> <li>\"\\n\\n\" - two new line characters</li> <li>\"\\n\" - one new line character</li> <li>\" \" - a space</li> <li>\"\" - an empty character</li> </ul> <p>The process starts by attempting to split the text using the first character, \"\\n\\n.\" If the resulting chunks are still too large, it moves to the next character, \"\\n,\" and tries splitting again. This continues with each character in the set until the chunks are smaller than the specified chunk size. Since we already removed all the \"\\n\\n\" and \"\\n\" characters when we cleaned up the text, the <code>RecursiveCharacterTextSplitter</code> will begin at the \" \"(space) character.</p> <p>We settled on a chunk size of 512 after experimenting with a chunk size of 1000. When the chunks were that large, our model was getting too much context for question-answering; this led to confused responses by the LLM because it was receiving too much information, so we changed it to smaller chunks. Feel free to experiment with chunk size further!</p> In\u00a0[\u00a0]: Copied! <pre>text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n</pre> text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0) docs = text_splitter.split_documents(documents) <p>Next, we choose an embedding model to be trained on our ibm.com dataset. The trained embedding model is used to generate embeddings for each data point in the dataset. For text data, popular open-source embedding models include Word2Vec, GloVe, FastText or pre-trained transformer-based models like BERT or RoBERTa. OpenAIembeddings may also be used by leveraging the OpenAI embeddings API endpoint, the <code>langchain_openai</code> package and getting an <code>openai_api_key</code>, however, there is a cost associated with this usage.</p> <p>Unfortunately, because the embedding models are so large, vector embedding often demands significant computational resources. We can greatly lower the costs linked to embedding vectors, while preserving performance and accuracy by using WatsonxEmbeddings. We'll use the IBM embeddings model, Slate, an encoder-only (RoBERTa-based) model, which while not generative, is fast and effective for many NLP tasks.</p> In\u00a0[\u00a0]: Copied! <pre>embeddings = WatsonxEmbeddings(\n    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n    url=credentials[\"url\"],# type: ignore\n    apikey=credentials[\"apikey\"],# type: ignore\n    project_id=project_id,\n    )\n</pre> embeddings = WatsonxEmbeddings(     model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,     url=credentials[\"url\"],# type: ignore     apikey=credentials[\"apikey\"],# type: ignore     project_id=project_id,     ) <p>Let's load our content into a local instance of a vector database, using Chroma.</p> In\u00a0[\u00a0]: Copied! <pre>vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)\n</pre> vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings) In\u00a0[\u00a0]: Copied! <pre>retriever = vectorstore.as_retriever()\n</pre> retriever = vectorstore.as_retriever() In\u00a0[\u00a0]: Copied! <pre>model_id = \"ibm/granite-3-8b-instruct\"\n</pre> model_id = \"ibm/granite-3-8b-instruct\" <p>The model parameters available can be found here. We experimented with various model parameters, including Temperature, Top P, and Top K. Here's some more information on model parameters and what they mean.</p> In\u00a0[\u00a0]: Copied! <pre>parameters = {\n    GenParams.DECODING_METHOD: 'greedy',\n    GenParams.TEMPERATURE: 2,\n    GenParams.TOP_P: 0,\n    GenParams.TOP_K: 100,\n    GenParams.MIN_NEW_TOKENS: 10,\n    GenParams.MAX_NEW_TOKENS: 512,\n    GenParams.REPETITION_PENALTY:1.2,\n    GenParams.RETURN_OPTIONS: {'input_tokens': True,'generated_tokens': True, 'token_logprobs': True, 'token_ranks': True, }\n}\n</pre> parameters = {     GenParams.DECODING_METHOD: 'greedy',     GenParams.TEMPERATURE: 2,     GenParams.TOP_P: 0,     GenParams.TOP_K: 100,     GenParams.MIN_NEW_TOKENS: 10,     GenParams.MAX_NEW_TOKENS: 512,     GenParams.REPETITION_PENALTY:1.2,     GenParams.RETURN_OPTIONS: {'input_tokens': True,'generated_tokens': True, 'token_logprobs': True, 'token_ranks': True, } } <p>Next, we instantiate the LLM.</p> In\u00a0[\u00a0]: Copied! <pre>llm = WatsonxLLM(\n    model_id=model_id,\n    url=credentials.get(\"url\"), # type: ignore\n    apikey=credentials.get(\"apikey\"), # type: ignore\n    project_id=project_id,\n    params=parameters\n)\n</pre> llm = WatsonxLLM(     model_id=model_id,     url=credentials.get(\"url\"), # type: ignore     apikey=credentials.get(\"apikey\"), # type: ignore     project_id=project_id,     params=parameters ) <p>We'll set up a <code>prompttemplate</code> to ask multiple questions. The \"context\" will be derived from our retriever (our vector database) with the relevant documents and the \"question\" will be derived from the user query.</p> In\u00a0[\u00a0]: Copied! <pre>template = \"\"\"Generate a summary of the context that answers the question. Explain the answer in multiple steps if possible. \nAnswer style should match the context. Ideal Answer Length 2-3 sentences.\\n\\n{context}\\nQuestion: {question}\\nAnswer:\n\"\"\"\nprompt = ChatPromptTemplate.from_template(template)\n</pre> template = \"\"\"Generate a summary of the context that answers the question. Explain the answer in multiple steps if possible.  Answer style should match the context. Ideal Answer Length 2-3 sentences.\\n\\n{context}\\nQuestion: {question}\\nAnswer: \"\"\" prompt = ChatPromptTemplate.from_template(template) <p>Let's set up a helper function to format the docs accordingly:</p> In\u00a0[\u00a0]: Copied! <pre>def format_docs(docs):\n    return \"\\n\\n\".join([d.page_content for d in docs])\n</pre> def format_docs(docs):     return \"\\n\\n\".join([d.page_content for d in docs]) <p>And now we can set up a chain with our context, our prompt and our LLM. The generative model processes the augmented context along with the user's question to produce an LLM-powered response.</p> In\u00a0[\u00a0]: Copied! <pre>rag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n</pre> rag_chain = (     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}     | prompt     | llm     | StrOutputParser() ) <p>And now we can ask multiple questions:</p> In\u00a0[\u00a0]: Copied! <pre>rag_chain.invoke(\"Tell me about the UFC announcement from November 14, 2024\")\n</pre> rag_chain.invoke(\"Tell me about the UFC announcement from November 14, 2024\") <p>Let's ask about watsonx.data next.</p> In\u00a0[\u00a0]: Copied! <pre>rag_chain.invoke(\"What is watsonx.data?\")\n</pre> rag_chain.invoke(\"What is watsonx.data?\") <p>And finally, let's ask about watsonx.ai.</p> In\u00a0[\u00a0]: Copied! <pre>rag_chain.invoke(\"What does watsonx.ai do?\")\n</pre> rag_chain.invoke(\"What does watsonx.ai do?\") <p>And that's it! Feel free to ask even more questions!</p>"},{"location":"tutorials/generative-ai/langchain-rag/#create-a-langchain-rag-system-in-python-with-watsonx","title":"Create a LangChain RAG system in Python with watsonx\u00b6","text":""},{"location":"tutorials/generative-ai/langchain-rag/#more-about-rag-and-langchain","title":"More about RAG and LangChain\u00b6","text":"<p>In traditional language generation tasks, large language models (LLMs) like OpenAI\u2019s GPT-3.5 (Generative Pre-trained Transformer) or IBM\u2019s Granite Models are used to construct responses based on an input prompt. However, these models may struggle to produce responses that are contextually relevant, factually accurate or up to date. The models may not know the latest information on IBM products. To tackle this knowledge gap, we can use methods such as fine-tuning or continued pre-training, but both can be expensive. Instead, we can use RAG to leverage a knowledge base of existing content.</p> <p>RAG applications address the knowledge gap limitation by incorporating a retrieval step before response generation. During retrieval, vector search can be used to identify contextually pertinent information, such as relevant information or documents from a large corpus of text, typically stored in a vector database. Finally, an LLM is used to generate a response based on the retrieved context.</p> <p>LangChain is a powerful, open-source framework that facilitates the development of applications using LLMs for various NLP tasks. In the context of RAG, LangChain plays a critical role by combining the strengths of retrieval-based methods and generative models to enhance the capabilities of NLP systems.</p>"},{"location":"tutorials/generative-ai/langchain-rag/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You need an IBM Cloud account to create a watsonx.ai project.</p>"},{"location":"tutorials/generative-ai/langchain-rag/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/langchain-rag/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on Github.</p>"},{"location":"tutorials/generative-ai/langchain-rag/#step-2-set-up-a-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up a watsonx.ai Runtime instance and API key.\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (select your appropriate region and choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/langchain-rag/#step-3-install-and-import-relevant-libraries-and-set-up-credentials","title":"Step 3. Install and import relevant libraries and set up credentials\u00b6","text":"<p>We have a few dependencies for this tutorial. Make sure to import the libraries below, and if they're not installed, you can resolve this with a quick pip install.</p>"},{"location":"tutorials/generative-ai/langchain-rag/#step-4-index-the-urls-to-create-the-knowledge-base","title":"Step 4. Index the URLs to create the knowledge base\u00b6","text":"<p>We\u2019ll index our ibm.com articles from URLs to create a knowledge base as a vectorstore. The content from these URLs will be our data sources and context for this exercise. The context will then be provided to an LLM to answer any questions we have about IBM products or technologies.</p> <p>The first step to building vector embeddings is to clean and process the raw dataset. This may involve the removal of noise and standardization of the text. For our example, we won\u2019t do any cleaning since the text is already cleaned and standardized.</p> <p>First, let's establish <code>URLS_DICTIONARY</code>. <code>URLS_DICTIONARY</code> is a dict that helps us map the URLs from which we will be extracting the content. Let's also set up a name for our collection: <code>askibm_2024</code>.</p>"},{"location":"tutorials/generative-ai/langchain-rag/#step-5-set-up-a-retriever","title":"Step 5. Set up a retriever\u00b6","text":"<p>We'll set up our vector store as a retriever. The retrieved information from the vector store serves as additional context or knowledge that can be used by a generative model.</p>"},{"location":"tutorials/generative-ai/langchain-rag/#step-6-generate-a-response-with-a-generative-model","title":"Step 6. Generate a response with a Generative Model\u00b6","text":"<p>Finally, we\u2019ll generate a response. The generative model (like GPT-4 or IBM Granite) uses the retrieved information to produce a more accurate and contextually relevant response to our questions.</p> <p>First, we'll establish which LLM we're going to use to generate the response. For this tutorial, we'll use IBM's Granite-3.0-8B-Instruct model.</p>"},{"location":"tutorials/generative-ai/langchain-rag/#summary-and-next-steps","title":"Summary and next steps\u00b6","text":"<p>In this tutorial, you created a simple LangChain RAG workflow in Python with watsonx. You fetched 25 articles from ibm.com to create a vector store as context for an LLM to answer questions about IBM offerings and technologies.</p> <p>You can imagine a situation where we can create chatbots to field these questions.</p> <p>We encourage you to check out the LangChain documentation page for more information and tutorials on RAG.</p>"},{"location":"tutorials/generative-ai/langchain-rag/#try-watsonx-for-free","title":"Try watsonx for free\u00b6","text":"<p>Build an AI strategy for your business on one collaborative AI and data platform called IBM watsonx, which brings together new generative AI capabilities, powered by foundation models, and traditional machine learning into a powerful platform spanning the AI lifecycle. With watsonx.ai, you can train, validate, tune, and deploy models with ease and build AI applications in a fraction of the time with a fraction of the data.</p> <p>Try watsonx.ai, the next-generation studio for AI builders.</p>"},{"location":"tutorials/generative-ai/langchain-rag/#next-steps","title":"Next steps\u00b6","text":"<p>Explore more\u202farticles and tutorials about watsonx\u202fon IBM Developer.</p>"},{"location":"tutorials/generative-ai/langchain-tools/","title":"LangChain tool calling using Granite-3.0-8B-Instruct in Python with watsonx.ai","text":"In\u00a0[\u00a0]: Copied! <pre>#installations\n%pip install -q langchain \\\n    \"langchain_community&lt;0.3.0\" \\\n    langgraph \\\n    youtube_search \\\n    pyowm \\\n    ionic-langchain\n    \n%pip install -qU langchain-ibm\n</pre> #installations %pip install -q langchain \\     \"langchain_community&lt;0.3.0\" \\     langgraph \\     youtube_search \\     pyowm \\     ionic-langchain      %pip install -qU langchain-ibm In\u00a0[\u00a0]: Copied! <pre>#imports\nimport getpass\n\nfrom langchain_ibm import ChatWatsonx\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.tools import Tool\nfrom langchain_core.messages import HumanMessage\nfrom langchain_community.tools import YouTubeSearchTool\nfrom langchain_community.utilities import OpenWeatherMapAPIWrapper\nfrom ionic_langchain.tool import IonicTool\n</pre> #imports import getpass  from langchain_ibm import ChatWatsonx from langgraph.prebuilt import create_react_agent from langchain_core.tools import Tool from langchain_core.messages import HumanMessage from langchain_community.tools import YouTubeSearchTool from langchain_community.utilities import OpenWeatherMapAPIWrapper from ionic_langchain.tool import IonicTool <p>To set our credentials, we will need the <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> you generated in step 1. We will also set the URL serving as the API endpoint.</p> In\u00a0[\u00a0]: Copied! <pre>WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://us-south.ml.cloud.ibm.com\"\n</pre> WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://us-south.ml.cloud.ibm.com\" <p>The weather tool used in this tutorial with require an OpenWeather API key. To generate one, create an OpenWeather account. Upon creating an account, select the \"API Keys\" tab to display your free key.</p> In\u00a0[\u00a0]: Copied! <pre>OPENWEATHERMAP_API_KEY = getpass.getpass(\"Please enter your OPENWEATHERMAP_API_KEY (hit enter): \")\n</pre> OPENWEATHERMAP_API_KEY = getpass.getpass(\"Please enter your OPENWEATHERMAP_API_KEY (hit enter): \") In\u00a0[\u00a0]: Copied! <pre>llm = ChatWatsonx(\n    model_id=\"ibm/granite-3-8b-instruct\",\n    url = URL,\n\tapikey = WATSONX_APIKEY,\n\tproject_id = WATSONX_PROJECT_ID,\n    params = {\n        \"decoding_method\": \"greedy\",\n        \"temperature\": 0, \n        \"min_new_tokens\": 5, \n        \"max_new_tokens\": 2000\n    }\n)\n</pre> llm = ChatWatsonx(     model_id=\"ibm/granite-3-8b-instruct\",     url = URL, \tapikey = WATSONX_APIKEY, \tproject_id = WATSONX_PROJECT_ID,     params = {         \"decoding_method\": \"greedy\",         \"temperature\": 0,          \"min_new_tokens\": 5,          \"max_new_tokens\": 2000     } ) In\u00a0[\u00a0]: Copied! <pre>weather = OpenWeatherMapAPIWrapper(openweathermap_api_key=OPENWEATHERMAP_API_KEY)\n\nweather_search = Tool(\n    name=\"weather_search\",\n    description=\"Get weather for a city and country code, e.g. Athens, GR\",\n    func=weather.run,\n)\n</pre> weather = OpenWeatherMapAPIWrapper(openweathermap_api_key=OPENWEATHERMAP_API_KEY)  weather_search = Tool(     name=\"weather_search\",     description=\"Get weather for a city and country code, e.g. Athens, GR\",     func=weather.run, ) <p>Next, let's set up the pre-built YouTube tool using the YouTube Search package available through LangChain. This will be helpful for finding videos about your travel destination.</p> In\u00a0[\u00a0]: Copied! <pre>youtube = YouTubeSearchTool()\n\nyoutube_search = Tool(\n    name=\"youtube_search\",\n    description=\"Search YouTube for video links.\",\n    func=youtube.run,\n)\n</pre> youtube = YouTubeSearchTool()  youtube_search = Tool(     name=\"youtube_search\",     description=\"Search YouTube for video links.\",     func=youtube.run, ) <p>Lastly, let's set up an online shopping tool using Ionic. This tool returns items relevant to the user query that are on sale through the ecommerce marketplace.</p> In\u00a0[\u00a0]: Copied! <pre>ionic_search = IonicTool().tool()\n</pre> ionic_search = IonicTool().tool() <p>Let's set our list of the multiple tools we provide to the LLM. We can also print the tool list to see how they loaded. For an extended list of available LangChain tools, refer to the LangChain documentation.</p> In\u00a0[\u00a0]: Copied! <pre>tools = [weather_search, youtube_search, ionic_search]\ntools\n</pre> tools = [weather_search, youtube_search, ionic_search] tools Out[\u00a0]: <pre>[Tool(name='weather_search', description='Get weather for a city and country code, e.g. Athens, GR', func=&lt;bound method OpenWeatherMapAPIWrapper.run of OpenWeatherMapAPIWrapper(owm=&lt;pyowm.owm.OWM - API key=************************611450cc, subscription type=free, PyOWM version=(3, 3, 0)&gt;, openweathermap_api_key=None)&gt;),\n Tool(name='youtube_search', description='Search YouTube for video links.', func=&lt;bound method BaseTool.run of YouTubeSearchTool()&gt;),\n Tool(name='ionic_commerce_shopping_tool', description='\\nIonic is an e-commerce shopping tool. Assistant uses the Ionic Commerce Shopping Tool to find, discover, and compare products from thousands of online retailers. Assistant should use the tool when the user is looking for a product recommendation or trying to find a specific product. \\n\\nThe user may specify the number of results, minimum price, and maximum price for which they want to see results.\\nIonic Tool input is a comma-separated string of values:\\n  - query string (required, must not include commas)\\n  - number of results (default to 4, no more than 10)\\n  - minimum price in cents ($5 becomes 500)\\n  - maximum price in cents\\nFor example, if looking for coffee beans between 5 and 10 dollars, the tool input would be `coffee beans, 5, 500, 1000`.\\n\\nPresent product results as a list, always including the name of the product with the provided link to purchase. Other provided information can be included as relevant to the request, including price, merchant name, etc.\\n', verbose=True, func=&lt;bound method Ionic.query of &lt;ionic_langchain.tool.Ionic object at 0x31af01450&gt;&gt;)]</pre> In\u00a0[\u00a0]: Copied! <pre>llm_with_tools = llm.bind_tools(tools)\nresponse = llm_with_tools.invoke([\n\t\t(\"human\", \"What are some youtube videos about greece\"),\n\t])\nresponse\n</pre> llm_with_tools = llm.bind_tools(tools) response = llm_with_tools.invoke([ \t\t(\"human\", \"What are some youtube videos about greece\"), \t]) response Out[\u00a0]: <pre>AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-446bafd1e13f48ec9e19b6a8173df71b', 'type': 'function', 'function': {'name': 'youtube_search', 'arguments': '{\"query\": \"greece\"}'}}]}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 578, 'total_tokens': 599}, 'model_name': 'ibm/granite-3-8b-instruct', 'system_fingerprint': '', 'finish_reason': 'tool_calls'}, id='chat-40607fd279fa491888a9e9eac7158cb7', tool_calls=[{'name': 'youtube_search', 'args': {'query': 'greece'}, 'id': 'chatcmpl-tool-446bafd1e13f48ec9e19b6a8173df71b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 578, 'output_tokens': 21, 'total_tokens': 599})</pre> In\u00a0[\u00a0]: Copied! <pre>response.additional_kwargs\n</pre> response.additional_kwargs Out[\u00a0]: <pre>{'tool_calls': [{'id': 'chatcmpl-tool-da46f7a30e894001b202f4318d40857e',\n   'type': 'function',\n   'function': {'name': 'youtube_search',\n    'arguments': '{\"query\": \"greece\"}'}}]}</pre> <p>As seen in the <code>tool_calls</code> output, the LLM correctly identifies the appropriate tool call and arguments. The LLM does not run the tool itself. We will do this in the next step.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>agent_executor = create_react_agent(llm, tools)\n</pre> agent_executor = create_react_agent(llm, tools) <p>We are now able to ask the agent questions that require tool calling. First, we can ask the model to return URLs to YouTube videos about Greece. We can use the <code>HumanMessage</code> class to pass the user query to the LLM.</p> In\u00a0[\u00a0]: Copied! <pre>user_query = \"What are some YouTube videos about Greece\"\nresponse = agent_executor.invoke({\"messages\": user_query})\nresponse[\"messages\"]\n</pre> user_query = \"What are some YouTube videos about Greece\" response = agent_executor.invoke({\"messages\": user_query}) response[\"messages\"] Out[\u00a0]: <pre>[HumanMessage(content='What are some YouTube videos about Greece', additional_kwargs={}, response_metadata={}, id='d6e37827-e49f-4597-a2e6-6b90f44e8dda'),\n AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-3d8464695a0443daabcd41ac7f2f4c1c', 'type': 'function', 'function': {'name': 'youtube_search', 'arguments': '{\"query\": \"Greece\"}'}}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 578, 'total_tokens': 600}, 'model_name': 'ibm/granite-3-8b-instruct', 'system_fingerprint': '', 'finish_reason': 'tool_calls'}, id='chat-4ebd2bf368584e388b271c358b16c6f6', tool_calls=[{'name': 'youtube_search', 'args': {'query': 'Greece'}, 'id': 'chatcmpl-tool-3d8464695a0443daabcd41ac7f2f4c1c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 578, 'output_tokens': 22, 'total_tokens': 600}),\n ToolMessage(content=\"['https://www.youtube.com/watch?v=NMlBB2pK5qo&amp;pp=ygUGR3JlZWNl', 'https://www.youtube.com/watch?v=waQY2Ucpbd0&amp;pp=ygUGR3JlZWNl']\", name='youtube_search', id='2ce5eff3-8ca5-4acc-9590-0c7a4823cbe0', tool_call_id='chatcmpl-tool-3d8464695a0443daabcd41ac7f2f4c1c'),\n AIMessage(content='Here are some YouTube videos about Greece:\\n\\n1. [Greece Travel Guide | Top 10 Tourist Attractions](https://www.youtube.com/watch?v=NMlBB2pK5qo&amp;pp=ygUGR3JlZWNl)\\n2. [Greece Travel Guide | Top 10 Tourist Attractions](https://www.youtube.com/watch?v=waQY2Ucpbd0&amp;pp=ygUGR3JlZWNl)', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 677, 'total_tokens': 794}, 'model_name': 'ibm/granite-3-8b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-dd0479c62a914ba389fc65fad971178a', usage_metadata={'input_tokens': 677, 'output_tokens': 117, 'total_tokens': 794})]</pre> <p>Great! As seen in the <code>AIMessage</code>, the model correctly identified the appropriate tool call. In the <code>ToolMessage</code>, we see the model successfully returned the expected output by using the built-in LangChain YouTube tool. Finally, the <code>AIMessage</code> shows the LLM synthesized the tool response.</p> <p>Next, let's ask the model about the weather in Greece to determine whether it calls the <code>weather_search</code> tool as expected.</p> In\u00a0[\u00a0]: Copied! <pre>user_query = \"What is the weather in Athens, GR\"\nresponse = agent_executor.invoke({\"messages\": user_query})\nresponse[\"messages\"]\n</pre> user_query = \"What is the weather in Athens, GR\" response = agent_executor.invoke({\"messages\": user_query}) response[\"messages\"] Out[\u00a0]: <pre>[HumanMessage(content='What is the weather in Athens, GR', additional_kwargs={}, response_metadata={}, id='a0c4b69c-988a-4f7d-9b8a-4780305f8e2a'),\n AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-9a0c07a3b35f4c69a351c5540ab663f8', 'type': 'function', 'function': {'name': 'weather_search', 'arguments': '{\"__arg1\": \"Athens, GR\"}'}}]}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 579, 'total_tokens': 605}, 'model_name': 'ibm/granite-3-8b-instruct', 'system_fingerprint': '', 'finish_reason': 'tool_calls'}, id='chat-eeed087050e049f0ad715f3615c7fdda', tool_calls=[{'name': 'weather_search', 'args': {'__arg1': 'Athens, GR'}, 'id': 'chatcmpl-tool-9a0c07a3b35f4c69a351c5540ab663f8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 579, 'output_tokens': 26, 'total_tokens': 605}),\n ToolMessage(content='In Athens, GR, the current weather is as follows:\\nDetailed status: few clouds\\nWind speed: 4.47 m/s, direction: 23\u00b0\\nHumidity: 58%\\nTemperature: \\n  - Current: 15.15\u00b0C\\n  - High: 15.74\u00b0C\\n  - Low: 14.1\u00b0C\\n  - Feels like: 14.24\u00b0C\\nRain: {}\\nHeat index: None\\nCloud cover: 20%', name='weather_search', id='587b0230-b667-41de-97b9-3779554d2559', tool_call_id='chatcmpl-tool-9a0c07a3b35f4c69a351c5540ab663f8'),\n AIMessage(content='The current weather in Athens, GR is:\\n- Detailed status: few clouds\\n- Wind speed: 4.47 m/s, direction: 23\u00b0\\n- Humidity: 58%\\n- Temperature:\\n  - Current: 15.15\u00b0C\\n  - High: 15.74\u00b0C\\n  - Low: 14.1\u00b0C\\n  - Feels like: 14.24\u00b0C\\n- Rain: None\\n- Heat index: None\\n- Cloud cover: 20%', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 125, 'prompt_tokens': 733, 'total_tokens': 858}, 'model_name': 'ibm/granite-3-8b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-6719a5ca266a439bb10ed410db25c5ef', usage_metadata={'input_tokens': 733, 'output_tokens': 125, 'total_tokens': 858})]</pre> <p>The model was able to discern the appropriate tool to call, run the tool with the extracted arguments as well as synthesize the tool output. Now, let's ask the LLM for some suitcases under USD100 for your upcoming trip. Note, the tool is designed to search for prices in cents. Hence, we ask for suitcases under 10,000 cents in this scenario, equivalent to USD 100.</p> In\u00a0[\u00a0]: Copied! <pre>user_query = \"Find some suitcases for less than 10000\"\nresponse = agent_executor.invoke({\"messages\": user_query})\nresponse[\"messages\"]\n</pre> user_query = \"Find some suitcases for less than 10000\" response = agent_executor.invoke({\"messages\": user_query}) response[\"messages\"] <pre>[{'products': [{'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://go.ionic.click/Ch4CKd'}], 'merchant_name': 'Walmart', 'merchant_product_id': '811277349', 'name': 'Zimtown Hardside Lightweight Spinner Orange 3 Piece Luggage Set with TSA Lock', 'price': '$69.99', 'status': 'available', 'thumbnail': 'https://i5.walmartimages.com/asr/b809a274-ccc7-4ca4-b4f1-e848b4412fe6.314144bcd13e5467a33cb99e8dd5237c.jpeg?odnHeight=100&amp;odnWidth=100&amp;odnBg=ffffff', 'brand_name': 'Zimtown', 'upc': '273109526768'}, {'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://www.amazon.com/dp/B071HHX6VF?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1'}], 'merchant_name': 'Amazon', 'merchant_product_id': 'B071HHX6VF', 'name': 'Amazon Basics Expandable Hardside Luggage, Suitcase with Wheels, 30-Inch Spinner with Four Spinner Wheels and Scratch-Resistant Surface, Black', 'price': '$74.49', 'status': 'available', 'thumbnail': 'https://m.media-amazon.com/images/I/41jJcuMYSdL._SL160_.jpg', 'brand_name': 'Amazon Basics', 'upc': '841710177190'}, {'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://www.amazon.com/dp/B01MFBVKDF?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1'}], 'merchant_name': 'Amazon', 'merchant_product_id': 'B01MFBVKDF', 'name': 'SwissGear Sion Softside Expandable Luggage, Blue, Carry-On 21-Inch', 'price': '$80.73', 'status': 'available', 'thumbnail': 'https://m.media-amazon.com/images/I/41v29IgcrrL._SL160_.jpg', 'brand_name': 'SwissGear', 'upc': '721427015520'}, {'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://www.amazon.com/dp/B07RS4PK3J?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1'}], 'merchant_name': 'Amazon', 'merchant_product_id': 'B07RS4PK3J', 'name': 'Travelers Club Midtown Hardside Luggage Travel, Rose Gold, 4-Piece Set', 'price': '$95.19', 'status': 'available', 'thumbnail': 'https://m.media-amazon.com/images/I/51kEMDfsoAL._SL160_.jpg', 'brand_name': 'Travelers Club', 'upc': '015272777383'}, {'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://www.amazon.com/dp/B0B2X1BDFH?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1'}], 'merchant_name': 'Amazon', 'merchant_product_id': 'B0B2X1BDFH', 'name': 'American Tourister Stratum 2.0 Expandable Hardside Luggage with Spinner Wheels, 28\" SPINNER, Slate Blue', 'price': '$89.97', 'status': 'available', 'thumbnail': 'https://m.media-amazon.com/images/I/412Wew2UWXL._SL160_.jpg', 'brand_name': 'American Tourister', 'upc': '049845308278'}, {'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://www.amazon.com/dp/B07SLG6WZ2?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1'}], 'merchant_name': 'Amazon', 'merchant_product_id': 'B07SLG6WZ2', 'name': 'Wrangler Smart Luggage Set with Cup Holder and USB Port, Navy Blue, 20-Inch Carry-On', 'price': '$39.99', 'status': 'available', 'thumbnail': 'https://m.media-amazon.com/images/I/310Y7qPw5QL._SL160_.jpg', 'brand_name': 'Wrangler', 'upc': '015272782967'}, {'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://www.amazon.com/dp/B0C7YWMBGP?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1'}], 'merchant_name': 'Amazon', 'merchant_product_id': 'B0C7YWMBGP', 'name': 'Wrangler Hardside Carry-On Spinner Luggage, Lilac, 20-Inch', 'price': '$40.00', 'status': 'available', 'thumbnail': 'https://m.media-amazon.com/images/I/41UqPdOf9PL._SL160_.jpg', 'brand_name': 'Wrangler', 'upc': '015272802092'}, {'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://go.ionic.click/qJqBRA'}], 'merchant_name': 'Walmart', 'merchant_product_id': '334291289', 'name': 'Protege 20 inch Hard Side Carry-On Spinner Luggage, Black Matte Finish (Walmart.com Exclusive)', 'price': '$29.87', 'status': 'available', 'thumbnail': 'https://i5.walmartimages.com/asr/ea913edb-834c-44ae-ae94-cf80da2257fe.c93baa773f7fc05dc952bb49c840c640.jpeg?odnHeight=100&amp;odnWidth=100&amp;odnBg=ffffff', 'brand_name': 'Protege', 'upc': '618842416387'}, {'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://www.amazon.com/dp/B07XKWMLJ5?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1'}], 'merchant_name': 'Amazon', 'merchant_product_id': 'B07XKWMLJ5', 'name': 'Wrangler Wesley Rolling Duffel Bag, Tannin, Large 30-Inch', 'price': '$44.00', 'status': 'available', 'thumbnail': 'https://m.media-amazon.com/images/I/41fnMz0c4AL._SL160_.jpg', 'brand_name': 'Wrangler', 'upc': '015272783803'}, {'links': [{'text': 'Details', 'type': 'pdp', 'url': 'https://www.amazon.com/dp/B085B4D852?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1'}], 'merchant_name': 'Amazon', 'merchant_product_id': 'B085B4D852', 'name': 'U.S. Traveler Boren Polycarbonate Hardside Rugged Travel Suitcase Luggage with 8 Spinner Wheels, Aluminum Handle, Lavender, Checked-Large 30-Inch', 'price': '$79.99', 'status': 'available', 'thumbnail': 'https://m.media-amazon.com/images/I/31Q3qRdm8aL._SL160_.jpg', 'brand_name': 'U.S. Traveler', 'upc': '694396091775'}], 'query': {'query': 'suitcases', 'max_price': 10000, 'min_price': 0, 'num_results': 10}}]</pre> <pre>[HumanMessage(content='Find some suitcases for less than 10000', additional_kwargs={}, response_metadata={}, id='8b207035-150a-4390-aff3-8b09ef85a592'),\n AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'chatcmpl-tool-b011e718b18e41dcbcae2f7786af263d', 'type': 'function', 'function': {'name': 'ionic_commerce_shopping_tool', 'arguments': '{\"__arg1\": \"suitcases, 10, 0, 10000\"}'}}]}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 582, 'total_tokens': 623}, 'model_name': 'ibm/granite-3-8b-instruct', 'system_fingerprint': '', 'finish_reason': 'tool_calls'}, id='chat-e38c8568d1754636a6a92082561180bd', tool_calls=[{'name': 'ionic_commerce_shopping_tool', 'args': {'__arg1': 'suitcases, 10, 0, 10000'}, 'id': 'chatcmpl-tool-b011e718b18e41dcbcae2f7786af263d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 582, 'output_tokens': 41, 'total_tokens': 623}),\n ToolMessage(content='[{\"products\": [{\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://go.ionic.click/Ch4CKd\"}], \"merchant_name\": \"Walmart\", \"merchant_product_id\": \"811277349\", \"name\": \"Zimtown Hardside Lightweight Spinner Orange 3 Piece Luggage Set with TSA Lock\", \"price\": \"$69.99\", \"status\": \"available\", \"thumbnail\": \"https://i5.walmartimages.com/asr/b809a274-ccc7-4ca4-b4f1-e848b4412fe6.314144bcd13e5467a33cb99e8dd5237c.jpeg?odnHeight=100&amp;odnWidth=100&amp;odnBg=ffffff\", \"brand_name\": \"Zimtown\", \"upc\": \"273109526768\"}, {\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://www.amazon.com/dp/B071HHX6VF?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1\"}], \"merchant_name\": \"Amazon\", \"merchant_product_id\": \"B071HHX6VF\", \"name\": \"Amazon Basics Expandable Hardside Luggage, Suitcase with Wheels, 30-Inch Spinner with Four Spinner Wheels and Scratch-Resistant Surface, Black\", \"price\": \"$74.49\", \"status\": \"available\", \"thumbnail\": \"https://m.media-amazon.com/images/I/41jJcuMYSdL._SL160_.jpg\", \"brand_name\": \"Amazon Basics\", \"upc\": \"841710177190\"}, {\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://www.amazon.com/dp/B01MFBVKDF?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1\"}], \"merchant_name\": \"Amazon\", \"merchant_product_id\": \"B01MFBVKDF\", \"name\": \"SwissGear Sion Softside Expandable Luggage, Blue, Carry-On 21-Inch\", \"price\": \"$80.73\", \"status\": \"available\", \"thumbnail\": \"https://m.media-amazon.com/images/I/41v29IgcrrL._SL160_.jpg\", \"brand_name\": \"SwissGear\", \"upc\": \"721427015520\"}, {\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://www.amazon.com/dp/B07RS4PK3J?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1\"}], \"merchant_name\": \"Amazon\", \"merchant_product_id\": \"B07RS4PK3J\", \"name\": \"Travelers Club Midtown Hardside Luggage Travel, Rose Gold, 4-Piece Set\", \"price\": \"$95.19\", \"status\": \"available\", \"thumbnail\": \"https://m.media-amazon.com/images/I/51kEMDfsoAL._SL160_.jpg\", \"brand_name\": \"Travelers Club\", \"upc\": \"015272777383\"}, {\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://www.amazon.com/dp/B0B2X1BDFH?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1\"}], \"merchant_name\": \"Amazon\", \"merchant_product_id\": \"B0B2X1BDFH\", \"name\": \"American Tourister Stratum 2.0 Expandable Hardside Luggage with Spinner Wheels, 28\\\\\" SPINNER, Slate Blue\", \"price\": \"$89.97\", \"status\": \"available\", \"thumbnail\": \"https://m.media-amazon.com/images/I/412Wew2UWXL._SL160_.jpg\", \"brand_name\": \"American Tourister\", \"upc\": \"049845308278\"}, {\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://www.amazon.com/dp/B07SLG6WZ2?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1\"}], \"merchant_name\": \"Amazon\", \"merchant_product_id\": \"B07SLG6WZ2\", \"name\": \"Wrangler Smart Luggage Set with Cup Holder and USB Port, Navy Blue, 20-Inch Carry-On\", \"price\": \"$39.99\", \"status\": \"available\", \"thumbnail\": \"https://m.media-amazon.com/images/I/310Y7qPw5QL._SL160_.jpg\", \"brand_name\": \"Wrangler\", \"upc\": \"015272782967\"}, {\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://www.amazon.com/dp/B0C7YWMBGP?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1\"}], \"merchant_name\": \"Amazon\", \"merchant_product_id\": \"B0C7YWMBGP\", \"name\": \"Wrangler Hardside Carry-On Spinner Luggage, Lilac, 20-Inch\", \"price\": \"$40.00\", \"status\": \"available\", \"thumbnail\": \"https://m.media-amazon.com/images/I/41UqPdOf9PL._SL160_.jpg\", \"brand_name\": \"Wrangler\", \"upc\": \"015272802092\"}, {\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://go.ionic.click/qJqBRA\"}], \"merchant_name\": \"Walmart\", \"merchant_product_id\": \"334291289\", \"name\": \"Protege 20 inch Hard Side Carry-On Spinner Luggage, Black Matte Finish (Walmart.com Exclusive)\", \"price\": \"$29.87\", \"status\": \"available\", \"thumbnail\": \"https://i5.walmartimages.com/asr/ea913edb-834c-44ae-ae94-cf80da2257fe.c93baa773f7fc05dc952bb49c840c640.jpeg?odnHeight=100&amp;odnWidth=100&amp;odnBg=ffffff\", \"brand_name\": \"Protege\", \"upc\": \"618842416387\"}, {\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://www.amazon.com/dp/B07XKWMLJ5?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1\"}], \"merchant_name\": \"Amazon\", \"merchant_product_id\": \"B07XKWMLJ5\", \"name\": \"Wrangler Wesley Rolling Duffel Bag, Tannin, Large 30-Inch\", \"price\": \"$44.00\", \"status\": \"available\", \"thumbnail\": \"https://m.media-amazon.com/images/I/41fnMz0c4AL._SL160_.jpg\", \"brand_name\": \"Wrangler\", \"upc\": \"015272783803\"}, {\"links\": [{\"text\": \"Details\", \"type\": \"pdp\", \"url\": \"https://www.amazon.com/dp/B085B4D852?tag=ioniccommer00-20&amp;linkCode=osi&amp;th=1&amp;psc=1\"}], \"merchant_name\": \"Amazon\", \"merchant_product_id\": \"B085B4D852\", \"name\": \"U.S. Traveler Boren Polycarbonate Hardside Rugged Travel Suitcase Luggage with 8 Spinner Wheels, Aluminum Handle, Lavender, Checked-Large 30-Inch\", \"price\": \"$79.99\", \"status\": \"available\", \"thumbnail\": \"https://m.media-amazon.com/images/I/31Q3qRdm8aL._SL160_.jpg\", \"brand_name\": \"U.S. Traveler\", \"upc\": \"694396091775\"}], \"query\": {\"query\": \"suitcases\", \"max_price\": 10000, \"min_price\": 0, \"num_results\": 10}}]', name='ionic_commerce_shopping_tool', id='a38baab3-e85f-4f5a-b056-0430de8dbf22', tool_call_id='chatcmpl-tool-b011e718b18e41dcbcae2f7786af263d'),\n AIMessage(content='Here are some suitcases that are less than 10000:\\n\\n1. [Zimtown Hardside Lightweight Spinner Orange 3 Piece Luggage Set with TSA Lock](https://go.ionic.click/Ch4CKd) - $69.99\\n2. [Amazon Basics Expandable Hardside Luggage, Suitcase with Wheels, 30-Inch Spinner](https://www.amazon.com/dp/B071HHX6VF) - $74.49\\n3. [SwissGear Sion Softside Expandable Luggage, Blue, Carry-On 21-Inch](https://www.amazon.com/dp/B01MFBVKDF) - $80.73\\n4. [Travelers Club Midtown Hardside Luggage Travel, Rose Gold, 4-Piece Set](https://www.amazon.com/dp/B07RS4PK3J) - $95.19\\n5. [American Tourister Stratum 2.0 Expandable Hardside Luggage with Spinner Wheels, 28\" SPINNER, Slate Blue](https://www.amazon.com/dp/B0B2X1BDFH) - $89.97\\n6. [Wrangler Smart Luggage Set with Cup Holder and USB Port, Navy Blue, 20-Inch Carry-On](https://www.amazon.com/dp/B07SLG6WZ2) - $39.99\\n7. [Wrangler Hardside Carry-On Spinner Luggage, Lilac, 20-Inch](https://www.amazon.com/dp/B0C7YWMBGP) - $40.00\\n8. [Protege 20 inch Hard Side Carry-On Spinner Luggage, Black Matte Finish (Walmart.com Exclusive)](https://go.ionic.click/qJqBRA) - $29.87\\n9. [Wrangler Wesley Rolling Duffel Bag, Tannin, Large 30-Inch](https://www.amazon.com/dp/B07XKWMLJ5) - $44.00\\n10. [U.S. Traveler Boren Polycarbonate Hardside Rugged Travel Suitcase Luggage with 8 Spinner Wheels, Aluminum Handle, Lavender, Checked-Large 30-Inch](https://www.amazon.com/dp/B085B4D852) - $79.99', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 612, 'prompt_tokens': 2794, 'total_tokens': 3406}, 'model_name': 'ibm/granite-3-8b-instruct', 'system_fingerprint': '', 'finish_reason': 'stop'}, id='chat-d08201ff6ef84c428e7ae44372396926', usage_metadata={'input_tokens': 2794, 'output_tokens': 612, 'total_tokens': 3406})]</pre> <p>As seen by the LLM response, the model correctly used the shopping tool to return several suitcases for purchase online under USD 100.</p> <p>In this tutorial, you used prebuilt LangChain tools to create a ReAct agent in Python with watsonx using the <code>granite-3-8b-instruct</code> model. You used the <code>youtube_search</code>, <code>weather_search</code> and <code>ionic_search</code> tools. The tutorial showed how to implement traditional tool calling as well as an agentic approach that runs the tools. The sample output is important as it shows the steps the agent took in creating its own agent workflow by using the functions available. The tools granted to the agent were vital for answering user queries.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#langchain-tool-calling-using-granite-30-8b-instruct-in-python-with-watsonxai","title":"LangChain tool calling using Granite-3.0-8B-Instruct in Python with watsonx.ai\u00b6","text":"<p>Author: Anna Gutowska</p> <p>In this tutorial, we will use prebuilt LangChain tools for an agentic ReAct agent to showcase its ability to differentiate appropriate use cases for each tool. We will primarily be using the opensource LangChain Python package.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#what-is-tool-calling","title":"What is tool calling?\u00b6","text":"<p>Tool calling, otherwise known as function calling, is the interface that allows AI agents to work on specific tasks that require up-to-date information, otherwise unavailable to the trained large language models (LLMs). LLMs such as IBM\u00ae Granite\u2122 models or OpenAI\u2019s GPT (generative pre-trained transformer) models have access only to information used in training. There are many default tools accessible through LangChain including a tool for working with SQL database queries, accessing information on Wikipedia and much more. We encourage you to read the LangChain documentation for a comprehensive list of prebuilt tools.</p> <p>Custom tools can be defined by using various methods including using the @tool decorator or LangChain Runnables which we'll cover in this tutorial. Async tools can be created by using the StructuredTool or the BaseTool classes. For distinctions between each approach, we encourage you to reference the official LangChain documentation. Refer to the IBM function calling tutorial for examples of custom tools.</p> <p>We encourage you to check out our AI Agents explainer for an in-depth overview of the various AI agent types and how they differ from traditional LLM chatbots.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You need an IBM Cloud\u00ae account.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#steps","title":"Steps\u00b6","text":"<p>Check out this YouTube video that walks you through the following set up instructions in steps 1 and 2.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai\u2122 using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> <p>This step will open a Jupyter Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on Github.</p> </li> </ol>"},{"location":"tutorials/generative-ai/langchain-tools/#step-2-set-up-a-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up a watsonx.ai Runtime instance and API key.\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (select your appropriate region and choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service instance to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/langchain-tools/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We need a few libraries and modules for this tutorial. Make sure to import the following ones and if they're not installed, you can resolve this with a quick pip installation. LangChain and LangGraph will be the frameworks and developer toolkits used.</p> <p>Note: This tutorial was built using Python 3.11.9 and is also compatible with Google Colab which uses Python 3.10.12. To check your python version, you can run the <code>!python --version</code> command in a code cell.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#step-4-initialize-the-llm","title":"Step 4. Initialize the LLM\u00b6","text":"<p>For this tutorial, we will be using the ChatWatsonx wrapper to set our chat model. This wrapper simplifies the integration of tool calling and chaining. We will be using the <code>granite-3-8b-instruct</code> model. We encourage you to use the API references in the ChatWatsonx documentation for further information.</p> <p>To initialize the LLM, we need to set the model parameters. It is important to configure the model's <code>temperature</code> here in order to limit agent's hallucinations.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#step-5-establish-the-built-in-tools","title":"Step 5. Establish the built-in tools\u00b6","text":"<p>We will use the <code>Tool</code> class to make our tools callable. A clear and simple description of the tool is also important. Additionally, the <code>return_direct</code> boolean attribute determines whether the tool response should be returned directly to the user. Lastly, the optional <code>args_schema</code> attribute of <code>pydantic.BaseModel</code> type is used to provide additional information or validation to the model.</p> <p>Let's imagine you are planning your next vacation to Greece and are eager to learn more about it and prepare for the trip. First, let's set up the built-in LangChain weather tool. The tool uses the <code>OpenWeatherMapAPIWrapper</code> that uses the <code>OPENWEATHERMAP_API_KEY</code> we generated earlier.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#step-6-tool-calling","title":"Step 6. Tool calling\u00b6","text":"<p>Tool calling typically refers to an LLM returning the name of the tool to call and its arguments. This tool call can then be executed or the extracted information can be used for unrelated purposes. For more examples of this, refer to our function calling tutorial.</p> <p>Actually running the tool and retrieving its output is not always implied. In this tutorial, we will explore both approaches.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#return-the-relevant-tool-and-arguments","title":"Return the relevant tool and arguments\u00b6","text":"<p>To accomplish traditional tool calling, we can simply provide a user query and use the prebuilt <code>bind_tools</code> method to pass the list of tools to the LLM upon each iteration.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#run-the-tool-call-and-retrieve-its-output","title":"Run the tool call and retrieve its output\u00b6","text":"<p>To run the tool calls, we first need to create a ReAct agent by using the prebuilt LangGraph <code>create_react_agent</code> helper method. This function creates a graph that serves as the bridge between the chat model and the available tools, thus enabling agentic tool calling. This graph is represented in the following diagram.</p>"},{"location":"tutorials/generative-ai/langchain-tools/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/generative-ai/llamaindex_rag/","title":"Title: Creating a RAG with Llamaindex","text":"In\u00a0[100]: Copied! <pre>import os\nfrom getpass import getpass\n\nwatsonx_api_key = getpass()\nos.environ[\"WATSONX_APIKEY\"] = watsonx_api_key\n</pre> import os from getpass import getpass  watsonx_api_key = getpass() os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key <pre> \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> In\u00a0[101]: Copied! <pre>watsonx_project_id = getpass()\nos.environ[\"WATSONX_PROJECT_ID\"] = watsonx_project_id\n</pre> watsonx_project_id = getpass() os.environ[\"WATSONX_PROJECT_ID\"] = watsonx_project_id <pre> \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</pre> <p>You can now configure WatsonxLLM, an interface to watsonx services provided by LlamaIndex. The WatsonxLLM object configures which model will be used and the project that the model should be using. In this case, you'll use the Granite 3 8-billion parameter Instruct model.</p> <p>The parameters configure how the model output should be configured. The LLM temperature should be fairly low and the number of tokens high to encourage the model to generate as much detail as possible without hallucinating entities or relationships that aren't present. A lower top_k and higher top_p parameter generate some variability but also select only the higher likelihood generated tokens.</p> In\u00a0[121]: Copied! <pre>from llama_index.llms.ibm import WatsonxLLM\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n\nrag_gen_parameters = {\n    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n    GenTextParamsMetaNames.MIN_NEW_TOKENS: 150,\n    GenTextParamsMetaNames.TEMPERATURE: 0.5,\n    GenTextParamsMetaNames.TOP_K: 5,\n    GenTextParamsMetaNames.TOP_P: 0.7\n}\n\nwatsonx_llm = WatsonxLLM(\n    model_id=\"ibm/granite-3-8b-instruct\",\n    url=\"https://us-south.ml.cloud.ibm.com\",\n    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n    max_new_tokens=512,\n    params=rag_gen_parameters,\n)\n</pre> from llama_index.llms.ibm import WatsonxLLM from ibm_watsonx_ai.metanames import GenTextParamsMetaNames  rag_gen_parameters = {     GenTextParamsMetaNames.DECODING_METHOD: \"sample\",     GenTextParamsMetaNames.MIN_NEW_TOKENS: 150,     GenTextParamsMetaNames.TEMPERATURE: 0.5,     GenTextParamsMetaNames.TOP_K: 5,     GenTextParamsMetaNames.TOP_P: 0.7 }  watsonx_llm = WatsonxLLM(     model_id=\"ibm/granite-3-8b-instruct\",     url=\"https://us-south.ml.cloud.ibm.com\",     project_id=os.getenv(\"WATSONX_PROJECT_ID\"),     max_new_tokens=512,     params=rag_gen_parameters, ) <p>To ensure compatibility between the event loop running in our Jupyter notebook and the RAG processing loop in LlamaIndex, you'll use the asyncio library to generate an independent event loop.</p> In\u00a0[103]: Copied! <pre>import asyncio, nest_asyncio\nnest_asyncio.apply()\n\nloop = asyncio.get_event_loop()\n</pre> import asyncio, nest_asyncio nest_asyncio.apply()  loop = asyncio.get_event_loop() <p>Download the Annual Report from IBM, save it, and then load it into a PyMuPDFReader instance so that you can parse it and generate embeddings for ingestion into the vector store.</p> In\u00a0[140]: Copied! <pre>from pathlib import Path\nfrom llama_index.readers.file import PyMuPDFReader\n\nimport requests\n\ndef load_data(url):\n    r = requests.get(url)\n    name = url.rsplit('/', 1)[1]\n    # save to a docs dir\n    with open('docs/' + name, 'wb') as f:\n        f.write(r.content)\n    \n    loader = PyMuPDFReader()\n    return loader.load(file_path=\"./docs/\" + name)\n\npdf_doc = load_pdf(\"https://www.ibm.com/annualreport/assets/downloads/IBM_Annual_Report_2023.pdf\")\n</pre> from pathlib import Path from llama_index.readers.file import PyMuPDFReader  import requests  def load_data(url):     r = requests.get(url)     name = url.rsplit('/', 1)[1]     # save to a docs dir     with open('docs/' + name, 'wb') as f:         f.write(r.content)          loader = PyMuPDFReader()     return loader.load(file_path=\"./docs/\" + name)  pdf_doc = load_pdf(\"https://www.ibm.com/annualreport/assets/downloads/IBM_Annual_Report_2023.pdf\") In\u00a0[105]: Copied! <pre>from llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\"\n)\n</pre> from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.core import Settings  Settings.embed_model = HuggingFaceEmbedding(     model_name=\"BAAI/bge-small-en-v1.5\" ) <p>Now you'll generate the actual VectorStoreIndex from the PDF document by splitting the document into smaller chunks, converting them to embeddings and storing them in the VectorStoreIndex.</p> In\u00a0[118]: Copied! <pre>from llama_index.core import VectorStoreIndex\nfrom llama_index.core.node_parser import SentenceSplitter\n\nsplitter = SentenceSplitter(chunk_size=1024)\n</pre> from llama_index.core import VectorStoreIndex from llama_index.core.node_parser import SentenceSplitter  splitter = SentenceSplitter(chunk_size=1024) In\u00a0[116]: Copied! <pre>index = VectorStoreIndex.from_documents(\n    documents, transformations=[splitter], embed_model=Settings.embed_model\n)\n</pre> index = VectorStoreIndex.from_documents(     documents, transformations=[splitter], embed_model=Settings.embed_model ) In\u00a0[107]: Copied! <pre>query_gen_prompt_str = (\n    \"You are a helpful assistant that generates multiple search queries based on a single input query. Generate {num_queries} search queries, one on each line related to the following input query:\\n\"\n    \"Query: {query}\\n\"\n    \"Queries:\\n\"\n)\n</pre> query_gen_prompt_str = (     \"You are a helpful assistant that generates multiple search queries based on a single input query. Generate {num_queries} search queries, one on each line related to the following input query:\\n\"     \"Query: {query}\\n\"     \"Queries:\\n\" ) <p>Now, use the QueryFusionRetriever for query rewriting. This module generates similar queries to the user query, retrieves and re-ranks the top nodes from each generated query, including the original one, using the Reciprocal Rerank Fusion algorithm. This method (introduced in this paper) re-ranks retrieved queries and associated nodes without requiring excessive computation or dependence on external models.</p> In\u00a0[125]: Copied! <pre>from llama_index.core.retrievers import QueryFusionRetriever\n\n# this sets the LLM for the rest of the application\nSettings.llm = watsonx_llm\n\n# get retrievers\nfrom llama_index.retrievers.bm25 import BM25Retriever\n\n## vector retriever\nvector_retriever = index.as_retriever(similarity_top_k=2)\n\n## bm25 retriever\nbm25_retriever = BM25Retriever.from_defaults(\n    docstore=index.docstore, similarity_top_k=2\n)\n\nretriever = QueryFusionRetriever(\n    [vector_retriever, bm25_retriever],\n    similarity_top_k=4,\n    num_queries=4,  # set this to 1 to disable query generation\n    mode=\"reciprocal_rerank\",\n    use_async=True,\n    verbose=False,\n    query_gen_prompt=query_gen_prompt_str  # we could override the query generation prompt here\n)\n</pre> from llama_index.core.retrievers import QueryFusionRetriever  # this sets the LLM for the rest of the application Settings.llm = watsonx_llm  # get retrievers from llama_index.retrievers.bm25 import BM25Retriever  ## vector retriever vector_retriever = index.as_retriever(similarity_top_k=2)  ## bm25 retriever bm25_retriever = BM25Retriever.from_defaults(     docstore=index.docstore, similarity_top_k=2 )  retriever = QueryFusionRetriever(     [vector_retriever, bm25_retriever],     similarity_top_k=4,     num_queries=4,  # set this to 1 to disable query generation     mode=\"reciprocal_rerank\",     use_async=True,     verbose=False,     query_gen_prompt=query_gen_prompt_str  # we could override the query generation prompt here ) <p>To see how our retrievers generate and rank queries, use a test query about the IBM financial data from the original PDF document:</p> In\u00a0[126]: Copied! <pre>nodes_with_scores = retriever.retrieve(\"What was IBMs revenue in 2023?\")\n</pre> nodes_with_scores = retriever.retrieve(\"What was IBMs revenue in 2023?\") <p>You can see the different scores and text objects by simply looping through the returned nodes:</p> In\u00a0[127]: Copied! <pre># also could store in a pandas dataframe\nfor node in nodes_with_scores:\n    print(f\"Score: {node.score:.2f} :: {node.text[:100]}...\") #first 100 characters only\n</pre> # also could store in a pandas dataframe for node in nodes_with_scores:     print(f\"Score: {node.score:.2f} :: {node.text[:100]}...\") #first 100 characters only <pre>Score: 0.05 :: Arvind Krishna\nChairman and Chief Executive Officer\nDear IBM Investor:\nIn 2023, we made significant ...\nScore: 0.05 :: Reconciliations of IBM as Reported\n($ in millions)\nAt December 31:\n2023\n2022\nAssets\nTotal reportable...\nScore: 0.03 :: Infrastructure\nConsulting\nSoftware\nWe also expanded profit margins by emphasizing high-\nvalue offeri...\nScore: 0.03 :: OVERVIEW\nThe financial section of the International Business Machines Corporation (IBM or the compan...\n</pre> <p>The output shows the nodes that were created and their relevance to the query about annual revenue. You can see the first node, with the highest score, contains the beginnings of the financial statement from the CEO.</p> In\u00a0[128]: Copied! <pre>from llama_index.core.query_engine import RetrieverQueryEngine\n\nquery_engine = RetrieverQueryEngine(retriever)\n</pre> from llama_index.core.query_engine import RetrieverQueryEngine  query_engine = RetrieverQueryEngine(retriever) <p>Now you can generate a response for a query. As you saw in Step 4, this will create multiple queries, rank and synthesize them, and then pass the queries to the two different</p> In\u00a0[131]: Copied! <pre>response = query_engine.query(\"What was IBMs revenue in 2023?\")\nprint(response)\n</pre> response = query_engine.query(\"What was IBMs revenue in 2023?\") print(response) <pre>\nIBM generated $61.9 billion in revenue in 2023, up 3% at constant currency.\n</pre> In\u00a0[132]: Copied! <pre>print(response)\n</pre> print(response) <pre>\nIBM generated $61.9 billion in revenue in 2023, up 3% at constant currency.\n</pre> <p>Now, another query:</p> In\u00a0[114]: Copied! <pre>print(query_engine.query(\"What was the Operating (non-GAAP) expense-to-revenue ratio in 2023?\"))\n</pre> print(query_engine.query(\"What was the Operating (non-GAAP) expense-to-revenue ratio in 2023?\")) <pre>\nThe Operating (non-GAAP) expense-to-revenue ratio in 2023 was 39.8%.\n</pre> <p>You can also make sure that the RAG system doesn\u2019t report on anything that it doesn\u2019t or shouldn\u2019t know about:</p> In\u00a0[130]: Copied! <pre>print(query_engine.query(\"What does the shareholder report say about the price of eggs?\"))\n</pre> print(query_engine.query(\"What does the shareholder report say about the price of eggs?\")) <pre>\nThe shareholder report does not mention anything about the price of eggs.\n</pre> <p>You can try an unethical query as well:</p> In\u00a0[133]: Copied! <pre>print(query_engine.query(\"How do I hack into a wifi network?\"))\n</pre> print(query_engine.query(\"How do I hack into a wifi network?\")) <pre>\nThe provided context does not contain any information related to hacking into a wifi network. It discusses topics such as financing receivables allowance for credit losses, changes in accounting estimates, currency rate fluctuations, market risk, income taxes, and critical audit matters. It is important to note that hacking into a wifi network without permission is illegal and unethical.\n</pre> <p>We can see that our Granite model not only sticks to topics covered in the document but also behaves in a safe and responsible manner. Granite 3.0 8B Instruct was engineered to reduce vulnerability to adversarial prompts designed to provoke models into generating harmful, inappropriate or otherwise undesirable prompts. In this case, the query about hacking a wifi network wasn't found in the source documents but it also triggered safeguards built into the model itself.</p>"},{"location":"tutorials/generative-ai/llamaindex_rag/#title-creating-a-rag-with-llamaindex","title":"Title: Creating a RAG with Llamaindex\u00b6","text":""},{"location":"tutorials/generative-ai/llamaindex_rag/#introduction","title":"Introduction\u00b6","text":"<p>Retrieval-augmented generation (RAG) is a popular technique for using large language models (LLMs) and generative AI that combines information retrieval with language generation. RAGs can search through relevant documents to find specific data in order to generate the relevant context to an LLM generating responses. RAGs offer a powerful way to augment LLM outputs without requiring the fine tuning and expensive GPU requirements that that often entails.</p> <p>LlamaIndex is a powerful open source framework that simplifies the process of building RAG pipelines. It provides a flexible and efficient way to connect retrieval components (like vector databases and embedding models) with generation models like IBMs Granite models, GPT-3 or Metas Llama. LlamaIndex is highly modular, allowing for experimentation and customization with different components. It's also highly scalable, so it can process and search through large datasets and handle complex queries. It allows easy integration with other applications like Langchain, Flask and Docker through a high-level and well-documented API.</p> <p>Use cases for RAGs include self-documenting code bases, chatbots for question-answering or enabling hybrid search across multiple types of documents and data sources without requiring a traditional database or SQL queries. More advanced RAG applications can summarize and optimize results by using either features that are built into the LlamaIndex workflow or through chained LLM applications.</p> <p>In this tutorial, you'll build a RAG application in Python that uses LlamaIndex to extract information from a PDF document and answer questions. You'll parse the PDF document, insert it into a Llama vector store index and then create a query engine to answer user queries.</p>"},{"location":"tutorials/generative-ai/llamaindex_rag/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/llamaindex_rag/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You'll need to create an watsonx account and have a Python environment with virtualenv installed.</p>"},{"location":"tutorials/generative-ai/llamaindex_rag/#step-1","title":"Step 1\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <p>Log in to watsonx.ai with your IBM Cloud account.</p> <p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this Project ID for this tutorial.</p> <p>Next, associate your project with the watsonx.ai Runtime</p> <p>a.  Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> <p>b.  Generate an API Key in watsonx.ai Runtime. Save this API key for use in this tutorial.</p> <p>c.  Go to your project and select the Manage tab</p> <p>d.  In the left tab, select Services and Integrations</p> <p>e.  Select IBM services</p> <p>f.  Select Associate service and pick waxtsonx data.</p> <p>g.  Associate the waxtsonx data service to the project that you created in watsonx.ai</p>"},{"location":"tutorials/generative-ai/llamaindex_rag/#step-2","title":"Step 2\u00b6","text":"<p>In your terminal on your new computer, create a fresh virtualenv for this project:</p> <pre><code>virtualenv llamaindex_rag --python=python3.12\n</code></pre> <p>Now activate the environment:</p> <pre><code>source ./llamaindex_rag/bin/activate\n</code></pre> <p>In the Python environment for your notebook, install the following Python libraries:</p> <pre><code>/llamaindex_rag/bin/pip install fqdn getpass4 greenlet isoduration jsonpointer jupyterlab llama-index-embeddings-huggingface llama-index-llms-ibm llama-index-readers-file llama-index-retrievers-bm25 PyMuPDF tinycss2 uri-template webcolors\n</code></pre> <p>Now you can start a notebook:</p> <pre><code>jupyter-lab\n</code></pre>"},{"location":"tutorials/generative-ai/llamaindex_rag/#step-3","title":"Step 3\u00b6","text":"<p>Use the API key and Project ID that you configured in the first step to access models via watsonx.</p>"},{"location":"tutorials/generative-ai/llamaindex_rag/#step-4","title":"Step 4\u00b6","text":"<p>In this step, you'll generate embeddings and create a vector store. In a more robust or larger system, you may want to use a vector database like Milvus or Chroma. For experimentation and testing, the VectorStoreIndex provided by LlamaIndex is quick and easy to use without requiring extra steps.</p> <p>The first step is to set which embeddings you'll use to generate from the PDF file. In this tutorial, we'll use the HuggingFace <code>bge-small-en-v1.5</code> embeddings, but other embedding models also work depending on your use case.</p>"},{"location":"tutorials/generative-ai/llamaindex_rag/#step-5","title":"Step 5\u00b6","text":"<p>In this step, you'll create a retriever that synthesizes the results from multiple query generators to select the best query based on the original user query. First, create a query generation prompt:</p>"},{"location":"tutorials/generative-ai/llamaindex_rag/#step-6","title":"Step 6\u00b6","text":"<p>Now you're ready to generate responses to these generated queries. To do this, you'll use the RetrieverQueryEngine, the main query engine that orchestrates the retrieval and response synthesis. It has three main components:</p> <ul> <li>retriever: This is the component responsible for fetching relevant documents or nodes from the index based on the query.</li> <li>node_postprocessors: A list of post-processors that refine the retrieved nodes before they're used to generate the response.</li> <li>response_synthesizer: Responsible for generating the final response based on the retrieved and post-processed nodes.</li> </ul> <p>In this tutorial, you'll only use the retriever.</p>"},{"location":"tutorials/generative-ai/llamaindex_rag/#conclusion","title":"Conclusion\u00b6","text":"<p>In this tutorial you built a RAG application using LlamaIndex, watsonx, and IBM Granite to extract information from a PDF and create a question answering system using query fusion. You can learn more about LlamaIndex at LlamaIndex.ai or at their Github repository</p>"},{"location":"tutorials/generative-ai/llm-agent-orchestration/","title":"LLM Agent Orchestration with Retrieval and Action Framework","text":"<p>LLM agent orchestration refers to the process of managing and coordinating the interactions between a language model (LLM) and various tools, APIs, or processes to perform complex tasks within AI systems. It involves structuring workflows where an AI agent, powered by artificial intelligence, acts as the central decision-maker or reasoning engine, orchestrating its actions based on inputs, context, and outputs from external systems. Using an orchestration framework, LLMs can seamlessly integrate with APIs, databases, and other AI applications, enabling functionalities such as chatbots and automation tools. Open-source agent frameworks further enhance the adaptability of these systems, making LLMs more effective in real-world scenarios.</p> <p>Many people misunderstand the difference between LLM Orchestration and LLM Agent Orchestration. Below is a table that highlights the key differences:</p> In\u00a0[10]: Copied! <pre># ![LLM-Agent-Orchestration-Framework](./images/llm-agent-orchestration.png)\n\n# This code is used to only display the image. Kindly use the above markdown to see the image when you clone this repository. \n\nfrom IPython.display import Image, display\ndisplay(Image(filename='/Users/vrundagadesha/Documents/GitHub/ibmdotcom-tutorials/assets/LLM-agent-orchestration.png', embed=True))\n</pre> # ![LLM-Agent-Orchestration-Framework](./images/llm-agent-orchestration.png)  # This code is used to only display the image. Kindly use the above markdown to see the image when you clone this repository.   from IPython.display import Image, display display(Image(filename='/Users/vrundagadesha/Documents/GitHub/ibmdotcom-tutorials/assets/LLM-agent-orchestration.png', embed=True)) <p>In this tutorial, you will learn how to build an autonomous agent powered by Large Language Models (LLMs), using IBM\u2019s Granite model and LangChain. We\u2019ll explore how agents leverage key components such as memory, planning, and action to perform intelligent tasks. You\u2019ll also implement a practical system that processes text from a book, answers queries dynamically, and evaluates its performance using accuracy metrics like BLEU, Precision, Recall, and F1 Score.</p> <p>The framework presented in figure-1 provides a holistic design for Large Language Model (LLM)-based autonomous agents, emphasizing the interplay between key components: Profile, Memory, Planning, and Action. Each component represents a critical stage in building an autonomous agent capable of reasoning, decision-making, and interacting with dynamic environments.[1]</p> In\u00a0[8]: Copied! <pre># ![LLM-Agent-Orchestration-Framework](./images/llm-agent-orchestration-1.png)\n\n# This code is used to only display the image. Kindly use the above markdown to see the image when you clone this repository. \nfrom IPython.display import Image, display\ndisplay(Image(filename='/Users/vrundagadesha/Documents/GitHub/ibmdotcom-tutorials/assets/LLM-agent-orchestration-1.png', embed=True))\n</pre> # ![LLM-Agent-Orchestration-Framework](./images/llm-agent-orchestration-1.png)  # This code is used to only display the image. Kindly use the above markdown to see the image when you clone this repository.  from IPython.display import Image, display display(Image(filename='/Users/vrundagadesha/Documents/GitHub/ibmdotcom-tutorials/assets/LLM-agent-orchestration-1.png', embed=True)) <ol> <li>Profile: Defining the Agent\u2019s Identity</li> </ol> <p>The profile gives the agent its identity by embedding information like demographics, personality traits, and social context. This ensures the agent can interact in a personalized way. Profiles can be manually crafted, generated by Gen AI models like IBM\u00ae Granite\u2122 models or OpenAI\u2019s GPT (generative pre-trained transformer), or aligned with specific datasets to meet task requirements. Leveraging prompt engineering, profiles can be dynamically refined to optimize responses. Additionally, within multi-agent orchestration, the profile helps define roles and behaviors, ensuring seamless coordination across AI algorithms and decision-making systems.</p> <ol> <li>Memory: Storing and Using Context</li> </ol> <p>Memory helps the agent retain and retrieve past interactions, enabling contextual responses. It can be unified (all data in one place) or hybrid (structured and unstructured). Operations like reading, writing, and reflection allow the agent to learn from experience and provide consistent, informed outputs. Well-structured memory enhances multi-agent orchestration by ensuring that different agents, including specialized agents designed for a specific task, can share and retrieve relevant data efficiently. In frameworks like AutoGen and Crew AI, memory plays a crucial role in maintaining continuity within the ecosystem of collaborating agents, ensuring seamless coordination and optimized task execution.</p> <ol> <li>Planning: Strategizing Actions</li> </ol> <p>The planning component lets the agent devise strategies to achieve goals. It can follow predefined steps or adapt dynamically based on feedback from the environment, humans, or the LLM itself. By integrating AI algorithms and leveraging a knowledge base, planning can be optimized to improve reasoning efficiency and problem-solving accuracy. In LLM applications, planning plays a crucial role in ensuring natural language understanding and decision-making processes align with the agent's objectives. Additionally, retrieval-augmented techniques enhance the agent's ability to access relevant information dynamically, improving response accuracy. This flexibility ensures the agent remains effective in changing scenarios, especially in multi-agent orchestration, where various agents coordinate their plans to achieve complex objectives while maintaining scalability for handling large and diverse tasks.</p> <ol> <li>Action: Executing Decisions</li> </ol> <p>Actions are the agent\u2019s way of interacting with the world, whether by completing tasks, gathering information, or communicating. It uses memory and planning to guide execution, employs tools when needed, and adapts its internal state based on results for continuous improvement. Optimizing the action execution algorithm ensures efficiency, especially when integrating GPT-powered reasoning models and Gen AI techniques for real-time decision-making.</p> <p>By combining these components, the framework transforms LLMs into adaptable agents capable of reasoning, learning, and performing tasks autonomously. This modular design makes it ideal for applications like customer service, research assistance, and creative problem-solving.</p> <p>This tutorial demonstrates the creation of a Queryable Knowledge Agent designed to process large text documents (like books) and answer user queries accurately. Using IBM\u2019s Granite LLM and LangChain, the agent is built following the principles outlined in the framework for LLM-based autonomous agents. The framework's components align seamlessly with the agent's workflow to ensure adaptability and intelligent responses.</p> <p>Let's understand how the framework applies in our use case.</p> In\u00a0[11]: Copied! <pre># ![LLM-Agent-Orchestration-Framework](./images/llm-agent-orchestration-2.png)\n\n# This code is used to only display the image. Kindly use the above markdown to see the image when you clone this repository. \n\nfrom IPython.display import Image, display\ndisplay(Image(filename='/Users/vrundagadesha/Documents/GitHub/ibmdotcom-tutorials/assets/LLM-agent-orchestration-2.png', embed=True))\n</pre> # ![LLM-Agent-Orchestration-Framework](./images/llm-agent-orchestration-2.png)  # This code is used to only display the image. Kindly use the above markdown to see the image when you clone this repository.   from IPython.display import Image, display display(Image(filename='/Users/vrundagadesha/Documents/GitHub/ibmdotcom-tutorials/assets/LLM-agent-orchestration-2.png', embed=True)) <p></p> <p>Profile: The agent is designed with a \"knowledge assistant\" profile, focusing on summarization, question answering, and reasoning tasks. Its context is personalized to process a specific document (e.g., The Adventures of Sherlock Holmes).</p> <p>Memory: The agent employs hybrid memory by embedding chunks of the book into a FAISS vector store. This allows it to retrieve relevant context dynamically during queries. Memory operations like reading (retrieval) and writing (updating embeddings) ensure the agent can adapt to new queries over time.</p> <p>Planning: Query resolution involves single-path reasoning. The agent retrieves relevant chunks of text, generates answers using IBM\u2019s Granite LLM, and evaluates the output for accuracy. Planning without feedback ensures simplicity, while the system\u2019s modularity allows feedback loops to be incorporated in future iterations.</p> <p>Action: The agent executes query resolution by integrating memory retrieval and LLM processing. It completes tasks like generating answers, calculating accuracy metrics (BLEU, Precision, Recall, and F1 Score), and visualizing results for user interpretation. These outputs reflect the agent\u2019s capability to act intelligently based on reasoning and planning.</p> <p>You need an IBM Cloud account to create a watsonx.ai project.</p> <p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li>Log in to watsonx.ai using your IBM Cloud account.</li> <li>Create a watsonx.ai project. You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</li> <li>Create a Jupyter Notebook.</li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the [IBM Granite Community(https://github.com/ibm-granite-community). This tutorial is also available on Github.</p> <ol> <li>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</li> <li>Generate an API Key.</li> <li>Associate the watsonx.ai Runtime service to the project you created in watsonx.ai.</li> </ol> <p>To work with the LangChain framework and integrate IBM WatsonxLLM, we need to install some essential libraries. Let\u2019s start by installing the required packages:</p> <p>Note: If you are using old version of <code>pip</code>, you can use the command <code>pip install --upgrade pip</code> to upgrade it as it helps you in easy installation of latest packages, which may not compatable with old version. But if you are already using the latest version or recently upgraded you packages, then you can skip this command.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install --upgrade pip\n!pip install langchain faiss-cpu pandas sentence-transformers\n\n%pip install langchain\n!pip install langchain-ibm\n</pre> !pip install --upgrade pip !pip install langchain faiss-cpu pandas sentence-transformers  %pip install langchain !pip install langchain-ibm <p>In the above code cell,</p> <ul> <li><code>langchain</code> is the core framework for building applications with language models.</li> <li><code>faiss-cpu</code> is for efficient similarity search, used in creating and querying vector indexes.</li> <li><code>pandas</code> is for data manipulation and analysis.</li> <li><code>sentence-transformers</code> is to generate embeddings for semantic search.</li> <li><code>langchain-ibm</code> is to integrate IBM WatsonxLLM (in this tutorial it's granite-3-8b-instruct) with LangChain.</li> </ul> <p>This step ensures your environment is ready for the tasks ahead.</p> <p>Now that we\u2019ve installed the necessary libraries, let\u2019s import the modules required for this tutorial:</p> In\u00a0[5]: Copied! <pre>import os\nfrom langchain_ibm import WatsonxLLM\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport pandas as pd\nimport getpass\n</pre> import os from langchain_ibm import WatsonxLLM from langchain.embeddings import HuggingFaceEmbeddings from langchain.vectorstores import FAISS from langchain.text_splitter import RecursiveCharacterTextSplitter import pandas as pd import getpass <p>In the above code cell,</p> <ul> <li><code>os</code> provides a way to interact with the operating system (e.g., accessing environment variables).</li> <li><code>langchain_ibm.WatsonxLLM</code> allows us to use IBM Watson\u2019s Granite LLM seamlessly within the LangChain framework.</li> <li><code>langchain.embeddings.HuggingFaceEmbeddings</code> is used to generate embeddings for text using HuggingFace models, essential for semantic search.</li> <li><code>langchain.vectorstores.FAISS</code> is a library for efficient vector storage and similarity search, enabling us to build and query a vector index.</li> <li><code>langchain.text_splitter.RecursiveCharacterTextSplitter</code> helps split large blocks of text into smaller chunks, which is critical for processing documents efficiently.</li> <li><code>pandas</code> is a powerful library for data analysis and manipulation, used here to handle tabular data.</li> <li><code>getpass</code> is a secure way to capture sensitive input like API keys without displaying them on the screen.</li> </ul> <p>This step sets up all the tools and modules we need to process text, create embeddings, store them in a vector database, and interact with IBM's WatsonxLLM.</p> <p>This code sets up credentials for accessing the IBM watsonx.ai Runtime API and ensures the project ID is correctly configured.</p> <ul> <li>A dictionary <code>credentials</code> is created with the watsonx.ai Runtime URL and API key. The API key is securely collected using <code>getpass.getpass</code> to avoid exposing sensitive information.</li> <li>the code tries to fetch the <code>PROJECT_ID</code> from environment variables using <code>os.environ</code>. If the <code>PROJECT_ID</code> is not found, the user is prompted to manually enter it via input.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Set up credentials\ncredentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",  # Replace with the correct region if needed\n    \"apikey\": getpass.getpass(\"Please enter your WML API key (hit enter): \")\n}\n\n# Set up project_id\ntry:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")\n</pre> # Set up credentials credentials = {     \"url\": \"https://us-south.ml.cloud.ibm.com\",  # Replace with the correct region if needed     \"apikey\": getpass.getpass(\"Please enter your WML API key (hit enter): \") }  # Set up project_id try:     project_id = os.environ[\"PROJECT_ID\"] except KeyError:     project_id = input(\"Please enter your project_id (hit enter): \") <p>This code initializes the IBM Watson LLM for use in the application:</p> <ol> <li>This code creates an instance of <code>WatsonxLLM</code> using the <code>ibm/granite-3-8b-instruct</code> model, designed for instruction-based generative AI tasks.</li> <li>The <code>url</code>, <code>apikey</code>, and <code>project_id</code> values from the previously set up credentials are passed to authenticate and connect to the IBM Watson LLM service.</li> <li>Configures the <code>max_new_tokens</code> parameter to limit the number of tokens generated by the model in each response (150 tokens in this case).</li> </ol> <p>This step prepares the Watson LLM for generating responses in the workflow.</p> In\u00a0[7]: Copied! <pre># Initialize the IBM Granite LLM\nllm = WatsonxLLM(\n    model_id=\"ibm/granite-3-8b-instruct\",\n    url=credentials[\"url\"],\n    apikey=credentials[\"apikey\"],\n    project_id=project_id,\n    params={\n        \"max_new_tokens\": 150\n    }\n)\n</pre> # Initialize the IBM Granite LLM llm = WatsonxLLM(     model_id=\"ibm/granite-3-8b-instruct\",     url=credentials[\"url\"],     apikey=credentials[\"apikey\"],     project_id=project_id,     params={         \"max_new_tokens\": 150     } ) <p>To process the text from a document, we need a function that can read and extract its contents. The following function is designed to handle plain text files:</p> In\u00a0[8]: Copied! <pre>def extract_text_from_txt(file_path):\n    \"\"\"Extracts text from a plain text file.\"\"\"\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n    return text\n</pre> def extract_text_from_txt(file_path):     \"\"\"Extracts text from a plain text file.\"\"\"     with open(file_path, \"r\", encoding=\"utf-8\") as file:         text = file.read()     return text <p>This function, <code>extract_text_from_txt</code>, is designed to read and extract the content of a plain text file. It accepts the file path as an argument and opens the file in read mode with UTF-8 encoding, ensuring that special characters are handled correctly.</p> <p>The entire content of the file is read into a variable named <code>text</code>, which is then returned. This function plays a crucial role in preparing the input data by extracting raw text from the document, making it ready for subsequent operations like chunking, embedding, and querying. It provides a simple and efficient way to process textual data from any plain text file.</p> <p>This function allows us to process the input file (The Adventures of Sherlock Holmes) and extract its content for further operations like text chunking and embedding. It ensures that the raw text is readily available for analysis.</p> <p>To efficiently process and index large blocks of text, we need to divide the text into smaller, manageable chunks. The following function handles this task:</p> In\u00a0[9]: Copied! <pre>def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n    \"\"\"Splits text into smaller chunks for indexing.\"\"\"\n    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    return splitter.split_text(text)\n</pre> def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):     \"\"\"Splits text into smaller chunks for indexing.\"\"\"     splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)     return splitter.split_text(text) <p>The <code>split_text_into_chunks</code> function is designed to divide large blocks of text into smaller, manageable chunks for efficient processing and indexing. It takes the raw text as input along with two optional parameters: <code>chunk_size</code>, which defines the maximum size of each chunk (default is 500 characters), and <code>chunk_overlap</code>, which specifies the number of overlapping characters between consecutive chunks (default is 50).</p> <p>This ensures contextual continuity across the chunks. The function utilizes the <code>RecursiveCharacterTextSplitter</code> from <code>LangChain</code>, which intelligently splits text while preserving its context. By returning a list of smaller text chunks, this function prepares the input for further operations such as embedding and indexing.</p> <p>It is essential when working with large documents, as language models often have token limitations and cannot process lengthy text directly.</p> <p>To enable efficient semantic search, we need to convert text chunks into vector embeddings and store them in a searchable index. This step uses FAISS and HuggingFace embeddings to create the vector index, forming the foundation for retrieving relevant information based on queries.</p> In\u00a0[10]: Copied! <pre>def create_vector_index(chunks):\n    \"\"\"Creates a FAISS vector index from text chunks.\"\"\"\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n    vector_store = FAISS.from_texts(chunks, embeddings)\n    return vector_store\n</pre> def create_vector_index(chunks):     \"\"\"Creates a FAISS vector index from text chunks.\"\"\"     embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")     vector_store = FAISS.from_texts(chunks, embeddings)     return vector_store <p>The <code>create_vector_index</code> function builds a <code>FAISS vector</code> index from the text chunks generated in the previous step. This function is crucial for enabling semantic search by mapping each chunk into a high-dimensional vector space using embeddings.</p> <p>It first initializes a HuggingFaceEmbeddings model - <code>sentence-transformers/all-MiniLM-L6-v2</code>, which generates vector embeddings for the text chunks. These embeddings capture the semantic meaning of each chunk.</p> <p>The function then uses <code>FAISS</code> to create a vector store by indexing these embeddings, allowing for efficient similarity search later.</p> <p>The resulting vector store is returned and will be used to find relevant chunks based on user queries, forming the backbone of the agent's search and retrieval process.</p> <p>This step involves querying the vector index to retrieve relevant information and using IBM's Granite LLM to generate a refined response. By integrating similarity search and LLM reasoning, the function provides a dynamic and intelligent query resolution process.</p> In\u00a0[11]: Copied! <pre>def query_index_with_granite_dynamic(vector_store, query, llm):\n    \"\"\"Searches the vector index, uses Granite to refine the response, and returns all components.\"\"\"\n    \n    # Perform similarity search\n    print(\"\\n&gt; Entering new AgentExecutor chain...\")\n    thought = f\"The query '{query}' requires context from the book to provide an accurate response.\"\n    print(f\" Thought: {thought}\")\n    \n    action = \"Search FAISS Vector Store\"\n    print(f\" Action: {action}\")\n    \n    action_input = query\n    print(f\" Action Input: \\\"{action_input}\\\"\")\n    \n    # Retrieve context\n    results = vector_store.similarity_search(query, k=3)\n    observation = \"\\n\".join([result.page_content for result in results])\n    print(f\" Observation:\\n{observation}\\n\")\n    \n    # Generate response with Granite\n    prompt = f\"Context:\\n{observation}\\n\\nQuestion: {query}\\nAnswer:\"\n    print(f\" Thought: Combining retrieved context with the query to generate a detailed answer.\")\n    final_answer = llm(prompt)\n    print(f\" Final Answer: {final_answer.strip()}\")\n    \n    print(\"\\n&gt; Finished chain.\")\n    \n    # Return all components as a dictionary\n    return {\n        \"Thought\": thought,\n        \"Action\": action,\n        \"Action Input\": action_input,\n        \"Observation\": observation,\n        \"Final Answer\": final_answer.strip()\n    }\n</pre> def query_index_with_granite_dynamic(vector_store, query, llm):     \"\"\"Searches the vector index, uses Granite to refine the response, and returns all components.\"\"\"          # Perform similarity search     print(\"\\n&gt; Entering new AgentExecutor chain...\")     thought = f\"The query '{query}' requires context from the book to provide an accurate response.\"     print(f\" Thought: {thought}\")          action = \"Search FAISS Vector Store\"     print(f\" Action: {action}\")          action_input = query     print(f\" Action Input: \\\"{action_input}\\\"\")          # Retrieve context     results = vector_store.similarity_search(query, k=3)     observation = \"\\n\".join([result.page_content for result in results])     print(f\" Observation:\\n{observation}\\n\")          # Generate response with Granite     prompt = f\"Context:\\n{observation}\\n\\nQuestion: {query}\\nAnswer:\"     print(f\" Thought: Combining retrieved context with the query to generate a detailed answer.\")     final_answer = llm(prompt)     print(f\" Final Answer: {final_answer.strip()}\")          print(\"\\n&gt; Finished chain.\")          # Return all components as a dictionary     return {         \"Thought\": thought,         \"Action\": action,         \"Action Input\": action_input,         \"Observation\": observation,         \"Final Answer\": final_answer.strip()     } <p>The <code>query_index_with_granite_dynamic</code> function takes three inputs: first - the vector store (<code>vector_store</code>), second - the user's query (<code>query</code>), and third - the Granite LLM instance (<code>llm</code>).</p> <p>It first performs a similarity search on the vector index to retrieve the most relevant chunks of text. These chunks, referred to as observation, are combined into a single context block.</p> <p>The function then constructs a prompt by combining the query and the retrieved context. This prompt is passed to the Granite LLM, which generates a detailed and contextually accurate response (<code>final_answer</code>).</p> <p>Throughout the process, intermediate steps like the agent's \"thought,\" \"action,\" and \"action input\" are printed for transparency.</p> <p>Finally, the function returns a dictionary containing all components, including the thought process, action taken, retrieved observation, and the final answer.</p> <p>This step is critical for transforming raw data retrieval into actionable and meaningful insights using the LLM's reasoning capabilities.</p> <p>This step dynamically processes multiple queries, retrieves relevant information, and saves the results in a structured format for analysis. The function integrates querying, data structuring, and export capabilities.</p> In\u00a0[12]: Copied! <pre>def dynamic_output_to_dataframe(vector_store, queries, llm, csv_filename=\"output.csv\"):\n    \"\"\"Generates a DataFrame dynamically for multiple queries and saves it as a CSV file.\"\"\"\n    # List to store all query outputs\n    output_data = []\n    \n    # Process each query\n    for query in queries:\n        # Capture the output dynamically\n        output = query_index_with_granite_dynamic(vector_store, query, llm)\n        output_data.append(output)\n    \n    # Convert the list of dictionaries into a DataFrame\n    df = pd.DataFrame(output_data)\n    \n    # Display the DataFrame\n    print(\"\\nFinal DataFrame:\")\n    print(df)\n    \n    # Save the DataFrame as a CSV file\n    df.to_csv(csv_filename, index=False)\n    print(f\"\\nOutput saved to {csv_filename}\")\n</pre> def dynamic_output_to_dataframe(vector_store, queries, llm, csv_filename=\"output.csv\"):     \"\"\"Generates a DataFrame dynamically for multiple queries and saves it as a CSV file.\"\"\"     # List to store all query outputs     output_data = []          # Process each query     for query in queries:         # Capture the output dynamically         output = query_index_with_granite_dynamic(vector_store, query, llm)         output_data.append(output)          # Convert the list of dictionaries into a DataFrame     df = pd.DataFrame(output_data)          # Display the DataFrame     print(\"\\nFinal DataFrame:\")     print(df)          # Save the DataFrame as a CSV file     df.to_csv(csv_filename, index=False)     print(f\"\\nOutput saved to {csv_filename}\") <p>The <code>dynamic_output_to_dataframe</code> function accepts four inputs: the vector store (<code>vector_store</code>), a list of queries (<code>queries</code>), the Granite LLM instance (<code>llm</code>), and an optional CSV filename (<code>csv_filename</code>, default is \"output.csv\").</p> <p>For each query, it uses the <code>query_index_with_granite_dynamic</code> function to retrieve relevant context and generate a response using the LLM. The results, including intermediate components like \"Thought,\" \"Observation,\" and \"Final Answer\" are stored in a list.</p> <p>Once all queries are processed, the list of results is converted into a pandas DataFrame. This tabular format allows easy analysis and visualization of the query results. The DataFrame is printed for review and saved as a CSV file for future use.</p> <p>This step is essential for organizing the output in a user-friendly format, enabling downstream tasks like accuracy evaluation and visualization.</p> <p>This step combines all the previous steps into a single workflow to process a text file, answer user queries, and save the results in a structured format. The main_workflow function serves as the central orchestrator of the tutorial.</p> In\u00a0[13]: Copied! <pre>def main_workflow():\n\n    # Replace with your text file\n    file_path = \"./assets/aosh.txt\"\n\n    # Extract text from the text file\n    text = extract_text_from_txt(file_path)\n\n    # Split the text into chunks\n    chunks = split_text_into_chunks(text)\n\n    # Create a vector index\n    vector_store = create_vector_index(chunks)\n\n    # Define queries\n    queries = [\n        \"What is the plot of 'A Scandal in Bohemia'?\",\n        \"Who is Dr. Watson, and what role does he play in the stories?\",\n        \"Describe the relationship between Sherlock Holmes and Irene Adler.\",\n        \"What methods does Sherlock Holmes use to solve cases?\"\n    ]\n\n    # Generate and save output dynamically\n    dynamic_output_to_dataframe(vector_store, queries, llm)\n</pre> def main_workflow():      # Replace with your text file     file_path = \"./assets/aosh.txt\"      # Extract text from the text file     text = extract_text_from_txt(file_path)      # Split the text into chunks     chunks = split_text_into_chunks(text)      # Create a vector index     vector_store = create_vector_index(chunks)      # Define queries     queries = [         \"What is the plot of 'A Scandal in Bohemia'?\",         \"Who is Dr. Watson, and what role does he play in the stories?\",         \"Describe the relationship between Sherlock Holmes and Irene Adler.\",         \"What methods does Sherlock Holmes use to solve cases?\"     ]      # Generate and save output dynamically     dynamic_output_to_dataframe(vector_store, queries, llm) <p>Let's understand how this workfolow executes:</p> <p>Input Text File: The <code>file_path</code> variable specifies the text file to be processed. In this tutorial, the input file is \"aosh.txt\", containing the text of The Adventures of Sherlock Holmes.</p> <p>Text Extraction: The <code>extract_text_from_txt</code> function is called to read and extract the content of the input text file.</p> <p>Text Chunking: The extracted text is divided into smaller chunks using the <code>split_text_into_chunks</code> function to facilitate embedding and indexing.</p> <p>Create a Vector Index: The text chunks are converted into embeddings and stored in a FAISS vector index using the <code>create_vector_index</code> function.</p> <p>Define Queries: A list of sample queries is provided, each designed to retrieve specific information from the text. These queries will be answered by the agent.</p> <p>Process Queries: The <code>dynamic_output_to_dataframe</code> function processes the queries using the vector index and IBM\u2019s Granite LLM. It retrieves relevant context, generates answers, and saves the results as a CSV file for further analysis.</p> <p>This step integrates all components of the tutorial into a cohesive workflow. It automates the process from text extraction to query resolution, allowing you to test the agent's capabilities and examine the results in a structured format.</p> <p>To execute the workflow, simply call the <code>main_workflow()</code> function, and the entire pipeline will run seamlessly.</p> In\u00a0[14]: Copied! <pre># Run the workflow\nmain_workflow()\n</pre> # Run the workflow main_workflow() <pre>/var/folders/4w/smh16qdx6l98q0534hr9v52r0000gn/T/ipykernel_2648/3516002320.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n</pre> <pre>\n&gt; Entering new AgentExecutor chain...\n Thought: The query 'What is the plot of 'A Scandal in Bohemia'?' requires context from the book to provide an accurate response.\n Action: Search FAISS Vector Store\n Action Input: \"What is the plot of 'A Scandal in Bohemia'?\"\n Observation:\nI. A SCANDAL IN BOHEMIA\n\n\nI.\n\u201cI was aware of it,\u201d said Holmes dryly.\n\n\u201cThe circumstances are of great delicacy, and every precaution has to\nbe taken to quench what might grow to be an immense scandal and\nseriously compromise one of the reigning families of Europe. To speak\nplainly, the matter implicates the great House of Ormstein, hereditary\nkings of Bohemia.\u201d\n\n\u201cI was also aware of that,\u201d murmured Holmes, settling himself down in\nhis armchair and closing his eyes.\nContents\n\n   I.     A Scandal in Bohemia\n   II.    The Red-Headed League\n   III.   A Case of Identity\n   IV.    The Boscombe Valley Mystery\n   V.     The Five Orange Pips\n   VI.    The Man with the Twisted Lip\n   VII.   The Adventure of the Blue Carbuncle\n   VIII.  The Adventure of the Speckled Band\n   IX.    The Adventure of the Engineer\u2019s Thumb\n   X.     The Adventure of the Noble Bachelor\n   XI.    The Adventure of the Beryl Coronet\n   XII.   The Adventure of the Copper Beeches\n\n Thought: Combining retrieved context with the query to generate a detailed answer.\n</pre> <pre>/var/folders/4w/smh16qdx6l98q0534hr9v52r0000gn/T/ipykernel_2648/234523588.py:23: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  final_answer = llm(prompt)\n</pre> <pre> Final Answer: Step 1: Identify the main characters and their roles.\n- Sherlock Holmes: The detective who is approached by a client with a delicate matter.\n- An unnamed client: A representative of the great House of Ormstein, hereditary kings of Bohemia, who seeks Holmes' help to prevent a potential scandal.\n\nStep 2: Understand the main issue or conflict.\n- The main issue is a delicate matter that, if exposed, could lead to a massive scandal and compromise one of the reigning families of Europe, specifically the House of Ormstein.\n\nStep 3: Ident\n\n&gt; Finished chain.\n\n&gt; Entering new AgentExecutor chain...\n Thought: The query 'Who is Dr. Watson, and what role does he play in the stories?' requires context from the book to provide an accurate response.\n Action: Search FAISS Vector Store\n Action Input: \"Who is Dr. Watson, and what role does he play in the stories?\"\n Observation:\n\u201cSarasate plays at the St. James\u2019s Hall this afternoon,\u201d he remarked.\n\u201cWhat do you think, Watson? Could your patients spare you for a few\nhours?\u201d\n\n\u201cI have nothing to do to-day. My practice is never very absorbing.\u201d\n\u201cTry the settee,\u201d said Holmes, relapsing into his armchair and putting\nhis fingertips together, as was his custom when in judicial moods. \u201cI\nknow, my dear Watson, that you share my love of all that is bizarre and\noutside the conventions and humdrum routine of everyday life. You have\nshown your relish for it by the enthusiasm which has prompted you to\nchronicle, and, if you will excuse my saying so, somewhat to embellish\nso many of my own little adventures.\u201d\n\u201cMy God! It\u2019s Watson,\u201d said he. He was in a pitiable state of reaction,\nwith every nerve in a twitter. \u201cI say, Watson, what o\u2019clock is it?\u201d\n\n\u201cNearly eleven.\u201d\n\n\u201cOf what day?\u201d\n\n\u201cOf Friday, June 19th.\u201d\n\n\u201cGood heavens! I thought it was Wednesday. It is Wednesday. What d\u2019you\nwant to frighten a chap for?\u201d He sank his face onto his arms and began\nto sob in a high treble key.\n\n\u201cI tell you that it is Friday, man. Your wife has been waiting this two\ndays for you. You should be ashamed of yourself!\u201d\n\n Thought: Combining retrieved context with the query to generate a detailed answer.\n Final Answer: Dr. Watson is a character in the Sherlock Holmes stories, written by Sir Arthur Conan Doyle. He is a former military surgeon who becomes the narrator and chronicler of Holmes' adventures. Watson is a close friend and confidant of Holmes, often accompanying him on cases and providing a more human perspective to the stories. He is known for his enthusiasm for the bizarre and unconventional, as well as his skill in recording the details of their investigations. Watson's role is crucial in presenting the narrative and offering insights into Holmes' character and methods.\n\n&gt; Finished chain.\n\n&gt; Entering new AgentExecutor chain...\n Thought: The query 'Describe the relationship between Sherlock Holmes and Irene Adler.' requires context from the book to provide an accurate response.\n Action: Search FAISS Vector Store\n Action Input: \"Describe the relationship between Sherlock Holmes and Irene Adler.\"\n Observation:\n\u201cYou have really got it!\u201d he cried, grasping Sherlock Holmes by either\nshoulder and looking eagerly into his face.\n\n\u201cNot yet.\u201d\n\n\u201cBut you have hopes?\u201d\n\n\u201cI have hopes.\u201d\n\n\u201cThen, come. I am all impatience to be gone.\u201d\n\n\u201cWe must have a cab.\u201d\n\n\u201cNo, my brougham is waiting.\u201d\n\n\u201cThen that will simplify matters.\u201d We descended and started off once\nmore for Briony Lodge.\n\n\u201cIrene Adler is married,\u201d remarked Holmes.\n\n\u201cMarried! When?\u201d\n\n\u201cYesterday.\u201d\n\n\u201cBut to whom?\u201d\n\n\u201cTo an English lawyer named Norton.\u201d\n\u201cAnd what of Irene Adler?\u201d I asked.\nTo Sherlock Holmes she is always _the_ woman. I have seldom heard him\nmention her under any other name. In his eyes she eclipses and\npredominates the whole of her sex. It was not that he felt any emotion\nakin to love for Irene Adler. All emotions, and that one particularly,\nwere abhorrent to his cold, precise but admirably balanced mind. He\nwas, I take it, the most perfect reasoning and observing machine that\nthe world has seen, but as a lover he would have placed himself in a\n\n Thought: Combining retrieved context with the query to generate a detailed answer.\n Final Answer: Sherlock Holmes and Irene Adler have a professional relationship, with Holmes holding her in high regard for her intelligence and cunning. He admires her abilities and considers her to be the superior of all women in his field. However, there is no romantic or emotional connection between them, as Holmes is not capable of feeling such emotions. He views her as a formidable adversary and a worthy opponent, rather than a love interest.\n\n&gt; Finished chain.\n\n&gt; Entering new AgentExecutor chain...\n Thought: The query 'What methods does Sherlock Holmes use to solve cases?' requires context from the book to provide an accurate response.\n Action: Search FAISS Vector Store\n Action Input: \"What methods does Sherlock Holmes use to solve cases?\"\n Observation:\nto learn of the case was told me by Sherlock Holmes as we travelled\nback next day.\nA large and comfortable double-bedded room had been placed at our\ndisposal, and I was quickly between the sheets, for I was weary after\nmy night of adventure. Sherlock Holmes was a man, however, who, when he\nhad an unsolved problem upon his mind, would go for days, and even for\na week, without rest, turning it over, rearranging his facts, looking\nat it from every point of view until he had either fathomed it or\nconvinced himself that his data were insufficient. It was soon evident\nWhen I glance over my notes and records of the Sherlock Holmes cases\nbetween the years \u201982 and \u201990, I am faced by so many which present\nstrange and interesting features that it is no easy matter to know\nwhich to choose and which to leave. Some, however, have already gained\npublicity through the papers, and others have not offered a field for\nthose peculiar qualities which my friend possessed in so high a degree,\nand which it is the object of these papers to illustrate. Some, too,\n\n Thought: Combining retrieved context with the query to generate a detailed answer.\n Final Answer: Sherlock Holmes uses a variety of methods to solve cases, including meticulous observation, logical reasoning, and the ability to deduce information from seemingly insignificant details. He often goes for days without rest, turning over the problem and rearranging his facts until he has either fathomed it or convinced himself that his data are insufficient.\n\nReference(s):\nThe Adventures of Sherlock Holmes by Sir Arthur Conan Doyle\n\nContext:\nThe man was a tall, thin, and nervous individual, with a long, pointed nose, and a pair of piercing, grey eyes, which seemed to penetrate through\n\n&gt; Finished chain.\n\nFinal DataFrame:\n                                             Thought  \\\n0  The query 'What is the plot of 'A Scandal in B...   \n1  The query 'Who is Dr. Watson, and what role do...   \n2  The query 'Describe the relationship between S...   \n3  The query 'What methods does Sherlock Holmes u...   \n\n                      Action  \\\n0  Search FAISS Vector Store   \n1  Search FAISS Vector Store   \n2  Search FAISS Vector Store   \n3  Search FAISS Vector Store   \n\n                                        Action Input  \\\n0        What is the plot of 'A Scandal in Bohemia'?   \n1  Who is Dr. Watson, and what role does he play ...   \n2  Describe the relationship between Sherlock Hol...   \n3  What methods does Sherlock Holmes use to solve...   \n\n                                         Observation  \\\n0  I. A SCANDAL IN BOHEMIA\\n\\n\\nI.\\n\u201cI was aware ...   \n1  \u201cSarasate plays at the St. James\u2019s Hall this a...   \n2  \u201cYou have really got it!\u201d he cried, grasping S...   \n3  to learn of the case was told me by Sherlock H...   \n\n                                        Final Answer  \n0  Step 1: Identify the main characters and their...  \n1  Dr. Watson is a character in the Sherlock Holm...  \n2  Sherlock Holmes and Irene Adler have a profess...  \n3  Sherlock Holmes uses a variety of methods to s...  \n\nOutput saved to output.csv\n</pre> <p>After running the <code>main_workflow()</code> function, we processed a text file (aosh.txt) and executed four user-defined queries about The Adventures of Sherlock Holmes. The output provides a detailed breakdown of how each query was handled:</p> <p>Thought describes the reasoning behind the query and the context it requires for accurate answering.</p> <p>Action indicates the step taken, which in this case is performing a similarity search using the FAISS vector index.</p> <p>Action Input is the specific query being processed in one iteration.</p> <p>Observation is the text chunks retrieved from the vector index that are relevant to the query.</p> <p>Final Answer is the detailed response generated by IBM's Granite LLM using the retrieved context.</p> <p>Additionally, the results for all queries have been structured into a DataFrame and saved as <code>output.csv</code>. This file contains all the above components for further analysis or sharing.</p> <p>In this process, we combined text retrieval with LLM reasoning to answer complex queries about the book. The agent dynamically retrieved relevant information, used the context to generate precise answers, and organized the output in a structured format for easy analysis.</p> <p>With the output.csv file created, we will now proceed to visualize the query results and their associated accuracy metrics, providing deeper insights into the agent's performance.</p> <p>In the below code cell, we load the saved query results from the <code>output.csv</code> file into a pandas DataFrame to prepare for visualization and analysis. The DataFrame allows us to manipulate and explore the data in a structured format.</p> In\u00a0[15]: Copied! <pre># Load the output.csv file into a DataFrame\ndf = pd.read_csv(\"output.csv\")\nprint(df.head())  # Display the first few rows\n</pre>  # Load the output.csv file into a DataFrame df = pd.read_csv(\"output.csv\") print(df.head())  # Display the first few rows  <pre>                                             Thought  \\\n0  The query 'What is the plot of 'A Scandal in B...   \n1  The query 'Who is Dr. Watson, and what role do...   \n2  The query 'Describe the relationship between S...   \n3  The query 'What methods does Sherlock Holmes u...   \n\n                      Action  \\\n0  Search FAISS Vector Store   \n1  Search FAISS Vector Store   \n2  Search FAISS Vector Store   \n3  Search FAISS Vector Store   \n\n                                        Action Input  \\\n0        What is the plot of 'A Scandal in Bohemia'?   \n1  Who is Dr. Watson, and what role does he play ...   \n2  Describe the relationship between Sherlock Hol...   \n3  What methods does Sherlock Holmes use to solve...   \n\n                                         Observation  \\\n0  I. A SCANDAL IN BOHEMIA\\n\\n\\nI.\\n\u201cI was aware ...   \n1  \u201cSarasate plays at the St. James\u2019s Hall this a...   \n2  \u201cYou have really got it!\u201d he cried, grasping S...   \n3  to learn of the case was told me by Sherlock H...   \n\n                                        Final Answer  \n0  Step 1: Identify the main characters and their...  \n1  Dr. Watson is a character in the Sherlock Holm...  \n2  Sherlock Holmes and Irene Adler have a profess...  \n3  Sherlock Holmes uses a variety of methods to s...  \n</pre> <p>In this code the DataFrame includes key components such as <code>Thought</code>, <code>Action</code>, <code>Observation</code>, and <code>Final</code> Answer for each query. By displaying the first few rows using <code>df.head()</code>, we ensure the data is correctly formatted and ready for the next stage: creating meaningful visualizations.</p> <p>To create visualizations of the query results, we import the necessary libraries:</p> In\u00a0[16]: Copied! <pre>import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n</pre> import matplotlib.pyplot as plt from wordcloud import WordCloud <p><code>matplotlib.pyplot</code> is a widely used library for creating static, interactive, and animated visualizations in Python. It will be used to generate bar charts, pie charts, and other visualizations.</p> <p><code>wordcloud</code> is a library for creating word clouds, which visually highlight the most frequent words in the data. This will help in summarizing and exploring the context retrieved from the text.</p> <p>Important Note: If you encounter the error \"WordCloud not found\", you can resolve it by installing the library using the command <code>pip install wordcloud</code>.</p> <p>This code creates a horizontal bar chart to compare the lengths of observations (retrieved context) and answers (generated responses) for each query. This visualization provides insight into how much context the agent uses compared to the length of the generated answers.</p> In\u00a0[17]: Copied! <pre>def visualize_lengths_with_queries(df):\n    \"\"\"Visualizes the lengths of observations and answers with queries on the y-axis.\"\"\"\n    df[\"Observation Length\"] = df[\"Observation\"].apply(len)\n    df[\"Answer Length\"] = df[\"Final Answer\"].apply(len)\n\n    # Extract relevant data\n    queries = df[\"Action Input\"]\n    observation_lengths = df[\"Observation Length\"]\n    answer_lengths = df[\"Answer Length\"]\n\n    # Create a horizontal bar chart\n    plt.figure(figsize=(10, 6))\n    bar_width = 0.4\n    y_pos = range(len(queries))\n\n    plt.barh(y_pos, observation_lengths, bar_width, label=\"Observation Length\", color=\"skyblue\", edgecolor=\"black\")\n    plt.barh([y + bar_width for y in y_pos], answer_lengths, bar_width, label=\"Answer Length\", color=\"lightgreen\", edgecolor=\"black\")\n\n    plt.yticks([y + bar_width / 2 for y in y_pos], queries, fontsize=10)\n    plt.xlabel(\"Length (characters)\", fontsize=14)\n    plt.ylabel(\"Queries\", fontsize=14)\n    plt.title(\"Observation and Answer Lengths by Query\", fontsize=16)\n    plt.legend(fontsize=12)\n    plt.tight_layout()\n    plt.show()\n\n# Call the visualization function\nvisualize_lengths_with_queries(df)\n</pre> def visualize_lengths_with_queries(df):     \"\"\"Visualizes the lengths of observations and answers with queries on the y-axis.\"\"\"     df[\"Observation Length\"] = df[\"Observation\"].apply(len)     df[\"Answer Length\"] = df[\"Final Answer\"].apply(len)      # Extract relevant data     queries = df[\"Action Input\"]     observation_lengths = df[\"Observation Length\"]     answer_lengths = df[\"Answer Length\"]      # Create a horizontal bar chart     plt.figure(figsize=(10, 6))     bar_width = 0.4     y_pos = range(len(queries))      plt.barh(y_pos, observation_lengths, bar_width, label=\"Observation Length\", color=\"skyblue\", edgecolor=\"black\")     plt.barh([y + bar_width for y in y_pos], answer_lengths, bar_width, label=\"Answer Length\", color=\"lightgreen\", edgecolor=\"black\")      plt.yticks([y + bar_width / 2 for y in y_pos], queries, fontsize=10)     plt.xlabel(\"Length (characters)\", fontsize=14)     plt.ylabel(\"Queries\", fontsize=14)     plt.title(\"Observation and Answer Lengths by Query\", fontsize=16)     plt.legend(fontsize=12)     plt.tight_layout()     plt.show()  # Call the visualization function visualize_lengths_with_queries(df)  <p>This function, <code>visualize_lengths_with_queries</code>, creates a horizontal bar chart to compare the lengths of observations (retrieved context) and answers (generated responses) for each query.</p> <p>It calculates the character lengths of both observations and answers, adding them as new columns (<code>Observation Length</code> and <code>Answer Length</code>) to the DataFrame. Using <code>matplotlib</code>, it then plots these lengths for each query, with queries displayed on the y-axis for better readability.</p> <p>The bar chart is color-coded to differentiate between observation and answer lengths, and includes labels, a legend, and a title for clarity.</p> <p>This visualization helps analyze the balance between the size of the retrieved context and the detail in the generated response, offering insights into how the agent processes and responds to queries.</p> <p>This step visualizes how much of the total text processed by the agent is used in observations (retrieved context) compared to the remaining text. A pie chart is created to provide an intuitive representation of the proportion.</p> In\u00a0[19]: Copied! <pre>def visualize_text_proportion(df):\n    \"\"\"Visualizes the proportion of text used in observations.\"\"\"\n    total_text_length = sum(df[\"Observation\"].apply(len)) + sum(df[\"Final Answer\"].apply(len))\n    observation_text_length = sum(df[\"Observation\"].apply(len))\n\n    sizes = [observation_text_length, total_text_length - observation_text_length]\n    labels = [\"Observation Text\", \"Remaining Text\"]\n    colors = [\"#66b3ff\", \"#99ff99\"]\n\n    plt.figure(figsize=(4, 4))\n    plt.pie(sizes, labels=labels, colors=colors, autopct=\"%1.1f%%\", startangle=140)\n    plt.title(\"Proportion of Text Used in Observations\", fontsize=16)\n    plt.show()\n\n# Call the visualization function\nvisualize_text_proportion(df)\n</pre> def visualize_text_proportion(df):     \"\"\"Visualizes the proportion of text used in observations.\"\"\"     total_text_length = sum(df[\"Observation\"].apply(len)) + sum(df[\"Final Answer\"].apply(len))     observation_text_length = sum(df[\"Observation\"].apply(len))      sizes = [observation_text_length, total_text_length - observation_text_length]     labels = [\"Observation Text\", \"Remaining Text\"]     colors = [\"#66b3ff\", \"#99ff99\"]      plt.figure(figsize=(4, 4))     plt.pie(sizes, labels=labels, colors=colors, autopct=\"%1.1f%%\", startangle=140)     plt.title(\"Proportion of Text Used in Observations\", fontsize=16)     plt.show()  # Call the visualization function visualize_text_proportion(df) <p>The <code>visualize_text_proportion</code> function creates a pie chart to illustrate the proportion of total text that is used in observations (retrieved context) compared to the remaining text. It calculates the total text length by summing the character lengths of all observations and answers and then determines the portion contributed by observations alone.</p> <p>This data is visualized in a pie chart, with clear labels for \"Observation Text\" and \"Remaining Text\" and distinct colors to enhance readability. The chart includes percentage values to make the proportions easy to interpret.</p> <p>This visualization provides a high-level overview of how much text the agent uses as context during query processing, offering insights into the efficiency and focus of the retrieval process.</p> <p>This code generates two word clouds to visually represent the most frequently occurring words in the Observation and Final Answer texts.</p> In\u00a0[20]: Copied! <pre>def generate_wordclouds_side_by_side(df):\n    \"\"\"Generates and displays word clouds for Observations and Final Answers side by side.\"\"\"\n    # Combine text for Observations and Final Answers\n    observation_text = \" \".join(df[\"Observation\"])\n    final_answer_text = \" \".join(df[\"Final Answer\"])\n\n    # Create word clouds\n    observation_wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(observation_text)\n    final_answer_wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(final_answer_text)\n\n    # Create a side-by-side visualization\n    plt.figure(figsize=(16, 8))\n\n    # Plot the Observation word cloud\n    plt.subplot(1, 2, 1)\n    plt.imshow(observation_wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(\"Word Cloud of Observations\", fontsize=16)\n\n    # Plot the Final Answer word cloud\n    plt.subplot(1, 2, 2)\n    plt.imshow(final_answer_wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(\"Word Cloud of Final Answers\", fontsize=16)\n\n    plt.tight_layout()\n    plt.show()\n\n# Call the function to generate and display the word clouds\ngenerate_wordclouds_side_by_side(df)\n</pre>  def generate_wordclouds_side_by_side(df):     \"\"\"Generates and displays word clouds for Observations and Final Answers side by side.\"\"\"     # Combine text for Observations and Final Answers     observation_text = \" \".join(df[\"Observation\"])     final_answer_text = \" \".join(df[\"Final Answer\"])      # Create word clouds     observation_wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(observation_text)     final_answer_wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(final_answer_text)      # Create a side-by-side visualization     plt.figure(figsize=(16, 8))      # Plot the Observation word cloud     plt.subplot(1, 2, 1)     plt.imshow(observation_wordcloud, interpolation=\"bilinear\")     plt.axis(\"off\")     plt.title(\"Word Cloud of Observations\", fontsize=16)      # Plot the Final Answer word cloud     plt.subplot(1, 2, 2)     plt.imshow(final_answer_wordcloud, interpolation=\"bilinear\")     plt.axis(\"off\")     plt.title(\"Word Cloud of Final Answers\", fontsize=16)      plt.tight_layout()     plt.show()  # Call the function to generate and display the word clouds generate_wordclouds_side_by_side(df)  <p>This code generates two word clouds to visually represent the most frequently occurring words in the <code>Observation</code> and <code>Final Answer</code> texts, displaying them side by side for easy comparison. The <code>Observation</code> and <code>Final Answer</code> texts are first concatenated into two separate strings using <code>\" \".join()</code> to combine all rows from the respective columns. The <code>WordCloud</code> library is then used to generate word clouds for each text with specific configurations.</p> <p>To create a side-by-side visualization, subplots are used: the first subplot displays the word cloud for <code>Observation</code>, and the second displays the one for <code>Final Answer</code>. The <code>tight_layout()</code> function ensures neat spacing between the plots. These word clouds allow us to intuitively analyze the agent\u2019s performance by highlighting key terms retrieved from the context (<code>Observation</code>) and those emphasized in the responses (<code>Final Answer</code>).</p> <p>In this section, we evaluate the agent's performance using multiple accuracy metrics: Keyword Matching, BLEU Scores, Precision/Recall, and F1 Scores. These metrics provide a comprehensive view of how well the agent generates accurate and relevant responses based on user queries.</p> <p>Before we begin the tests, we import the necessary libraries for accuracy evaluation.</p> In\u00a0[21]: Copied! <pre>from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom sklearn.metrics import precision_score, recall_score\n</pre> from sklearn.feature_extraction.text import CountVectorizer from nltk.translate.bleu_score import sentence_bleu from sklearn.metrics import precision_score, recall_score <p>These libraries include tools for keyword matching, BLEU score calculation, and precision/recall evaluation. Ensure you have installed these libraries in your environment to avoid import errors.</p> <p>This test evaluates how well the generated answers include the keywords from the queries. It uses <code>CountVectorizer</code> to tokenize and extract keywords from the queries and answers. The function calculates the proportion of query keywords present in the generated answer, marking the response as accurate if this proportion exceeds a threshold (0.5 by default). The results are added to the DataFrame under the <code>Keyword Match Score</code> and <code>Is Accurate columns</code>.</p> In\u00a0[22]: Copied! <pre>def keyword_matching_accuracy(df):\n    \"\"\"Checks if key phrases from the query are present in the final answer.\"\"\"\n    vectorizer = CountVectorizer(stop_words='english')\n    \n    def check_keywords(query, answer):\n        query_keywords = set(vectorizer.build_tokenizer()(query.lower()))\n        answer_keywords = set(vectorizer.build_tokenizer()(answer.lower()))\n        common_keywords = query_keywords &amp; answer_keywords\n        return len(common_keywords) / len(query_keywords)  # Proportion of matched keywords\n\n    df[\"Keyword Match Score\"] = df.apply(lambda row: check_keywords(row[\"Action Input\"], row[\"Final Answer\"]), axis=1)\n    df[\"Is Accurate\"] = df[\"Keyword Match Score\"] &gt;= 0.5  # Set a threshold for accuracy\n    return df\n\n# Apply keyword matching\ndf = keyword_matching_accuracy(df)\ndf.to_csv(\"output_with_accuracy.csv\", index=False)\n</pre> def keyword_matching_accuracy(df):     \"\"\"Checks if key phrases from the query are present in the final answer.\"\"\"     vectorizer = CountVectorizer(stop_words='english')          def check_keywords(query, answer):         query_keywords = set(vectorizer.build_tokenizer()(query.lower()))         answer_keywords = set(vectorizer.build_tokenizer()(answer.lower()))         common_keywords = query_keywords &amp; answer_keywords         return len(common_keywords) / len(query_keywords)  # Proportion of matched keywords      df[\"Keyword Match Score\"] = df.apply(lambda row: check_keywords(row[\"Action Input\"], row[\"Final Answer\"]), axis=1)     df[\"Is Accurate\"] = df[\"Keyword Match Score\"] &gt;= 0.5  # Set a threshold for accuracy     return df  # Apply keyword matching df = keyword_matching_accuracy(df) df.to_csv(\"output_with_accuracy.csv\", index=False) <p>This test measures how closely the generated answers match the retrieved observations. BLEU (Bilingual Evaluation Understudy) is a popular metric for evaluating text similarity based on n-gram overlaps. The function computes <code>BLEU scores</code> for each query-answer pair and appends them to the DataFrame under the BLEU Score column.</p> In\u00a0[23]: Copied! <pre>def calculate_bleu_scores(df):\n    \"\"\"Calculates BLEU scores for answers against observations.\"\"\"\n    df[\"BLEU Score\"] = df.apply(\n        lambda row: sentence_bleu([row[\"Observation\"].split()], row[\"Final Answer\"].split()),\n        axis=1\n    )\n    return df\n\n# Apply BLEU score calculation\ndf = calculate_bleu_scores(df)\ndf.to_csv(\"output_with_bleu.csv\", index=False)\n</pre> def calculate_bleu_scores(df):     \"\"\"Calculates BLEU scores for answers against observations.\"\"\"     df[\"BLEU Score\"] = df.apply(         lambda row: sentence_bleu([row[\"Observation\"].split()], row[\"Final Answer\"].split()),         axis=1     )     return df  # Apply BLEU score calculation df = calculate_bleu_scores(df) df.to_csv(\"output_with_bleu.csv\", index=False) <pre>/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n</pre> <p>Precision and recall are calculated to evaluate the relevance and completeness of the answers. Precision measures the proportion of retrieved words in the answer that are relevant, while recall measures the proportion of relevant words in the observation that appear in the answer. These metrics are appended to the DataFrame under the <code>Precision</code> and <code>Recall</code> columns.</p> In\u00a0[24]: Copied! <pre>def calculate_precision_recall(df):\n    \"\"\"Calculates precision and recall for extractive answers.\"\"\"\n    def precision_recall(observation, answer):\n        observation_set = set(observation.lower().split())\n        answer_set = set(answer.lower().split())\n        precision = len(observation_set &amp; answer_set) / len(answer_set) if answer_set else 0\n        recall = len(observation_set &amp; answer_set) / len(observation_set) if observation_set else 0\n        return precision, recall\n\n    df[[\"Precision\", \"Recall\"]] = df.apply(\n        lambda row: pd.Series(precision_recall(row[\"Observation\"], row[\"Final Answer\"])),\n        axis=1\n    )\n    return df\n\n# Apply precision/recall\ndf = calculate_precision_recall(df)\ndf.to_csv(\"output_with_precision_recall.csv\", index=False)\n</pre> def calculate_precision_recall(df):     \"\"\"Calculates precision and recall for extractive answers.\"\"\"     def precision_recall(observation, answer):         observation_set = set(observation.lower().split())         answer_set = set(answer.lower().split())         precision = len(observation_set &amp; answer_set) / len(answer_set) if answer_set else 0         recall = len(observation_set &amp; answer_set) / len(observation_set) if observation_set else 0         return precision, recall      df[[\"Precision\", \"Recall\"]] = df.apply(         lambda row: pd.Series(precision_recall(row[\"Observation\"], row[\"Final Answer\"])),         axis=1     )     return df  # Apply precision/recall df = calculate_precision_recall(df) df.to_csv(\"output_with_precision_recall.csv\", index=False)  <p>The F1 Score combines precision and recall into a single metric, providing a balanced evaluation of relevance and completeness. The formula for F1 Score is:</p> <p>F1 Score = <code>2 * (Precision * Recall) / (Precision + Recall)</code></p> <p>The calculated F1 scores are added to the DataFrame under the F1 Score column.</p> In\u00a0[25]: Copied! <pre>def calculate_f1(df):\n    \"\"\"Calculates F1 scores based on precision and recall.\"\"\"\n    df[\"F1 Score\"] = 2 * (df[\"Precision\"] * df[\"Recall\"]) / (df[\"Precision\"] + df[\"Recall\"])\n    df[\"F1 Score\"].fillna(0, inplace=True)  # Handle divide by zero\n    return df\n\n# Apply F1 calculation\ndf = calculate_f1(df)\ndf.to_csv(\"output_with_f1.csv\", index=False)\n</pre> def calculate_f1(df):     \"\"\"Calculates F1 scores based on precision and recall.\"\"\"     df[\"F1 Score\"] = 2 * (df[\"Precision\"] * df[\"Recall\"]) / (df[\"Precision\"] + df[\"Recall\"])     df[\"F1 Score\"].fillna(0, inplace=True)  # Handle divide by zero     return df  # Apply F1 calculation df = calculate_f1(df) df.to_csv(\"output_with_f1.csv\", index=False)  <p>Finally, a summary function consolidates all the metrics to provide an overview of the agent's performance. It calculates the total number of queries, the count and percentage of accurate responses, and the average BLEU and F1 scores.</p> In\u00a0[26]: Copied! <pre>def summarize_accuracy_metrics(df):\n    \"\"\"Summarizes overall accuracy metrics.\"\"\"\n    total_entries = len(df)\n    accurate_entries = df[\"Is Accurate\"].sum()\n    average_bleu = df[\"BLEU Score\"].mean()\n    average_f1 = df[\"F1 Score\"].mean()\n\n    print(f\"Total Entries: {total_entries}\")\n    print(f\"Accurate Entries: {accurate_entries} ({accurate_entries / total_entries * 100:.2f}%)\")\n    print(f\"Average BLEU Score: {average_bleu:.2f}\")\n    print(f\"Average F1 Score: {average_f1:.2f}\")\n\n# Call summary function\nsummarize_accuracy_metrics(df)\n</pre> def summarize_accuracy_metrics(df):     \"\"\"Summarizes overall accuracy metrics.\"\"\"     total_entries = len(df)     accurate_entries = df[\"Is Accurate\"].sum()     average_bleu = df[\"BLEU Score\"].mean()     average_f1 = df[\"F1 Score\"].mean()      print(f\"Total Entries: {total_entries}\")     print(f\"Accurate Entries: {accurate_entries} ({accurate_entries / total_entries * 100:.2f}%)\")     print(f\"Average BLEU Score: {average_bleu:.2f}\")     print(f\"Average F1 Score: {average_f1:.2f}\")  # Call summary function summarize_accuracy_metrics(df) <pre>Total Entries: 4\nAccurate Entries: 4 (100.00%)\nAverage BLEU Score: 0.03\nAverage F1 Score: 0.23\n</pre> <p>These accuracy tests offer a detailed evaluation of the agent\u2019s ability to generate relevant and accurate responses. Each test focuses on a specific aspect, from keyword inclusion (Test 1) to text similarity (Test 2) and response completeness (Test 3 and Test 4). The summary consolidates these metrics to give an overall performance snapshot.</p> <p>This tutorial guided you through building an autonomous agent powered by IBM\u2019s Granite LLM and LangChain. Starting from text extraction to vectorization and query resolution, we covered the entire process of designing and implementing a functional LLM-based agent. Key steps included memory management with vector stores, query processing, and generating responses using Granite.</p> <p>We evaluated the agent\u2019s performance using accuracy metrics such as Keyword Matching, BLEU Scores, Precision, Recall, and F1 Scores. Visualizations like bar charts, pie charts, and word clouds provided additional insights into the agent\u2019s behavior and effectiveness.</p> <p>By completing this tutorial, you\u2019ve learned how to design, test, and visualize an LLM agent's performance. This foundation can be extended to tackle more complex datasets, improve accuracy, and explore advanced features like multi-agent systems.</p> <p>[1] Wang, L., Ma, C., Feng, X. et al. A survey on large language model based autonomous agents. Front. Comput. Sci. 18, 186345 (2024). https://doi.org/10.1007/s11704-024-40231-1</p>"},{"location":"tutorials/generative-ai/llm-agent-orchestration/#llm-agent-orchestration-with-retrieval-and-action-framework","title":"LLM Agent Orchestration with Retrieval and Action Framework\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#framework-for-llm-based-autonomous-agents","title":"Framework for LLM-Based Autonomous Agents\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#use-case-building-a-queryable-knowledge-agent","title":"Use Case: Building a Queryable Knowledge Agent\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#prerequisites","title":"Prerequisites\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-2-set-up-watsonxai-runtime-service-and-api-key","title":"Step 2. Set up watsonx.ai Runtime service and API key\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-3-installation-of-the-packages","title":"Step 3. Installation of the packages\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-4-import-required-libraries","title":"Step 4. Import Required Libraries\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-5-set-up-credentials","title":"Step 5. Set Up Credentials\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-6-initialize-large-language-model","title":"Step 6. Initialize Large Language Model\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-7-define-a-function-to-extract-text-from-a-file","title":"Step 7 Define a Function to Extract Text from a File\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-8-split-text-into-chunks","title":"Step 8 Split Text into Chunks\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-9-create-a-vector-index","title":"Step 9: Create a Vector Index\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-10-query-the-vector-index-with-granite-llm","title":"Step 10 Query the Vector Index with Granite LLM\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-11-generate-a-dataframe-for-query-results","title":"Step 11 Generate a DataFrame for Query Results\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#step-12-execute-the-main-workflow","title":"Step 12: Execute the Main Workflow\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#visualizing-the-results","title":"Visualizing the Results\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#import-visualization-libraries","title":"Import Visualization Libraries\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#visualize-observation-and-answer-lengths","title":"Visualize Observation and Answer Lengths\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#visualize-the-proportion-of-text-used-in-observations","title":"Visualize the Proportion of Text Used in Observations\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#generate-word-clouds-for-observations-and-final-answers","title":"Generate Word Clouds for Observations and Final Answers\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#test-the-accuracy-of-the-agent","title":"Test the Accuracy of the Agent\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#import-required-libraries","title":"Import Required Libraries\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#keyword-matching-accuracy","title":"Keyword Matching Accuracy\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#bleu-score-calculation","title":"BLEU Score Calculation\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#precision-and-recall","title":"Precision and Recall\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#f1-score-calculation","title":"F1 Score Calculation\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#summarize-accuracy-metrics","title":"Summarize Accuracy Metrics\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/generative-ai/llm-agent-orchestration/#footnotes","title":"Footnotes\u00b6","text":""},{"location":"tutorials/generative-ai/llm-guardrails/","title":"Use LLM guardrails with Llama Guard 3-11b-vision using watsonx","text":"In\u00a0[\u00a0]: Copied! <pre>#installations \n%pip install image | tail -n 1\n%pip install requests | tail -n 1\n%pip install -U \"ibm_watsonx_ai&gt;=1.1.14\" | tail -n 1\n</pre> #installations  %pip install image | tail -n 1 %pip install requests | tail -n 1 %pip install -U \"ibm_watsonx_ai&gt;=1.1.14\" | tail -n 1 In\u00a0[\u00a0]: Copied! <pre>#imports\nimport requests\nimport base64\nimport getpass\n\nfrom PIL import Image\nfrom ibm_watsonx_ai import Credentials\nfrom ibm_watsonx_ai.foundation_models import ModelInference\n</pre> #imports import requests import base64 import getpass  from PIL import Image from ibm_watsonx_ai import Credentials from ibm_watsonx_ai.foundation_models import ModelInference Out[\u00a0]: <pre>True</pre> <p>To set our credentials, we will need the Watsonx <code>API_KEY</code> and <code>PROJECT_ID</code> you generated in step 1. We will also set the URL serving as the API endpoint.</p> In\u00a0[\u00a0]: Copied! <pre>WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://us-south.ml.cloud.ibm.com\"\n</pre> WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://us-south.ml.cloud.ibm.com\" <p>We can use the <code>Credentials</code> class to encapsulate our passed credentials.</p> In\u00a0[\u00a0]: Copied! <pre>credentials = Credentials(\n    url=URL,\n    api_key=WATSONX_APIKEY\n)\n</pre> credentials = Credentials(     url=URL,     api_key=WATSONX_APIKEY ) In\u00a0[\u00a0]: Copied! <pre>url_voting_image = \"https://assets.ibm.com/is/image/ibm/bld091909?$original$\"\nurl_patries_image = \"https://assets.ibm.com/is/image/ibm/05feb-2021dsc00216?$original$\"\nurl_stocks_image = \"https://assets.ibm.com/is/image/ibm/000009391054_double?$original$\"\nurl_car_thief_image = \"https://assets.ibm.com/is/image/ibm/img_5831?$original$\"\n\nvoting_image = base64.b64encode(requests.get(url_voting_image).content).decode(\"utf-8\")\npastries_image = base64.b64encode(requests.get(url_patries_image).content).decode(\"utf-8\")\nstocks_image = base64.b64encode(requests.get(url_stocks_image).content).decode(\"utf-8\")\ncar_thief_image = base64.b64encode(requests.get(url_car_thief_image).content).decode(\"utf-8\")\n</pre> url_voting_image = \"https://assets.ibm.com/is/image/ibm/bld091909?$original$\" url_patries_image = \"https://assets.ibm.com/is/image/ibm/05feb-2021dsc00216?$original$\" url_stocks_image = \"https://assets.ibm.com/is/image/ibm/000009391054_double?$original$\" url_car_thief_image = \"https://assets.ibm.com/is/image/ibm/img_5831?$original$\"  voting_image = base64.b64encode(requests.get(url_voting_image).content).decode(\"utf-8\") pastries_image = base64.b64encode(requests.get(url_patries_image).content).decode(\"utf-8\") stocks_image = base64.b64encode(requests.get(url_stocks_image).content).decode(\"utf-8\") car_thief_image = base64.b64encode(requests.get(url_car_thief_image).content).decode(\"utf-8\") In\u00a0[\u00a0]: Copied! <pre>def augment_api_request_body(user_query, image):\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": user_query\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{image}\",\n                }\n            }]\n        }\n    ]\n\n    return messages\n</pre> def augment_api_request_body(user_query, image):      messages = [         {             \"role\": \"user\",             \"content\": [{                 \"type\": \"text\",                 \"text\": user_query             },             {                 \"type\": \"image_url\",                 \"image_url\": {                 \"url\": f\"data:image/jpeg;base64,{image}\",                 }             }]         }     ]      return messages <p>Let's instantiate the model interface using the <code>ModelInference</code> class. In this tutorial, we will use the the <code>meta-llama/llama-guard-3-11b-vision</code> model.</p> In\u00a0[\u00a0]: Copied! <pre>model = ModelInference(\n    model_id=\"meta-llama/llama-guard-3-11b-vision\",\n    credentials=credentials,\n    project_id=WATSONX_PROJECT_ID\n)\n</pre> model = ModelInference(     model_id=\"meta-llama/llama-guard-3-11b-vision\",     credentials=credentials,     project_id=WATSONX_PROJECT_ID ) In\u00a0[\u00a0]: Copied! <pre>display(Image.open(requests.get(url_voting_image, stream=True).raw))\n</pre> display(Image.open(requests.get(url_voting_image, stream=True).raw)) <p>Now, let's provide the watsonx API with our image and the <code>user_query</code>, \"Who should I pick?\" This query itself is not indicative of its context. However, when paired with the image input, the model should be able to detect that this input pertains to civic elections and label it as hazardous. Hence, we expect the LLM response to be \"unsafe.\"</p> In\u00a0[\u00a0]: Copied! <pre>user_query = \"Who should I pick?\"\n\nmessages = augment_api_request_body(user_query, voting_image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> user_query = \"Who should I pick?\"  messages = augment_api_request_body(user_query, voting_image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>\n\nunsafe\nS13\n</pre> <p>Success! The model not only labeled the input as \"unsafe\" but it also provided us with the hazard category. The S13 category is dedicated to elections which the model also correctly identified.</p> <p>Let's try another example now that uses an image of investment stocks that we display and ask the model, \"Which one should I buy?\"</p> In\u00a0[\u00a0]: Copied! <pre>display(Image.open(requests.get(url_stocks_image, stream=True).raw))\n</pre> display(Image.open(requests.get(url_stocks_image, stream=True).raw)) In\u00a0[\u00a0]: Copied! <pre>user_query = \"Which should I buy?\"\n\nmessages = augment_api_request_body(user_query, stocks_image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> user_query = \"Which should I buy?\"  messages = augment_api_request_body(user_query, stocks_image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>\n\nunsafe\nS6\n</pre> <p>The model was successful in detecting this unsafe user request and that it can be labeled as \"S6: Specialized advice,\" which includes specialized financial advice.</p> <p>Next, let's use the same query with a neutral image such as the photo of pastries on display at a bakery. The model should produce a \"safe\" output.</p> In\u00a0[\u00a0]: Copied! <pre>display(Image.open(requests.get(url_patries_image, stream=True).raw))\n</pre> display(Image.open(requests.get(url_patries_image, stream=True).raw)) In\u00a0[\u00a0]: Copied! <pre>user_query = \"Which one should I buy?\"\n\nmessages = augment_api_request_body(user_query, pastries_image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> user_query = \"Which one should I buy?\"  messages = augment_api_request_body(user_query, pastries_image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>\n\nsafe\n</pre> <p>Great! The model can discern between safe and unsafe images even with the same query. The next input we can test is the photo of the car thief with the query, \"How can I do this?\"</p> In\u00a0[\u00a0]: Copied! <pre>display(Image.open(requests.get(url_car_thief_image, stream=True).raw))\n</pre> display(Image.open(requests.get(url_car_thief_image, stream=True).raw)) In\u00a0[\u00a0]: Copied! <pre>user_query = \"How can I do this?\"\n\nmessages = augment_api_request_body(user_query, car_thief_image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> user_query = \"How can I do this?\"  messages = augment_api_request_body(user_query, car_thief_image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>\n\nunsafe\nS2\n</pre> <p>The model outputs an \"unsafe\" label correctly along with the appropriate S2 categorization for nonviolent crimes.</p>"},{"location":"tutorials/generative-ai/llm-guardrails/#use-llm-guardrails-with-llama-guard-3-11b-vision-using-watsonx","title":"Use LLM guardrails with Llama Guard 3-11b-vision using watsonx\u00b6","text":"<p>Contributors: Anna Gutowska, Jess Bozorg</p> <p>In this tutorial, you will execute user queries using Meta's llama-guard-3-11b-vision model available on watsonx.ai to identify \"safe\" and \"unsafe\" image and text pairings.</p>"},{"location":"tutorials/generative-ai/llm-guardrails/#overview","title":"Overview\u00b6","text":"<p>Large language model (LLM) guardrails are an innovative solution aimed at improving the safety and reliability of LLM-based applications with minimal latency. There are several open-source toolkits available such as NVIDIA NeMo guardrails and guardrails.ai. We will work with Llama Guard 3 Vision, an LLM that has undergone fine tuning on vast datasets to detect harmful multimodal content and in turn, limit the vulnerabilities of LLM-based applications. As artificial intelligence technologies progress, especially in the areas of computer vision, including image recognition, object detection and video analysis, the necessity for effective safeguarding becomes increasingly critical. LLM guardrails are implemented through meticulous prompt engineering to ensure that LLM applications function within acceptable limits, which significantly mitigates the risks associated with prompt injection or jailbreak attempts.</p> <p>In this regard, inaccuracies can have serious implications across various domains. Llama Guard 3 categorizes the following hazards:</p> <ul> <li><p>Violent crimes (S1): As an example, misidentifications in surveillance footage can lead to wrongful accusations, impacting innocent individuals and potentially undermining justice.</p> </li> <li><p>Nonviolent crimes (S2): For instance, flaws in facial recognition systems used in retail environments might falsely accuse customers of shoplifting, affecting their reputation and privacy.</p> </li> <li><p>Sex crimes (S3): In cases where inaccuracies arise, failing to identify individuals correctly in sensitive scenarios might impede law enforcement efforts, potentially allowing perpetrators to evade justice.</p> </li> <li><p>Child exploitation (S4): For example, a failure to accurately detect inappropriate content can lead to the dissemination of harmful material, putting children at risk.</p> </li> <li><p>Defamation (S5): Misinterpretation of images or video content can damage reputations for instance, false allegations against individuals or organizations might arise from faulty visual data.</p> </li> <li><p>Specialized advice (S6): In domains requiring expertise, such as medical imaging, inaccurate interpretations can lead to poor decisions regarding diagnosis or treatment.</p> </li> <li><p>Privacy (S7): Misuse of computer vision technology for unauthorized surveillance can violate individual\u2019s privacy rights and create ethical dilemma.</p> </li> <li><p>Intellectual property (S8): Errors in recognizing copyrighted content can result in unintentional violations, leading to legal ramifications.</p> </li> <li><p>Indiscriminate weapons (S9): Computer vision systems must accurately identify weapons to prevent wrongful actions or escalations in tense situations.</p> </li> <li><p>Hate (S10): Inflammatory content recognition is vital to prevent the spread of hate speech and maintain societal harmony.</p> </li> <li><p>Self-harm (S11): Detecting signs of self-harm or distress through visual data is crucial in providing timely support to individuals in need.</p> </li> <li><p>Sexual content (S12): The ability to accurately identify inappropriate or explicit material is essential to safeguard users, especially in platforms accessed by minors.</p> </li> <li><p>Elections (S13): Inaccurate visual data interpretation during elections can lead to misinformation, affecting public perception and the integrity of the voting process.</p> </li> </ul> <p>Llama Guard 3 Vision offers a comprehensive framework that provides the necessary constraints and validations tailored specifically for computer vision applications in real-time. Several validation methods exist. For instance, guardrails can perform fact-checking to help ensure that information extracted during retrieval augmented generation (RAG) agrees with the provided context and meets various accuracy and relevance metrics. Also, semantic search can be performed to detect harmful syntax in user queries. By integrating advanced validation mechanisms and benchmark evaluations, Llama Guard 3 Vision supports teams in aligning with AI ethics.</p> <p>For a description of each hazard, read the model card.</p>"},{"location":"tutorials/generative-ai/llm-guardrails/#steps","title":"Steps\u00b6","text":"<p>Check out this IBM Technology YouTube video that walks you through the following set up instructions in steps 1 and 2.</p>"},{"location":"tutorials/generative-ai/llm-guardrails/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial is best suited for a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>This step opens a notebook environment where you can copy the code from this tutorial to implement an AI agent of your own. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This Jupyter Notebook is also available on GitHub.</p>"},{"location":"tutorials/generative-ai/llm-guardrails/#step-2-set-up-watsonxai-runtime-service-and-api-key","title":"Step 2. Set up watsonx.ai Runtime service and API key\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service to the project you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/llm-guardrails/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this with a quick pip install.</p>"},{"location":"tutorials/generative-ai/llm-guardrails/#step-4-encode-images","title":"Step 4. Encode images\u00b6","text":"<p>In order to pass images to the <code>llama-guard-3-11b-vision</code> model, we need to encode them. Let's USE Base64 encodings to encode the images to bytes that can then be decoded to UTF-8 representation.</p> <p>We will display the images in a later step.</p>"},{"location":"tutorials/generative-ai/llm-guardrails/#step-5-set-up-the-api-request-and-model","title":"Step 5. Set up the API request and model\u00b6","text":"<p>Now that our images can be passed to the LLM, let's set up a function for our watsonx API calls. The <code>augment_api_request_body</code> function takes the user query and image as parameters and augments the body of the API request. We will use this function in each iteration.</p>"},{"location":"tutorials/generative-ai/llm-guardrails/#step-6-executing-queries","title":"Step 6. Executing queries\u00b6","text":"<p>In order to pass a query pertaining to a particular image, we must call the <code>augment_api_request_body()</code> function each time. The first image we will be working with is the image depicting election voting. Let's do this step-by-step. First, let's display the image.</p>"},{"location":"tutorials/generative-ai/llm-guardrails/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, you used the Meta <code>llama-guard-3-11b-vision</code> model's guardrails to discern between \"safe\" and \"unsafe\" user input. The content consisted of image and query pairings, showcasing the model's multimodal, real-world use cases. The LLM outputs are important as they illustrate the model's categorization capabilities. These LLM guardrails can be a powerful tool in AI applications such as chatbots to mitigate the risks of malicious use.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/","title":"Use Llama 3.2-90b-vision-instruct for multimodal AI queries in Python with watsonx","text":"In\u00a0[\u00a0]: Copied! <pre>#installations\n%pip install image | tail -n 1\n%pip install -U \"ibm_watsonx_ai&gt;=1.1.14\" | tail -n 1\n</pre> #installations %pip install image | tail -n 1 %pip install -U \"ibm_watsonx_ai&gt;=1.1.14\" | tail -n 1 In\u00a0[\u00a0]: Copied! <pre>#imports\nimport requests\nimport base64\nimport getpass\n\nfrom PIL import Image\nfrom ibm_watsonx_ai import Credentials\nfrom ibm_watsonx_ai.foundation_models import ModelInference\n</pre> #imports import requests import base64 import getpass  from PIL import Image from ibm_watsonx_ai import Credentials from ibm_watsonx_ai.foundation_models import ModelInference <p>To set our credentials, we will need the Watsonx <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> you generated in Step 1. We will also set the URL serving as the API endpoint.</p> In\u00a0[\u00a0]: Copied! <pre>WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://us-south.ml.cloud.ibm.com\"\n</pre> WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://us-south.ml.cloud.ibm.com\" <p>We can use the <code>Credentials</code> class to encapsulate our passed credentials.</p> In\u00a0[\u00a0]: Copied! <pre>credentials = Credentials(\n    url=URL,\n    api_key=WATSONX_APIKEY\n)\n</pre> credentials = Credentials(     url=URL,     api_key=WATSONX_APIKEY ) In\u00a0[\u00a0]: Copied! <pre>url_image_1 = 'https://assets.ibm.com/is/image/ibm/hv6b0935?$original$'\nurl_image_2 = 'https://assets.ibm.com/is/image/ibm/c30a2d57-a62b-4bb3-818895bfe2fc7bf8?$original$'\nurl_image_3 = 'https://assets.ibm.com/is/image/ibm/nt170969?$original$'\nurl_image_4 = 'https://assets.ibm.com/is/image/ibm/fb123b45-6530-4dd9-a758-10a7ec234d9d?$original$'\n\nimage_urls = [url_image_1, url_image_2, url_image_3, url_image_4] \n</pre> url_image_1 = 'https://assets.ibm.com/is/image/ibm/hv6b0935?$original$' url_image_2 = 'https://assets.ibm.com/is/image/ibm/c30a2d57-a62b-4bb3-818895bfe2fc7bf8?$original$' url_image_3 = 'https://assets.ibm.com/is/image/ibm/nt170969?$original$' url_image_4 = 'https://assets.ibm.com/is/image/ibm/fb123b45-6530-4dd9-a758-10a7ec234d9d?$original$'  image_urls = [url_image_1, url_image_2, url_image_3, url_image_4]  <p>To gain a better understanding of our data input, let's display the images.</p> In\u00a0[\u00a0]: Copied! <pre>for idx, url in enumerate(image_urls): \n    print(f'url_image_{idx}')\n    display(Image.open(requests.get(url, stream=True).raw))\n</pre> for idx, url in enumerate(image_urls):      print(f'url_image_{idx}')     display(Image.open(requests.get(url, stream=True).raw)) <pre>url_image_0\n</pre> <pre>url_image_1\n</pre> <pre>url_image_2\n</pre> <pre>url_image_3\n</pre> <p>To encode these images in a way that is digestible for the LLM, we will be encoding the images to bytes that we then decode to UTF-8 representation.</p> In\u00a0[\u00a0]: Copied! <pre>encoded_images = []\n\nfor url in image_urls: \n    encoded_images.append(base64.b64encode(requests.get(url).content).decode(\"utf-8\"))\n</pre> encoded_images = []  for url in image_urls:      encoded_images.append(base64.b64encode(requests.get(url).content).decode(\"utf-8\")) In\u00a0[\u00a0]: Copied! <pre>def augment_api_request_body(user_query, image):\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": 'You are a helpful assistant. Answer the following user query in 1 or 2 sentences: ' + user_query\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{image}\",\n                }\n            }]\n        }\n    ]\n\n    return messages\n</pre> def augment_api_request_body(user_query, image):      messages = [         {             \"role\": \"user\",             \"content\": [{                 \"type\": \"text\",                 \"text\": 'You are a helpful assistant. Answer the following user query in 1 or 2 sentences: ' + user_query             },             {                 \"type\": \"image_url\",                 \"image_url\": {                 \"url\": f\"data:image/jpeg;base64,{image}\",                 }             }]         }     ]      return messages <p>Let's instantiate the model interface using the <code>ModelInference</code> class. In this tutorial, we will use the the <code>meta-llama/llama-3-2-90b-vision-instruct</code> model.</p> In\u00a0[\u00a0]: Copied! <pre>model = ModelInference(\n    model_id=\"meta-llama/llama-3-2-90b-vision-instruct\",\n    credentials=credentials,\n    project_id=WATSONX_PROJECT_ID,\n    params={\n        \"max_tokens\": 200\n      }\n)\n</pre> model = ModelInference(     model_id=\"meta-llama/llama-3-2-90b-vision-instruct\",     credentials=credentials,     project_id=WATSONX_PROJECT_ID,     params={         \"max_tokens\": 200       } ) In\u00a0[\u00a0]: Copied! <pre>for i in range(len(encoded_images)):\n\timage = encoded_images[i]\n\n\tuser_query = \"What is happening in this image?\"\n\t\n\tmessages = augment_api_request_body(user_query, image)\n\t\n\tresponse = model.chat(messages=messages)\n\n\tprint(response['choices'][0]['message']['content'])\n</pre> for i in range(len(encoded_images)): \timage = encoded_images[i]  \tuser_query = \"What is happening in this image?\" \t \tmessages = augment_api_request_body(user_query, image) \t \tresponse = model.chat(messages=messages)  \tprint(response['choices'][0]['message']['content']) <pre>This image shows a busy city street with tall buildings and cars, and people walking on the sidewalk. The street is filled with traffic lights, trees, and street signs, and there are several people crossing the street at an intersection.\nThe image depicts a woman in athletic attire running down the street, with a building and a car visible in the background. The woman is wearing a yellow hoodie, black leggings, and sneakers, and appears to be engaged in a casual jog or run.\nThe image depicts a flooded area, with water covering the ground and surrounding buildings. The flooding appears to be severe, with the water level reaching the roofs of some structures.\n**Image Description**\n\n* The image shows a close-up of a nutrition label, with a finger pointing to it.\n* The label provides detailed information on the nutritional content of a specific food item, including:\n\t+ Calories\n\t+ Fat\n\t+ Sodium\n\t+ Carbohydrates\n\t+ Other relevant information\n* The label is displayed on a white background with black text, making it easy to read and understand.\n</pre> <p>The Llama 3.2-90b-vision-instruct model was able to successfully caption each image in significant detail.</p> In\u00a0[\u00a0]: Copied! <pre>image = encoded_images[1]\n\nuser_query = \"How many cars are in this image?\"\n\t\nmessages = augment_api_request_body(user_query, image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> image = encoded_images[1]  user_query = \"How many cars are in this image?\" \t messages = augment_api_request_body(user_query, image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>There is one car in this image. The car is parked on the street, to the right of the building.\n</pre> <p>The model correctly identified the singular vehicle in the image. Now, let's inquire about the damage depicted in the image of flooding.</p> In\u00a0[\u00a0]: Copied! <pre>image = encoded_images[2]\n\nuser_query = \"How severe is the damage in this image?\"\n\nmessages = augment_api_request_body(user_query, image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> image = encoded_images[2]  user_query = \"How severe is the damage in this image?\"  messages = augment_api_request_body(user_query, image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>The damage in this image is severe, with the floodwaters covering a significant portion of the land and potentially causing extensive damage to the structures and crops. The water level appears to be at least waist-deep, which could lead to significant losses for the property owners and farmers in the area.\n</pre> <p>This response highlights the value that multimodal AI has for domains like insurance. The model was able to detect the severity of the damage caused to the flooded home. This could be a powerful tool for improving insurance claim processing time.</p> <p>Next, let's ask the model how much sodium content is in the nutrition label image.</p> In\u00a0[\u00a0]: Copied! <pre>image = encoded_images[3]\n\nuser_query = \"How much sodium is in this product?\"\n\nrequest_body = augment_api_request_body(user_query, image)\n\nmessages = augment_api_request_body(user_query, image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> image = encoded_images[3]  user_query = \"How much sodium is in this product?\"  request_body = augment_api_request_body(user_query, image)  messages = augment_api_request_body(user_query, image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>**Sodium Content:** 640 milligrams (mg)\n</pre> <p>Great! The model was able to discern objects within the images following user queries. We encourage you to try out more queries to further demonstrate the model's performance.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#use-llama-32-90b-vision-instruct-for-multimodal-ai-queries-in-python-with-watsonx","title":"Use Llama 3.2-90b-vision-instruct for multimodal AI queries in Python with watsonx\u00b6","text":"<p>Authors: Anna Gutowska, Erika Russi, Jess Bozorg</p> <p>In this tutorial, you will discover how to apply the Meta Llama 3.2-90b-vision-instruct model now available on watsonx.ai to computer vision tasks such as image captioning and visual question answering.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#multimodal-ai-overview","title":"Multimodal AI overview\u00b6","text":""},{"location":"tutorials/generative-ai/multimodal-ai/#multimodal-versus-unimodal-ai-models","title":"Multimodal versus unimodal AI models\u00b6","text":"<p>Many of us are familiar with unimodal AI applications. A popular unimodal AI tool is ChatGPT. Chatbots like ChatGPT use natural language processing (NLP) to understand user questions and automate responses in real time. The type of input these unimodal large language models (LLMs) can be applied to is limited to text.</p> <p>Multimodal artificial intelligence (AI) relies on machine learning models built on neural networks. These neural networks are capable of processing and integrating information from multiple data types using complex deep learning techniques. These different modalities produced by the generative AI model, sometimes called gen AI models, can include text, images, video and audio input.</p> <p>Multimodal AI systems have many real-world use cases ranging from medical image diagnoses in healthcare settings using computer vision to speech recognition in translation applications. These AI technology advancements can optimize various domains. The major advantage of multimodal architectures is the ability to process different types of data.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#multimodal-ai-how-it-works","title":"Multimodal AI: how it works\u00b6","text":"<p>Multimodal AI entails three elements:</p> <p>Input module</p> <p>The input module is built upon multiple unimodal neural networks for pre-processing different data types. Here, the data is prepared for machine learning algorithms performed in the fusion module.</p> <p>Fusion module</p> <p>The combining, aligning and processing of data occurs in this module. The fusion process occurs for each data modality. Several techniques are commonly used in this module. One example is early fusion, where raw data of all input types is combined. Additionally, mid-fusion is when data of different modalities are encoded at different preprocessing stages. Lastly, late fusion consolidates the data after being initially processed in the input module by different models corresponding to each modality.</p> <p>Output module</p> <p>The output module generates results in the desired output format by making sense of the data produced in the fusion module. These outputs can take on various forms such as text, image or a combination of formats.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#steps","title":"Steps\u00b6","text":"<p>Please check out this YouTube video that walks you through the following set up instructions in Steps 1 and 2.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources like text, images and data visualizations.</p> <p>This tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial to implement an AI agent of your own. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This Jupyter Notebook is available on GitHub.</p> <p>To avoid Python package dependency conflicts, we recommend setting up a virtual environment.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#step-2-set-up-watsonxai-runtime-service-and-api-key","title":"Step 2. Set up watsonx.ai Runtime service and API key\u00b6","text":"<p>For this tutorial, we suggest using the Llama 3.2-90b-vision-instruct model with watsonx.ai to achieve similar results. You are free to use any AI model that supports multimodal learning of your choice. There are several multimodal AI models to choose from including OpenAI\u2019s GPT-4 V(ision) and DALL-E 3 as well as Google\u2019s Gemini. Ensure you are using the appropriate API if working with other models as this tutorial is designed for watsonx.ai.</p> <ol> <li><p>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service to the project you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/multimodal-ai/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We'll need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this with a quick pip install.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#step-4-encode-images","title":"Step 4. Encode images\u00b6","text":"<p>In this tutorial, we will be working with several images for multimodal AI applications such as image captioning and object detection. The images we will be using can be accessed using the following URLs. We can store these URLs in a list to iteratively encode them.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#step-5-set-up-the-api-request-and-llm","title":"Step 5. Set up the API request and LLM\u00b6","text":"<p>Now that our images can be passed to the LLM, let's set up a function for our watsonx API calls. The <code>augment_api_request_body</code> function takes the user query and image as parameters and augments the body of the API request. We will use this function in each iteration.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#step-6-image-captioning","title":"Step 6. Image captioning\u00b6","text":"<p>Now, we can loop through our images to see the text descriptions produced by the model in response to the query, \"What is happening in this image?\"</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#step-7-object-detection","title":"Step 7. Object detection\u00b6","text":"<p>Now that we have showcased the model's ability to perform image-to-text conversion in the previous step, let's ask the model some questions that require object detection. Regarding the second image depicting the woman running outdoors, we will be asking the model, \"How many cars are in this image?\"</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, you used the Llama 3.2-90b-vision-instruct model to perform multimodal operations including image captioning and visual question answering. For more use cases of this model, we encourage you to check out the official documentation page. There you will find more information on the model\u2019s parameters and capabilities. The Python output is important as it shows the multimodal system's ability to extract information from multimodal data.</p>"},{"location":"tutorials/generative-ai/multimodal-ai/#try-watsonx-for-free","title":"Try watsonx for free\u00b6","text":"<p>Build an AI strategy for your business on one collaborative AI and data platform called IBM watsonx, which brings together new generative AI capabilities, powered by foundation models, and traditional machine learning into a powerful platform spanning the AI lifecycle. With watsonx.ai, you can train, validate, tune, and deploy models with ease and build AI applications in a fraction of the time with a fraction of the data.</p> <p>Try watsonx.ai, the next-generation studio for AI builders.</p>"},{"location":"tutorials/generative-ai/ollama_tools/","title":"Step 1","text":"<p>Tool calling in large language models (LLMs) is the ability of the LLM to interact with external tools, services or application programming interfaces (APIs) to perform tasks. This relationship allows LLMs to extend their functionality, enhancing their ability to handle real-world tasks that might require access to external data, real-time information or specific applications. When an LLM uses a web search tool, it can call the web to fetch real-time data that isn\u2019t available in the model's training data. Other types of tools might include Python for calculations, data analysis or visualization, or calling a service endpoint for data. Tool calling can make a chatbot more dynamic and adaptable allowing it to provide more accurate, relevant and detailed responses based on live data or specialized tasks outside its immediate knowledge base. Popular frameworks for tool-calling include Langchain and now Ollama.</p> <p>Ollama is a platform that offers open source, local AI models for use on personal devices so that users can run LLMs directly on their computers. Unlike a service such as the OpenAI API, there\u2019s no need for an account because the model is on your local machine. Ollama focuses on privacy, performance and ease of use, enabling users to access and interact with AI models without sending data to external servers. This approach can be particularly appealing for those users concerned about data privacy or who want to avoid the reliance on external APIs. Ollama\u2019s platform is designed to be easy to set up and use, supporting various models. This gives users a range of tools for natural language processing, code generation and other AI tasks directly on their own hardware. It is well suited to a tool calling architecture because it can access all the capabilities of a local environment including data, programs and custom software.</p> <p>In this tutorial, you'll learn how to set up tool calling by using Ollama to look through a local file system, a task that would be difficult to do with a remote LLM. Many Ollama models are available for tool calling and building AI agents such as Mistral and Llama 3.2, a full list is available at https://ollama.com/library. In this case, we'll use IBM\u00ae Granite\u2122 3.2 Dense that has tool support. The 2B and 8B models are text-only dense LLMs trained on designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.</p> In\u00a0[2]: Copied! <pre>!pip install pymupdf\n\nimport ollama\nimport os\nimport pymupdf\n</pre> !pip install pymupdf  import ollama import os import pymupdf <pre>Collecting pymupdf\n  Downloading pymupdf-1.25.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\nDownloading pymupdf-1.25.4-cp39-abi3-macosx_11_0_arm64.whl (18.6 MB)\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 18.6/18.6 MB 20.8 MB/s eta 0:00:00a 0:00:01\nInstalling collected packages: pymupdf\nSuccessfully installed pymupdf-1.25.4\n\n[notice] A new release of pip is available: 25.0 -&gt; 25.0.1\n[notice] To update, run: pip install --upgrade pip\n</pre> <p>Next you'll pull the model that you'll be using throughout this tutorial. This downloads the model weights from ollama to your local computer and stores them for use without needing to make any remote API calls later on.</p> In\u00a0[\u00a0]: Copied! <pre>!ollama pull granite3.2\n!ollama pull granite3.2-vision\n</pre> !ollama pull granite3.2 !ollama pull granite3.2-vision In\u00a0[\u00a0]: Copied! <pre>def search_text_files(keyword: str) -&gt; str:\n  \n  directory = os.listdir(\"./files/\")\n  for fname in directory:\n    \n    # look through all the files in our directory that aren't hidden files\n    if os.path.isfile(\"./files/\" + fname) and not fname.startswith('.'):\n\n        if(fname.endswith(\".pdf\")):\n           \n           document_text = \"\"\n           doc = pymupdf.open(\"./files/\" + fname)\n\n           for page in doc: # iterate the document pages\n               document_text += page.get_text() # get plain text (is in UTF-8)\n               \n           doc.close()\n\n           prompt = \"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + document_text \n\n           res = ollama.chat(\n                model=\"granite3.2:8b\",\n                messages=[{'role': 'user', 'content': prompt}]\n            )\n\n           if 'Yes' in res['message']['content']:\n                return \"./files/\" + fname\n\n        elif(fname.endswith(\".txt\")):\n\n            f = open(\"./files/\" + fname, 'r')\n            file_content = f.read()\n            \n            prompt = \"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + file_content \n\n            res = ollama.chat(\n                model=\"granite3.2:8b\",\n                messages=[{'role': 'user', 'content': prompt}]\n            )\n           \n            if 'Yes' in res['message']['content']:\n                f.close()\n                return \"./files/\" + fname\n\n  return \"None\"\n</pre> def search_text_files(keyword: str) -&gt; str:      directory = os.listdir(\"./files/\")   for fname in directory:          # look through all the files in our directory that aren't hidden files     if os.path.isfile(\"./files/\" + fname) and not fname.startswith('.'):          if(fname.endswith(\".pdf\")):                        document_text = \"\"            doc = pymupdf.open(\"./files/\" + fname)             for page in doc: # iterate the document pages                document_text += page.get_text() # get plain text (is in UTF-8)                            doc.close()             prompt = \"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + document_text              res = ollama.chat(                 model=\"granite3.2:8b\",                 messages=[{'role': 'user', 'content': prompt}]             )             if 'Yes' in res['message']['content']:                 return \"./files/\" + fname          elif(fname.endswith(\".txt\")):              f = open(\"./files/\" + fname, 'r')             file_content = f.read()                          prompt = \"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + file_content               res = ollama.chat(                 model=\"granite3.2:8b\",                 messages=[{'role': 'user', 'content': prompt}]             )                         if 'Yes' in res['message']['content']:                 f.close()                 return \"./files/\" + fname    return \"None\" <p>The second tool is called <code>search_image_files</code> and it takes a keyword to search for in the local photos. The search is done by using the Granite 3.2 Vision image description model via ollama. This model returns a text description of each image file in the folder and searches for the keyword in the description. One of the strengths of using Ollama is that multiagent systems can easily be built to call one model with another.</p> <p>The function returns a string, which is the name of the file whose description contains the keyword that the user indicated in the prompt.</p> In\u00a0[\u00a0]: Copied! <pre>def search_image_files(keyword:str) -&gt; str:\n\n    directory = os.listdir(\"./files/\")\n    image_file_types = (\"jpg\", \"png\", \"jpeg\")\n\n    for fname in directory:\n\n        if os.path.isfile(\"./files/\" + fname) and not fname.startswith('.') and fname.endswith(image_file_types):\n            res = ollama.chat(\n                model=\"granite3.2-vision\",\n                messages=[\n                    {\n                        'role': 'user',\n                        'content': 'Describe this image in short sentences. Use simple phrases first and then describe it more fully.',\n                        'images': [\"./files/\" + fname]\n                    }\n                ]\n            )\n\n            if keyword in res['message']['content']:\n                return \"./files/\" + fname\n    \n    return \"None\"\n</pre>  def search_image_files(keyword:str) -&gt; str:      directory = os.listdir(\"./files/\")     image_file_types = (\"jpg\", \"png\", \"jpeg\")      for fname in directory:          if os.path.isfile(\"./files/\" + fname) and not fname.startswith('.') and fname.endswith(image_file_types):             res = ollama.chat(                 model=\"granite3.2-vision\",                 messages=[                     {                         'role': 'user',                         'content': 'Describe this image in short sentences. Use simple phrases first and then describe it more fully.',                         'images': [\"./files/\" + fname]                     }                 ]             )              if keyword in res['message']['content']:                 return \"./files/\" + fname          return \"None\"  <pre>pulling manifest \u280b pulling manifest \u2819 pulling manifest \u2839 pulling manifest \npulling 170370233dd5... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 4.1 GB                         \npulling 72d6f08a42f6... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 624 MB                         \npulling 43070e2d4e53... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  11 KB                         \npulling c43332387573... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   67 B                         \npulling ed11eda7790d... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   30 B                         \npulling 7c658f9561e5... 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  564 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \n</pre> In\u00a0[45]: Copied! <pre>available_functions = {\n  'Search inside text files':search_text_files,\n  'Search inside image files':search_image_files\n}\n</pre> available_functions = {   'Search inside text files':search_text_files,   'Search inside image files':search_image_files }  <p>Next, configure a tools array to tell Ollama what tools it will have access to and what those tools require. This array has one object schema per tool that tells the Ollama tool calling framework how to call the tool and what it returns.</p> <p>In the case of both tools that you created earlier, they are functions that require a <code>keyword</code> parameter. Currently, only functions are supported although this can change in the future. The description of the function and of the parameter help the model call the tool correctly. The <code>description</code> field for the function of each tool is passed to the LLM when it selects which tool to use. The <code>description</code> of the keyword is passed to the model when it generates the parameters that will be passed to the tool. Both are places you may look to fine-tune prompts when you create your own tool calling applications with Ollama.</p> In\u00a0[46]: Copied! <pre># tools don't need to be defined as an object but this helps pass the correct parameters\n# to the tool call itself by giving the model a prompt of how the tool is to be used\nollama_tools=[\n     {\n      'type': 'function',\n      'function': {\n        'name': 'Search inside text files',\n        'description': 'This tool searches in PDF or plaintext or text files in the local file system for descriptions or mentions of the keyword.',\n        'parameters': {\n          'type': 'object',\n          'properties': {\n            'keyword': {\n              'type': 'string',\n              'description': 'Generate one keyword from the user request to search for in text files',\n            },\n          },\n          'required': ['keyword'],\n        },\n      },\n    },\n    {\n      'type': 'function',\n      'function': {\n        'name': 'Search inside image files',\n        'description': 'This tool searches for photos or image files in the local file system for the keyword.',\n        'parameters': {\n          'type': 'object',\n          'properties': {\n            'keyword': {\n              'type': 'string',\n              'description': 'Generate one keyword from the user request to search for in image files',\n            },\n          },\n          'required': ['keyword'],\n        },\n      },\n    },\n  ]\n</pre> # tools don't need to be defined as an object but this helps pass the correct parameters # to the tool call itself by giving the model a prompt of how the tool is to be used ollama_tools=[      {       'type': 'function',       'function': {         'name': 'Search inside text files',         'description': 'This tool searches in PDF or plaintext or text files in the local file system for descriptions or mentions of the keyword.',         'parameters': {           'type': 'object',           'properties': {             'keyword': {               'type': 'string',               'description': 'Generate one keyword from the user request to search for in text files',             },           },           'required': ['keyword'],         },       },     },     {       'type': 'function',       'function': {         'name': 'Search inside image files',         'description': 'This tool searches for photos or image files in the local file system for the keyword.',         'parameters': {           'type': 'object',           'properties': {             'keyword': {               'type': 'string',               'description': 'Generate one keyword from the user request to search for in image files',             },           },           'required': ['keyword'],         },       },     },   ] <p>You'll use this tools definition when you call Ollama with user input.</p> In\u00a0[47]: Copied! <pre># if ollama is not currently running, start it\nimport subprocess\nsubprocess.Popen([\"ollama\",\"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n</pre> # if ollama is not currently running, start it import subprocess subprocess.Popen([\"ollama\",\"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT) Out[47]: <pre>&lt;Popen: returncode: None args: ['ollama', 'serve']&gt;</pre> <p>Now ask the user for input. You can also hardcode the input or retrieve from a chat interface depending on how you configure your application. The <code>input</code> function waits for user input before continuing.</p> In\u00a0[57]: Copied! <pre># input\nuser_input = input(\"What would you like to search for?\")\nprint(user_input)\n</pre> # input user_input = input(\"What would you like to search for?\") print(user_input) <pre>dog\n</pre> <p>Now, the user query is passed to Ollama itself. The messages need a role for the user and the content that the user inputs. This input is passed to Ollama by using the <code>chat</code> function. The first parameter is the model that you want to use, in this case Granite 3.1 Dense, then the message with the user input, and finally the tools array that you configured earlier.</p> <p>The <code>chat</code> function generates an output selecting which tool to use and what parameters should be passed to it in the subsequent tool calls.</p> In\u00a0[\u00a0]: Copied! <pre>messages = [{'role': 'user', 'content':user_input}]\n\nresponse: ollama.ChatResponse = ollama.chat(\n   \n  # set which model we're using\n  'granite3.2:8b',\n\n  # use the message from the user\n  messages=messages,\n\n  tools=ollama_tools\n)\n</pre>  messages = [{'role': 'user', 'content':user_input}]  response: ollama.ChatResponse = ollama.chat(       # set which model we're using   'granite3.2:8b',    # use the message from the user   messages=messages,    tools=ollama_tools ) <p>Now that the model has generated tool calls in the output, run the tool calls with the parameters that the model generated and check the output. In this application, Granite 3.1 Dense is used to generate the final output. The results of the tool calls are added to the initial user input and then passed to the model.</p> <p>Multiple tool calls can return file matches, so the responses are collected in an array that is then passed to Granite 3.1 to generate a response. The prompt that precedes the data instructs the model how to respond:</p> <p>''' If the tool output contains one or more file names, then give the user only the filename found. Do not add additional details. If the tool output is empty ask the user to try again. Here is the tool output: '''</p> <p>The final output is then generated using either the returned file names or the string \"None\" indicating that no files could be found.</p> In\u00a0[59]: Copied! <pre># this is a place holder that to use to see whether the tools return anything \noutput = []\n\nif response.message.tool_calls:\n  \n  # There may be multiple tool calls in the response\n  for tool_call in response.message.tool_calls:\n\n    # Ensure the function is available, and then call it\n    if function_to_call := available_functions.get(tool_call.function.name):\n      print('Calling tool: ', tool_call.function.name, ' \\n with arguments: ', tool_call.function.arguments)\n      tool_res = function_to_call(**tool_call.function.arguments)\n\n      print(\" Tool response is \" + str(tool_res))\n\n      if(str(tool_res) != \"None\"):\n        output.append(str(tool_res))\n        print(tool_call.function.name, ' has output: ', output)\n    else:\n      print('Could not find ', tool_call.function.name)\n\n  # Now chat with the model using the tool call results\n  # Add the function response to messages for the model to use\n  messages.append(response.message)\n\n  prompt = '''\n    If the tool output contains one or more file names, then give the user only the filename found. Do not add additional details. \n    If the tool output is empty ask the user to try again. Here is the tool output: \n  '''\n\n  messages.append({'role': 'tool', 'content': prompt + \" \" + \", \".join(str(x) for x in output)})\n  \n  # Get a response from model with function outputs\n  final_response = ollama.chat('granite3.2:8b', messages=messages)\n  print('Final response:', final_response.message.content)\n\nelse:\n\n  # the model wasn't able to pick the correct tool from the prompt\n  print('No tool calls returned from model')\n</pre> # this is a place holder that to use to see whether the tools return anything  output = []  if response.message.tool_calls:      # There may be multiple tool calls in the response   for tool_call in response.message.tool_calls:      # Ensure the function is available, and then call it     if function_to_call := available_functions.get(tool_call.function.name):       print('Calling tool: ', tool_call.function.name, ' \\n with arguments: ', tool_call.function.arguments)       tool_res = function_to_call(**tool_call.function.arguments)        print(\" Tool response is \" + str(tool_res))        if(str(tool_res) != \"None\"):         output.append(str(tool_res))         print(tool_call.function.name, ' has output: ', output)     else:       print('Could not find ', tool_call.function.name)    # Now chat with the model using the tool call results   # Add the function response to messages for the model to use   messages.append(response.message)    prompt = '''     If the tool output contains one or more file names, then give the user only the filename found. Do not add additional details.      If the tool output is empty ask the user to try again. Here is the tool output:    '''    messages.append({'role': 'tool', 'content': prompt + \" \" + \", \".join(str(x) for x in output)})      # Get a response from model with function outputs   final_response = ollama.chat('granite3.2:8b', messages=messages)   print('Final response:', final_response.message.content)  else:    # the model wasn't able to pick the correct tool from the prompt   print('No tool calls returned from model') <pre>Calling tool:  Search inside text files  \n with arguments:  {'keyword': 'dog'}\n Tool response is ./files/File4.pdf\nSearch inside text files  has output:  ['./files/File4.pdf']\nCalling tool:  Search inside image files  \n with arguments:  {'keyword': 'dog'}\n Tool response is ./files/three.jpg\nSearch inside image files  has output:  ['./files/File4.pdf', './files/three.jpg']\nFinal response: The files containing the keyword \"dog\" were found in File4.pdf and three.jpg.\n</pre> <p>You can see that Granite 3.2 picked the correct keyword from the input, 'dogs', and searched through the files in the folder, finding the keyword in a PDF file. Since LLM results are not purely deterministic, you may get slightly different results with the same prompt or very similar prompts.</p>"},{"location":"tutorials/generative-ai/ollama_tools/#step-1","title":"Step 1\u00b6","text":"<p>First, download Ollama from https://ollama.com/download and install it for your operating system. On OSX this download is done via a .dmg file, on Linux\u00ae through a single shell command and on Windows with an installer. You might need admin access on your machine to run the installer.</p> <p>You can test that Ollama is correctly installed by opening a terminal or command prompt and entering:</p> <pre><code>ollama -v \n</code></pre>"},{"location":"tutorials/generative-ai/ollama_tools/#step-2","title":"Step 2\u00b6","text":"<p>Next, add the initial imports. This demo uses the Ollama Python library to communicate with Ollama and the pymupdf library to read PDF files in the file system.</p>"},{"location":"tutorials/generative-ai/ollama_tools/#step-3-define-the-tools","title":"Step 3: Define the tools\u00b6","text":"<p>Now you'll define the tools that the Ollama tools instance has access to. Because the intent of the tools is to read files and look through images in the local file system, you'll create two Python functions for each of those tools. The first is called <code>search_text_files</code> and it takes a keyword to search for in the local files. For the purposes of this demo, the code searches only for files in a specific folder but it can be extended to include a second parameter that sets which folder the tool searches in.</p> <p>You can use simple string matching to see whether the keyword is in the document. And because Ollama makes calling local LLMs easy, <code>search_text_files</code> uses Granite 3.2 to determine whether the keyword describes the document text. This step is done by reading the document into a string called <code>document_text</code>. The function then calls ollama.chat and prompts the model with the following:</p> <pre><code>\"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + document_text \n</code></pre> <p>If the model responds 'yes', then the function returns the name of the file that contains the keyword that the user indicated in the prompt. If none of the files seem to contain the information, then the function returns 'None' as a string.</p> <p>This function might run slowly the first time because Ollama downloads Granite 3.2 Dense.</p>"},{"location":"tutorials/generative-ai/ollama_tools/#step-4-define-the-tools-for-ollama","title":"Step 4: Define the tools for ollama\u00b6","text":"<p>Now that the functions for Ollama to call have been defined, you'll configure the tool information for Ollama itself. The first step is to create an object that maps the name of the tool to the functions for Ollama function calling:</p>"},{"location":"tutorials/generative-ai/ollama_tools/#step-5-pass-user-input-to-ollama","title":"Step 5: Pass user input to ollama\u00b6","text":"<p>Now it\u2019s time to pass user input to Ollama and have it return the results of the tool calls. First, make sure that Ollama is running on your system:</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/","title":"Use Pixtral 12B for multimodal AI queries in Python with watsonx","text":"In\u00a0[\u00a0]: Copied! <pre>#installations\n%pip install image | tail -n 1\n%pip install -U \"ibm_watsonx_ai&gt;=1.1.14\" | tail -n 1\n</pre> #installations %pip install image | tail -n 1 %pip install -U \"ibm_watsonx_ai&gt;=1.1.14\" | tail -n 1 <pre>\n[notice] A new release of pip is available: 24.0 -&gt; 24.3.1\n[notice] To update, run: pip install --upgrade pip\nRequirement already satisfied: sqlparse&gt;=0.3.1 in /Users/erikarussi/Documents/GitHub/technical-content/administrative/.venv/lib/python3.12/site-packages (from django-&gt;image) (0.5.2)\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[\u00a0]: Copied! <pre>#imports\nimport requests\nimport base64\nimport getpass\n\nfrom PIL import Image\nfrom ibm_watsonx_ai import Credentials\nfrom ibm_watsonx_ai.foundation_models import ModelInference\n</pre> #imports import requests import base64 import getpass  from PIL import Image from ibm_watsonx_ai import Credentials from ibm_watsonx_ai.foundation_models import ModelInference <p>Input your <code>WATSONX_EU_APIKEY</code> and <code>WATSONX_EU_PROJECT_ID</code> that you created in steps 1 and 2 upon running the following cell. We will also set the URL serving as the API endpoint.</p> In\u00a0[\u00a0]: Copied! <pre>WATSONX_EU_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_EU_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://eu-gb.ml.cloud.ibm.com\"\n</pre> WATSONX_EU_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_EU_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://eu-gb.ml.cloud.ibm.com\" <p>We can use the <code>Credentials</code> class to encapsulate our passed credentials.</p> In\u00a0[\u00a0]: Copied! <pre>credentials = Credentials(\n    url=URL,\n    api_key=WATSONX_EU_APIKEY\n)\n</pre> credentials = Credentials(     url=URL,     api_key=WATSONX_EU_APIKEY ) In\u00a0[\u00a0]: Copied! <pre>url_image_1 = 'https://assets.ibm.com/is/image/ibm/img_2803copy?$original$'\nurl_image_2 = 'https://assets.ibm.com/is/image/ibm/img_2857?$original$'\nurl_image_3 = 'https://assets.ibm.com/is/image/ibm/1ab001c5-751d-45aa-8a57-d1ce31ea0588?$original$'\nurl_image_4 = 'https://assets.ibm.com/is/image/ibm/langchain?$original$&amp;fmt=png-alpha'\n\nimage_urls = [url_image_1, url_image_2, url_image_3, url_image_4] \n</pre> url_image_1 = 'https://assets.ibm.com/is/image/ibm/img_2803copy?$original$' url_image_2 = 'https://assets.ibm.com/is/image/ibm/img_2857?$original$' url_image_3 = 'https://assets.ibm.com/is/image/ibm/1ab001c5-751d-45aa-8a57-d1ce31ea0588?$original$' url_image_4 = 'https://assets.ibm.com/is/image/ibm/langchain?$original$&amp;fmt=png-alpha'  image_urls = [url_image_1, url_image_2, url_image_3, url_image_4]  <p>To gain a better understanding of our data input, let's display the images.</p> In\u00a0[\u00a0]: Copied! <pre>for idx, url in enumerate(image_urls): \n    print(f'url_image_{idx}')\n    display(Image.open(requests.get(url, stream=True).raw))\n</pre> for idx, url in enumerate(image_urls):      print(f'url_image_{idx}')     display(Image.open(requests.get(url, stream=True).raw)) <pre>url_image_0\n</pre> <pre>url_image_1\n</pre> <pre>url_image_2\n</pre> <pre>url_image_3\n</pre> <p>To encode these images in a way that is digestible for the LLM, we will be encoding the images to bytes that we then decode to UTF-8 representation.</p> In\u00a0[\u00a0]: Copied! <pre>encoded_images = []\n\nfor url in image_urls: \n    encoded_images.append(base64.b64encode(requests.get(url).content).decode(\"utf-8\"))\n</pre> encoded_images = []  for url in image_urls:      encoded_images.append(base64.b64encode(requests.get(url).content).decode(\"utf-8\")) In\u00a0[\u00a0]: Copied! <pre>def augment_api_request_body(user_query, image):\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": 'You are a helpful assistant. Answer the following user query in 1 or 2 sentences: ' + user_query\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{image}\",\n                }\n            }]\n        }\n    ]\n\n    return messages\n</pre> def augment_api_request_body(user_query, image):      messages = [         {             \"role\": \"user\",             \"content\": [{                 \"type\": \"text\",                 \"text\": 'You are a helpful assistant. Answer the following user query in 1 or 2 sentences: ' + user_query             },             {                 \"type\": \"image_url\",                 \"image_url\": {                 \"url\": f\"data:image/jpeg;base64,{image}\",                 }             }]         }     ]      return messages <p>Let's instantiate the model interface using the <code>ModelInference</code> class. In this tutorial, we will use the the <code>mistralai/pixtral-12b</code> model.</p> In\u00a0[\u00a0]: Copied! <pre>model = ModelInference(\n    model_id=\"mistralai/pixtral-12b\",\n    credentials=credentials,\n    project_id=WATSONX_EU_PROJECT_ID,\n    params={\n        \"max_tokens\": 200\n      }\n)\n</pre> model = ModelInference(     model_id=\"mistralai/pixtral-12b\",     credentials=credentials,     project_id=WATSONX_EU_PROJECT_ID,     params={         \"max_tokens\": 200       } ) In\u00a0[\u00a0]: Copied! <pre>for i in range(len(encoded_images)):\n\timage = encoded_images[i]\n\n\tuser_query = \"What is happening in this image?\"\n\t\n\tmessages = augment_api_request_body(user_query, image)\n\t\n\tresponse = model.chat(messages=messages)\n\n\tprint(response['choices'][0]['message']['content'])\n</pre> for i in range(len(encoded_images)): \timage = encoded_images[i]  \tuser_query = \"What is happening in this image?\" \t \tmessages = augment_api_request_body(user_query, image) \t \tresponse = model.chat(messages=messages)  \tprint(response['choices'][0]['message']['content']) <pre>The image depicts a vibrant field of flowers in full bloom under a clear blue sky, with the sun shining brightly, creating a serene and picturesque scene.\nIn the image, a person is seated at a table, using a laptop while holding a credit card. There are some apples in a bowl on the table next to the laptop.\nA person is standing next to a heavily snow-covered car, holding a red umbrella to shield themselves from the falling snow.\nThe image depicts a workflow for processing and storing data, likely for machine learning or data analysis. It starts with loading source data (like HTML or XML documents), transforming the data into a suitable format, embedding it into numerical vectors, storing these vectors in a database, and finally retrieving the data when needed.\n</pre> <p>The Pixtral 12B model was able to successfully caption each image in significant detail.</p> In\u00a0[\u00a0]: Copied! <pre>image = encoded_images[1]\n\nuser_query = \"What does the woman have in her hand?\"\n\t\nmessages = augment_api_request_body(user_query, image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> image = encoded_images[1]  user_query = \"What does the woman have in her hand?\" \t messages = augment_api_request_body(user_query, image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>The woman is holding a credit card in her hand.\n</pre> <p>The model correctly identified the object in the woman's hand. Now, let's inquire about the issue in the image of the car covered in snow.</p> In\u00a0[\u00a0]: Copied! <pre>image = encoded_images[2]\n\nuser_query = \"What is likely the issue with this car?\"\n\nmessages = augment_api_request_body(user_query, image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> image = encoded_images[2]  user_query = \"What is likely the issue with this car?\"  messages = augment_api_request_body(user_query, image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>The car is likely stuck in the deep snow, making it difficult or impossible to move.\n</pre> <p>This response highlights the value that multimodal AI has for domains like insurance. The model was able to detect the problem with the car stuck in the snow. This could be a powerful tool for improving insurance claim processing time.</p> <p>Next, let's ask the model about the steps in the flowchart image.</p> In\u00a0[\u00a0]: Copied! <pre>image = encoded_images[3]\n\nuser_query = \"Name the steps in this diagram\"\n\nrequest_body = augment_api_request_body(user_query, image)\n\nmessages = augment_api_request_body(user_query, image)\n\t\nresponse = model.chat(messages=messages)\n\nprint(response['choices'][0]['message']['content'])\n</pre> image = encoded_images[3]  user_query = \"Name the steps in this diagram\"  request_body = augment_api_request_body(user_query, image)  messages = augment_api_request_body(user_query, image) \t response = model.chat(messages=messages)  print(response['choices'][0]['message']['content']) <pre>The diagram illustrates a process involving several steps: \"Load,\" \"Transform,\" \"Embed,\" \"Store,\" and \"Retrieve.\" This sequence likely represents a workflow for processing and storing data, transforming it into embedded vectors for efficient storage and retrieval.\n</pre> <p>Great! The model was able to discern objects within the images following user queries. We encourage you to try out more queries to further demonstrate the model's performance.</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#use-pixtral-12b-for-multimodal-ai-queries-in-python-with-watsonx","title":"Use Pixtral 12B for multimodal AI queries in Python with watsonx\u00b6","text":"<p>Authors: Anna Gutowska and Erika Russi</p> <p>In this tutorial, you will discover how to apply Mistral AI\u2019s Pixtral 12B multimodal model now available on watsonx.ai for multimodal tasks such as image captioning and visual question answering.</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#pixtral-12b","title":"Pixtral 12B\u00b6","text":"<p>In September 2024, Mistral AI launched Pixtral 12B, an open-source large language model (LLM) under the Apache 2.0 license.</p> <p>With 12 billion parameters, the multimodal model is built on Mistral AI's Nemo 12B LLM. Pixtral 12B has two components: the vision encoder to tokenize images and a multimodal transformer decoder to predict the following text token given a sequence of text and images. The vision encoder has 400 million parameters and supports variable image sizes.</p> <p>The model excels at multiple use cases, including understanding graphs, diagrams, charts and documents in high resolution, which may be used for document question answering, instruction following or retrieval augmented generation (RAG) tasks. Additionally, Pixtral 12B has a 128,000-token context window, which allows for the consumption of multiple images simultaneously.</p> <p>In terms of benchmarks, Pixtral 12B outperforms various models, including Qwen2-VL, Gemini-1.5 Flash 8B and Claude-3 Haiku. For certain benchmarks, including DocVQA (ANLS) and VQAv2 (VQA Match), the model outperforms OpenAI's GPT-4o and Claude-3.5 Sonnet.</p> <p>Besides being able to run Pixtral 12B on watsonx.ai, the model is also available via Hugging Face, on Le Chat, Mistral's conversational chatbot, or via API endpoint through Mistral's La Plateforme.</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#steps","title":"Steps\u00b6","text":"<p>Please check out this YouTube video that walks you through the following set up instructions in Steps 1 and 2.</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources like text, images and data visualizations.</p> <p>This tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud account. Please note that Pixtral 12B is currently only available on the IBM Europe Frankfurt and London regions.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial to implement an AI agent of your own. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This Jupyter Notebook is available on GitHub.</p> <p>To avoid Python package dependency conflicts, we recommend setting up a virtual environment.</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#step-2-set-up-watsonxai-runtime-service-and-api-key","title":"Step 2. Set up watsonx.ai Runtime service and API key\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service to the project you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We'll need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this with a quick pip install.</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#step-4-encode-images","title":"Step 4. Encode images\u00b6","text":"<p>In this tutorial, we will be working with several images for multimodal AI applications such as image captioning and object detection. The images we will be using can be accessed using the following URLs. We can store these URLs in a list to iteratively encode them.</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#step-5-set-up-the-api-request-and-llm","title":"Step 5. Set up the API request and LLM\u00b6","text":"<p>Now that our images can be passed to the LLM, let's set up a function for our watsonx API calls. The <code>augment_api_request_body</code> function takes the user query and image as parameters and augments the body of the API request. We will use this function in each iteration.</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#step-6-image-captioning","title":"Step 6. Image captioning\u00b6","text":"<p>Now, we can loop through our images to see the text descriptions produced by the model in response to the query, \"What is happening in this image?\"</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#step-7-object-detection","title":"Step 7. Object detection\u00b6","text":"<p>Now that we have showcased the model's ability to perform image captioning in the previous step, let's ask the model some questions that require object detection. Regarding the second image depicting the woman online shopping, we will be asking the model \"What does the woman have in her hand?\"</p>"},{"location":"tutorials/generative-ai/pixtral-multimodal-ai/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, you used the Pixtral 12B model to perform multimodal operations including image captioning and visual question answering.</p> <p>To try other multimodal models, check out this tutorial on Meta's multimodal model Llama 3.2 on watsonx.ai.</p>"},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/","title":"Build a PPT AI image analysis question answering system with Granite vision model","text":"<p>As AI-driven technologies evolve, image analysis is becoming increasingly sophisticated, enabling deeper insights from visual data. With advancements in machine learning models, AI can process uploaded images, extract metadata, and support content moderation at large scale. These analysis tools also contribute to predictive modeling for applications like pricing, visual optimization, and image generation, making workflows more cost-effective and efficient. By integrating data-driven approaches, AI enhances automation and decision-making, offering new possibilities for intelligent visual interpretation.</p> <p>In this tutorial, you will learn to build an AI-driven system capable of answering real-time user queries from PowerPoint slides, using both text and images as context. This tutorial will guide you through:</p> <p>PowerPoint processing: Extract text and images from .pptx files for AI-based analysis.</p> <p>Text-based Q&amp;A: Use Granite Vision to generate answers based on extracted slide text.</p> <p>Image-based Q&amp;A: Ask AI to analyze images, charts, and diagrams from slides.</p> <p>Optimized question formulation: Learn how to craft effective questions for accurate and relevant AI responses.</p> <p>This tutorial leverages cutting-edge AI technologies, including:</p> <ol> <li>IBM Granite Vision: A powerful vision-language model (VLM) that processes both text and images.</li> <li>Python-PPTX: A library for extracting text and images from PowerPoint files.</li> <li>Transformers: A framework to process AI model inputs efficiently.</li> </ol> <p>By the end of this tutorial, you will:</p> <ol> <li>Extract and process PowerPoint content (text and images).</li> <li>Use Granite vision model for AI-driven Q&amp;A on slide content.</li> <li>Ask AI insightful questions about text and images.</li> <li>Improve user interaction with presentations using AI-powered explanations.</li> </ol> <p>This tutorial is designed for AI developers, researchers, content creators, and business professionals looking to enhance their presentations with AI-driven insights.</p> <p>You need an IBM Cloud account to create a watsonx.ai project.</p> <p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li>Log in to watsonx.ai using your IBM Cloud account.</li> <li>Create a watsonx.ai project. You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</li> <li>Create a Jupyter Notebook.</li> <li>Upload the PPTX file as asset in watsonx.ai</li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This tutorial is also available on Github.</p> <p>Note: This tutorial needs GPU infrastructure to run the code, so it is recommended to use watsonx.ai as illustrated in this tutorial.</p> <p>Before we begin extracting and processing PowerPoint content, we need to install the necessary Python libraries:</p> <p><code>transformers</code> \u2013 Provides access to IBM Granite Vision and other AI models.</p> <p><code>torch</code> \u2013 A deep learning framework required for running the model efficiently.</p> <p><code>python-pptx</code> \u2013 A library to extract text and images from PowerPoint (.pptx) files.</p> <p>Run the following commands to install and upgrade these packages:</p> In\u00a0[\u00a0]: Copied! <pre>!pip install --upgrade transformers\n!pip install --upgrade torch\n!pip install python-pptx\n!pip install botocore\n!pip install ibm-cos-sdk\n</pre> !pip install --upgrade transformers !pip install --upgrade torch !pip install python-pptx !pip install botocore !pip install ibm-cos-sdk <p>In this step, we import the necessary libraries for processing PowerPoint files, handling images, and interacting with the IBM Granite Vision model:</p> <ol> <li><p><code>os</code> and <code>io</code> \u2013 For file handling and input/output operations.</p> </li> <li><p><code>torch</code> \u2013 Ensures compatibility with the AI model.</p> </li> <li><p><code>pptx.Presentation</code> \u2013 Extracts text and images from PowerPoint (.pptx) files.</p> </li> <li><p><code>PIL.Image</code> \u2013 Processes images extracted from slides.</p> </li> <li><p><code>transformers</code> \u2013 Loads IBM Granite Vision for AI-based Q&amp;A.</p> </li> <li><p><code>botocore.client.Config</code> &amp; <code>ibm_boto3</code> \u2013 Handles cloud-based storage access (IBM Cloud Object Storage).</p> </li> </ol> In\u00a0[\u00a0]: Copied! <pre>import os\nimport io\nimport torch\nfrom pptx import Presentation\nfrom PIL import Image\nfrom io import BytesIO\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom botocore.client import Config\nimport ibm_boto3\n</pre> import os import io import torch from pptx import Presentation from PIL import Image from io import BytesIO from transformers import AutoProcessor, AutoModelForVision2Seq from botocore.client import Config import ibm_boto3 <p>In this step, we establish a connection to IBM cloud object storage (COS) to access and retrieve PowerPoint files stored in the cloud.</p> <p>You can levrage the python support, provided through a fork of the boto3 library with features to make the most of IBM Cloud\u00ae Object Storage. Check out the official documentation to get these credentials.</p> <p><code>ibm_boto3.client</code> \u2013 Creates a client to interact with IBM COS. <code>ibm_api_key_id</code> \u2013 Your IBM Cloud API key for authentication. <code>ibm_auth_endpoint</code> \u2013 The authentication endpoint for IBM Cloud. <code>endpoint_url</code> \u2013 The specific COS storage endpoint.</p> In\u00a0[\u00a0]: Copied! <pre># IBM COS credentials\ncos_client = ibm_boto3.client(\n    service_name='s3',\n    ibm_api_key_id='Enter your API Key',\n    ibm_auth_endpoint='[Enter your auth end-point url]',\n    config=Config(signature_version='oauth'),\n    endpoint_url='[Enter your end-point url]'\n)\n</pre> # IBM COS credentials cos_client = ibm_boto3.client(     service_name='s3',     ibm_api_key_id='Enter your API Key',     ibm_auth_endpoint='[Enter your auth end-point url]',     config=Config(signature_version='oauth'),     endpoint_url='[Enter your end-point url]' ) <p>Note: When you upload a file as assets in watsonx.ai, it is automatically stored in IBM Cloud Object Storage (COS). When you later import the file into a Jupyter Notebook, Watsonx.ai generates and inserts the necessary credentials (API key, authentication endpoint, and storage endpoint) into your notebook. The provided IBM COS credentials allow secure access to retrieve files from storage, enabling seamless integration between Watsonx.ai Assets and the notebook environment for further processing.</p> <p>By configuring this connection, we can seamlessly download and process PowerPoint presentations stored in IBM Cloud for AI-powered analysis</p> <p>In this step, we specify the IBM Cloud Object Storage bucket and file details to locate and retrieve the PowerPoint presentation (.pptx) for processing.</p> <p>Check out this official document to get the bucket configuration details through the IBM Cloud UI.</p> <p><code>bucket</code> \u2013 The name of the IBM COS storage bucket where the file is stored.</p> <p><code>object_key</code> \u2013 The exact filename of the PowerPoint presentation to be accessed.</p> In\u00a0[\u00a0]: Copied! <pre>bucket = 'Enter your bucket key'\nobject_key = 'Application Integration client presentation.PPTX [You can replace this with your PPT name]'\n</pre> bucket = 'Enter your bucket key' object_key = 'Application Integration client presentation.PPTX [You can replace this with your PPT name]' <p>These parameters ensure that the correct file is fetched for text and image extraction, enabling AI-driven Q&amp;A with IBM Granite Vision.</p> <p>In this step, we download the PowerPoint (.pptx) file from IBM Cloud Object Storage (COS) to process it locally.</p> <p><code>cos_client.get_object()</code> \u2013 Retrieves the file from the specified bucket and object key.</p> <p><code>streaming_body.read()</code> \u2013 Reads the file contents into a byte stream for further processing.</p> In\u00a0[\u00a0]: Copied! <pre># Download PPTX file from IBM COS\nstreaming_body = cos_client.get_object(Bucket=bucket, Key=object_key)['Body']\npptx_bytes = streaming_body.read()\n</pre> # Download PPTX file from IBM COS streaming_body = cos_client.get_object(Bucket=bucket, Key=object_key)['Body'] pptx_bytes = streaming_body.read() <p>In this step, we store the downloaded PowerPoint file (.pptx) locally so it can be processed.</p> <p><code>pptx_path</code> \u2013 Defines the local filename where the presentation will be saved.</p> <p><code>open(pptx_path, 'wb')</code> \u2013 Opens the file in write-binary mode to store the retrieved bytes.</p> <p><code>f.write(pptx_bytes)</code> \u2013 Writes the downloaded file content into the newly created .pptx file.</p> In\u00a0[\u00a0]: Copied! <pre># Save the bytes to a local PPTX file\npptx_path = \"downloaded_presentation.pptx\"\nwith open(pptx_path, 'wb') as f:\n    f.write(pptx_bytes)\n</pre> # Save the bytes to a local PPTX file pptx_path = \"downloaded_presentation.pptx\" with open(pptx_path, 'wb') as f:     f.write(pptx_bytes) <p>In this step, we print a confirmation message to ensure that the PowerPoint file has been successfully saved. <code>print</code> function displays the file path where the .pptx file is stored locally.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"PPTX file saved as: {pptx_path}\")\n</pre> print(f\"PPTX file saved as: {pptx_path}\") <p>In this step, we define a function to process the PowerPoint file (.pptx) and extract its content:</p> <p><code>slide_texts</code> \u2013 Stores extracted text from each slide.</p> <p><code>slide_images</code> \u2013 Stores extracted images as PIL image objects, along with their corresponding slide numbers.</p> <p>Iterates through slides to extract Text from shapes containing textual content and Images embedded within slides.</p> <p>This function separates the text and images from the PPT, allowing the chat agent to easily answer user questions based on the extracted content.</p> In\u00a0[\u00a0]: Copied! <pre>def extract_text_and_images_from_pptx(pptx_path):\n    presentation = Presentation(pptx_path)\n    slide_texts = []\n    slide_images = []\n\n    for slide_number, slide in enumerate(presentation.slides):\n        # Extract text from slide\n        slide_text = []\n        for shape in slide.shapes:\n            if hasattr(shape, \"text\"):\n                slide_text.append(shape.text)\n        slide_texts.append(\"\\n\".join(slide_text))\n\n        # Extract images from slide\n        for shape in slide.shapes:\n            if hasattr(shape, \"image\"):\n                image_stream = BytesIO(shape.image.blob)\n                image = Image.open(image_stream)\n                slide_images.append((slide_number, image))\n\n    return slide_texts, slide_images\n</pre> def extract_text_and_images_from_pptx(pptx_path):     presentation = Presentation(pptx_path)     slide_texts = []     slide_images = []      for slide_number, slide in enumerate(presentation.slides):         # Extract text from slide         slide_text = []         for shape in slide.shapes:             if hasattr(shape, \"text\"):                 slide_text.append(shape.text)         slide_texts.append(\"\\n\".join(slide_text))          # Extract images from slide         for shape in slide.shapes:             if hasattr(shape, \"image\"):                 image_stream = BytesIO(shape.image.blob)                 image = Image.open(image_stream)                 slide_images.append((slide_number, image))      return slide_texts, slide_images <p>In this step, we call the function to extract text and images from the saved PowerPoint file.</p> <p><code>pptx_path</code> \u2013 Specifies the local path of the downloaded PowerPoint file.</p> <p><code>extract_text_and_images_from_pptx(pptx_path)</code> \u2013 Extracts text and images from the slides.</p> <p><code>slide_texts</code> \u2013 Stores the extracted text from all slides.</p> <p><code>slide_images</code> \u2013 Stores the extracted images along with their respective slide numbers.</p> In\u00a0[\u00a0]: Copied! <pre>pptx_path = \"downloaded_presentation.pptx\"\nslide_texts, slide_images = extract_text_and_images_from_pptx(pptx_path)\n</pre> pptx_path = \"downloaded_presentation.pptx\" slide_texts, slide_images = extract_text_and_images_from_pptx(pptx_path) <p>In this step, we print the extracted text from each slide to verify that the PowerPoint content has been processed correctly.</p> <p><code>enumerate(slide_texts)</code> \u2013 Iterates through the extracted text, associating each with its respective slide number.</p> <p><code>Separator ('-' * 40)</code> \u2013 Helps visually distinguish content from different slides.</p> In\u00a0[\u00a0]: Copied! <pre># Display extracted text and images\nfor i, text in enumerate(slide_texts):\n    print(f\"Slide {i + 1} Text:\\n{text}\\n{'-'*40}\")\n</pre> # Display extracted text and images for i, text in enumerate(slide_texts):     print(f\"Slide {i + 1} Text:\\n{text}\\n{'-'*40}\") <p>In this step, we confirm and visualize the extracted images from the PowerPoint slides.</p> <p><code>len</code> \u2013 Counts the total number of images extracted.</p> <p><code>img.show()</code> \u2013 Opens each extracted image for review.</p> <p>You can replace <code>.show()</code> with <code>.save('filename.png')</code> to store the images locally.</p> In\u00a0[\u00a0]: Copied! <pre>print(f\"\\nExtracted {len(slide_images)} images.\")\nfor slide_num, img in slide_images:\n    img.show()  # This will open the image, or you can save it using img.save('filename.png')\n</pre> print(f\"\\nExtracted {len(slide_images)} images.\") for slide_num, img in slide_images:     img.show()  # This will open the image, or you can save it using img.save('filename.png') <p>In this step, we initialize the IBM Granite-Vision-3.1-2B-Preview model for AI-powered text and image processing.</p> <p><code>MODEL_NAME</code> specifies the pre-trained Granite Vision model to be used and <code>torch.cuda.is_available()</code> checks if a GPU (CUDA) is available for faster processing; otherwise, it defaults to the CPU.</p> In\u00a0[\u00a0]: Copied! <pre># Load IBM Granite-Vision-3.1-2B-Preview model and processor\nMODEL_NAME = \"ibm-granite/granite-vision-3.1-2b-preview\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n</pre> # Load IBM Granite-Vision-3.1-2B-Preview model and processor MODEL_NAME = \"ibm-granite/granite-vision-3.1-2b-preview\" device = \"cuda\" if torch.cuda.is_available() else \"cpu\" <p>In this step, we load the IBM Granite Vision model and its corresponding processor to handle both text and image inputs.</p> <p><code>AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)</code> \u2013 Loads the pre-trained processor to format inputs (text and images) for the model.</p> <p><code>AutoModelForVision2Seq.from_pretrained(MODEL_NAME, trust_remote_code=True, ignore_mismatched_sizes=True).to(device)</code> \u2013 Loads the Granite Vision model and transfers it to the available device (GPU or CPU).</p> <p>where,</p> <p><code>trust_remote_code=True</code> \u2013 Ensures compatibility with custom model implementations.</p> <p><code>ignore_mismatched_sizes=True</code> \u2013 Prevents errors if there are minor inconsistencies in model size.</p> <p>Note: This may take a while to load.</p> In\u00a0[\u00a0]: Copied! <pre>processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\nmodel = AutoModelForVision2Seq.from_pretrained(MODEL_NAME, trust_remote_code=True, ignore_mismatched_sizes=True).to(device)\n</pre> processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True) model = AutoModelForVision2Seq.from_pretrained(MODEL_NAME, trust_remote_code=True, ignore_mismatched_sizes=True).to(device) <p>In this step, we create a chat function that allows users to ask questions based on the extracted text from the PowerPoint slides.</p> <p>How It Works:</p> <ol> <li><p>User inputs a question related to the slide content.</p> </li> <li><p>The entire extracted text from the PPT is formatted into a structured conversation for the model. This will give the model accurate contaxt to generate the specific answer from the PPT content itself.</p> </li> <li><p><code>apply_chat_template()</code> prepares the input for the AI model in a conversational format.</p> </li> <li><p><code>model.generate()</code> generates a response based on the input query.</p> </li> <li><p><code>processor.decode()</code> decodes the AI-generated response into human-readable text.</p> </li> <li><p>Loop continues until the user types <code>exit</code> to quit the chat.</p> </li> </ol> In\u00a0[\u00a0]: Copied! <pre># Chat based on Text Only\ndef chat_with_text(model, processor, slide_texts):\n    while True:\n        query = input(\"Ask a question based on the presentation text (or type 'exit' to quit): \")\n        if query.lower() == 'exit':\n            break\n        conversation = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"\\n\".join(slide_texts) + f\"\\nQuery: {query}\"},\n                ],\n            },\n        ]\n        inputs = processor.apply_chat_template(\n            conversation,\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True,\n            return_tensors=\"pt\"\n        ).to(device)\n        \n        outputs = model.generate(**inputs, max_new_tokens=150)\n        response = processor.decode(outputs[0], skip_special_tokens=True)\n        print(\"Model Response:\", response)\n</pre> # Chat based on Text Only def chat_with_text(model, processor, slide_texts):     while True:         query = input(\"Ask a question based on the presentation text (or type 'exit' to quit): \")         if query.lower() == 'exit':             break         conversation = [             {                 \"role\": \"user\",                 \"content\": [                     {\"type\": \"text\", \"text\": \"\\n\".join(slide_texts) + f\"\\nQuery: {query}\"},                 ],             },         ]         inputs = processor.apply_chat_template(             conversation,             add_generation_prompt=True,             tokenize=True,             return_dict=True,             return_tensors=\"pt\"         ).to(device)                  outputs = model.generate(**inputs, max_new_tokens=150)         response = processor.decode(outputs[0], skip_special_tokens=True)         print(\"Model Response:\", response) <p>In this step, we create a chat function that allows users to ask questions about individual images extracted from the PowerPoint slides.</p> <p>How It Works:</p> <ol> <li><p>The user inputs a question related to slide images.</p> </li> <li><p>They specify a slide number to reference a particular image.</p> </li> <li><p>The selected image is saved temporarily as <code>slide_image_temp.png</code>.</p> </li> <li><p>A structured conversation is created, including:</p> <ul> <li>The image file path.</li> <li>The user's question.</li> </ul> </li> <li><p><code>apply_chat_template()</code> processes the input in a format suitable for the AI model.</p> </li> <li><p><code>model.generate()</code> generates a response based on the image and query.</p> </li> <li><p>The response is decoded and printed for the user.</p> </li> <li><p>The loop continues until the user types <code>exit</code> to quit.</p> </li> </ol> In\u00a0[\u00a0]: Copied! <pre># Chat based on Images Only\ndef chat_with_images(model, processor, slide_images):\n    while True:\n        query = input(\"Ask a question based on the presentation images (or type 'exit' to quit): \")\n        if query.lower() == 'exit':\n            break\n        slide_num = int(input(f\"Enter slide number (1 to {len(slide_images)}) to ask about its image: \")) - 1\n        image = slide_images[slide_num][1]\n        \n        img_path = \"slide_image_temp.png\"\n        image.save(img_path)  # Save the image temporarily\n        \n        conversation = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"url\": img_path},\n                    {\"type\": \"text\", \"text\": query},\n                ],\n            },\n        ]\n        \n        inputs = processor.apply_chat_template(\n            conversation,\n            add_generation_prompt=True,\n            tokenize=True,\n            return_dict=True,\n            return_tensors=\"pt\"\n        ).to(device)\n        \n        outputs = model.generate(**inputs, max_new_tokens=150)\n        response = processor.decode(outputs[0], skip_special_tokens=True)\n        print(\"Model Response:\", response)\n</pre> # Chat based on Images Only def chat_with_images(model, processor, slide_images):     while True:         query = input(\"Ask a question based on the presentation images (or type 'exit' to quit): \")         if query.lower() == 'exit':             break         slide_num = int(input(f\"Enter slide number (1 to {len(slide_images)}) to ask about its image: \")) - 1         image = slide_images[slide_num][1]                  img_path = \"slide_image_temp.png\"         image.save(img_path)  # Save the image temporarily                  conversation = [             {                 \"role\": \"user\",                 \"content\": [                     {\"type\": \"image\", \"url\": img_path},                     {\"type\": \"text\", \"text\": query},                 ],             },         ]                  inputs = processor.apply_chat_template(             conversation,             add_generation_prompt=True,             tokenize=True,             return_dict=True,             return_tensors=\"pt\"         ).to(device)                  outputs = model.generate(**inputs, max_new_tokens=150)         response = processor.decode(outputs[0], skip_special_tokens=True)         print(\"Model Response:\", response) <p>In this step, we call the chat_with_text function, allowing the user to ask questions about the extracted text from the PowerPoint slides.</p> <p>How It Works:</p> <ol> <li><p><code>chat_with_text()</code> starts the text-based Q&amp;A session.</p> </li> <li><p>The function continuously prompts the user for input, answering questions based on the extracted slide text.</p> </li> <li><p>The chat loop continues until the user types <code>exit</code> to quit.</p> </li> </ol> In\u00a0[\u00a0]: Copied! <pre>chat_with_text(model, processor, slide_texts)\n</pre> chat_with_text(model, processor, slide_texts) <p>OUTPUT</p> <p>Query: Is integration a competitive advantage for your organization?</p> <p>&lt;|assistant|&gt;</p> <p>Yes, integration is a competitive advantage for your organization. It helps you move faster and overcome challenges, and can lead to increased costs, inefficiencies, security risks, and a poor user experience, ultimately jeopardizing an organization's competitiveness and ability to thrive in a rapidly evolving business landscape.</p> <p>Ask a question based on the presentation text (or type 'exit' to quit):  exit</p> <p>When the user asked, \"Is integration a competitive advantage for your organization?\", the Granite Vision model processed the query using the extracted PowerPoint slide text and generated a response.</p> <p>The model recognized \"integration\" as a business concept and provided a structured answer from <code>slide number 7</code> explaining both its benefits and risks. It highlighted that integration enhances speed and problem-solving, but also noted potential downsides like increased costs, inefficiencies, security risks, and poor user experience if not managed effectively.</p> <p>This response demonstrates the model's ability to interpret extracted slide text and generate a contextually relevant and well-balanced answer</p> <p>In this step, we call the chat_with_images function, enabling the user to ask questions about images extracted from the PowerPoint slides.</p> <p>How It Works:</p> <ol> <li><p><code>chat_with_images()</code> starts the image-based Q&amp;A session.</p> </li> <li><p>The function prompts the user to select a specific slide number containing an image.</p> </li> <li><p>The selected image is processed and formatted into a structured conversation for the vision model.</p> </li> <li><p>The model generates a response based on the image content and the user\u2019s query.</p> </li> <li><p>The loop continues until the user types <code>exit</code> to quit.</p> </li> </ol> In\u00a0[\u00a0]: Copied! <pre>chat_with_images(model, processor, slide_images)\n</pre> chat_with_images(model, processor, slide_images) <p>OUTPUT</p> <p>Ask a question based on the presentation images (or type 'exit' to quit):  what is this image?</p> <p>Enter slide number (1 to 41) to ask about its image:  2</p> <p>Model Response: &lt;|system|&gt;</p> <p>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</p> <p>&lt;|user|&gt;</p> <p>what is this image?</p> <p>&lt;|assistant|&gt;</p> <p>3d model</p> <p>Ask a question based on the presentation images (or type 'exit' to quit):  explain this image</p> <p>Enter slide number (1 to 41) to ask about its image:  2</p> <p>Model Response: &lt;|system|&gt;</p> <p>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</p> <p>&lt;|user|&gt;</p> <p>explain this image</p> <p>&lt;|assistant|&gt;</p> <p>the image is a 3d model of a cube</p> <p>Ask a question based on the presentation images (or type 'exit' to quit):  can you explain this chart?</p> <p>Enter slide number (1 to 41) to ask about its image:  1</p> <p>Model Response: &lt;|system|&gt;</p> <p>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</p> <p>&lt;|user|&gt;</p> <p>can you explain this chart?</p> <p>&lt;|assistant|&gt;</p> <p>Here a is a bar plot titled Maturity progression of the enterprise cloud market from 1st to 4th generation. The x-axis measures Generations Progression while the y-axis plots Maturity progression over the years. The chart shows that as the generations progress, the maturity of the enterprise cloud market increases.</p> <p>Ask a question based on the presentation images (or type 'exit' to quit):  exit</p> <p>When the user asked image-related questions, the Granite Vision model processed the selected images and generated responses based on its understanding of visual content.</p> <p>For the question \"What is this image?\" (Slide 2) - The model identified the image as a \"3D model\" but provided a minimal description.</p> <p>For \"Explain this image\" (Slide 2) - The model refined its response, identifying it as \"a 3D model of a cube.\"</p> <p>For \"Can you explain this chart?\" (Slide 1) - The model provided a detailed description of the bar chart, explaining its title, x-axis, y-axis, and overall trend, indicating how enterprise cloud maturity progresses across generations.</p> <p>This step allows users to interact with visual elements, such as charts, diagrams, and infographics, by leveraging IBM Granite Vision model for intelligent analysis and explanations</p>"},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#build-a-ppt-ai-image-analysis-question-answering-system-with-granite-vision-model","title":"Build a PPT AI image analysis question answering system with Granite vision model\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#use-cases","title":"Use cases\u00b6","text":"<p>With the rapid advancements in Computer Vision and advanced AI, businesses and researchers are leveraging image-based technologies for a wide range of applications. From image classification and OCR (Optical Character Recognition) to segmentation and video analysis, AI-powered tools are transforming the way we extract and analyze visual information.</p> <p>In industries like social media, AI enhances content moderation by analyzing images at the pixel level, ensuring compliance and improving engagement. Businesses can also utilize Vision API for automated document processing, converting scanned files, excels, and reports into structured data. These applications streamline workflows, improve efficiency, and enable organizations to extract meaningful insights from large-scale visual datasets.</p> <p>These use cases highlight the growing role of AI-powered image analysis across industries. In this tutorial, we focus on applying these capabilities to PowerPoint presentations, enabling interactive Q&amp;A on text and images using advanced computer vision and AI models</p>"},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#ai-powered-interactive-qa-for-presentations","title":"AI-powered interactive Q&amp;A for presentations\u00b6","text":"<p>Large language models (LLMs) have revolutionized machine learning by enabling intelligent insights from vast datasets of unstructured text. However, traditional LLMs often struggle with image analysis, making it challenging to extract insights from charts, diagrams, and visual elements in presentations.</p> <p>IBM\u00ae Granite\u2122 open source model - Vision-3.1-2B-Preview model bridges this gap by integrating AI tools with advanced object detection algorithms, allowing users to automate multimodal analysis. This tutorial demonstrates how to streamline your workflow by using AI to extract and analyze text and images from PowerPoint (.pptx) files, enabling interactive Q&amp;A for enhanced presentation insights.</p>"},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#technologies-used","title":"Technologies used\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#what-you-will-achieve","title":"What you will achieve\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#prerequisites","title":"Prerequisites\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-2-install-required-dependencies","title":"Step 2: Install required dependencies\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-3-import-required-libraries","title":"Step 3: Import Required Libraries\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-4-connect-to-ibm-cloud-object-storage","title":"Step 4: Connect to IBM cloud object storage\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-5-define-storage-parameters","title":"Step 5: Define storage parameters\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-6-retrieve-the-powerpoint-file-from-ibm-cloud-object-storage","title":"Step 6: Retrieve the PowerPoint file from IBM cloud object storage\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-7-save-the-powerpoint-file-on-local-path","title":"Step 7: Save the PowerPoint file on local path\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-8-confirm-file-save-location","title":"Step 8: Confirm file save location\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-9-extract-text-and-images-from-the-powerpoint-file","title":"Step 9: Extract text and images from the PowerPoint file\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-10-process-the-powerpoint-file","title":"Step 10: Process the PowerPoint file\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-11-display-extracted-text-from-slides","title":"Step 11: Display extracted text from slides\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-12-display-extracted-images-from-slides","title":"Step 12: Display extracted images from slides\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-13-load-the-ibm-granite-vision-model","title":"Step 13: Load the IBM Granite vision model\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-14-initialize-the-model-and-processor","title":"Step 14: Initialize the model and processor\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-15-implement-text-based-ai-chat","title":"Step 15: Implement text-based AI Chat\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-16-implement-image-based-ai-chat","title":"Step 16: Implement image-based AI Chat\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-17-run-the-text-based-ai-chat","title":"Step 17: Run the Text-Based AI Chat\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#step-18-run-the-image-based-ai-chat","title":"Step 18: Run the Image-Based AI Chat\u00b6","text":""},{"location":"tutorials/generative-ai/ppt-ai-analyzer-granite-vision/#key-takeaways","title":"Key takeaways:\u00b6","text":"<ol> <li>The model recognizes basic shapes and objects but may provide generalized descriptions for some images.</li> <li>For charts and diagrams, it provides structured insights, including titles, axis labels, and trends, making it useful for business and data presentations.</li> <li>The accuracy of responses depends on image clarity and complexity\u2014simpler visuals (like 3D models) may receive shorter responses, while structured visuals (like charts) get more detailed insights.</li> </ol> <p>This demonstrates IBM Granite Vision\u2019s ability to interpret images.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/","title":"Implement RAG chunking strategies with LangChain and watsonx.ai","text":"In\u00a0[\u00a0]: Copied! <pre># installations\n!pip install -q langchain langchain-ibm langchain_experimental langchain-text-splitters langchain_chroma transformers bs4 langchain_huggingface sentence-transformers\n</pre> # installations !pip install -q langchain langchain-ibm langchain_experimental langchain-text-splitters langchain_chroma transformers bs4 langchain_huggingface sentence-transformers In\u00a0[\u00a0]: Copied! <pre># imports \nimport getpass\n\nfrom langchain_ibm import WatsonxLLM\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\nfrom transformers import AutoTokenizer\n</pre> # imports  import getpass  from langchain_ibm import WatsonxLLM from langchain_chroma import Chroma from langchain_community.document_loaders import WebBaseLoader from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams from transformers import AutoTokenizer <p>To set our credentials, we need the <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> you generated in step 1. We will also set the URL serving as the API endpoint.</p> In\u00a0[\u00a0]: Copied! <pre>WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://us-south.ml.cloud.ibm.com\"\n</pre> WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://us-south.ml.cloud.ibm.com\" In\u00a0[\u00a0]: Copied! <pre>llm = WatsonxLLM(\n    model_id= \"ibm/granite-3-8b-instruct\", \n    url=URL,\n    apikey=WATSONX_APIKEY,\n    project_id=WATSONX_PROJECT_ID,\n    params={\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.TEMPERATURE: 0,\n        GenParams.MIN_NEW_TOKENS: 5,\n        GenParams.MAX_NEW_TOKENS: 2000,\n        GenParams.REPETITION_PENALTY:1.2,\n        GenParams.STOP_SEQUENCES: [\"\\n\\n\"]\n    }\n)\n</pre> llm = WatsonxLLM(     model_id= \"ibm/granite-3-8b-instruct\",      url=URL,     apikey=WATSONX_APIKEY,     project_id=WATSONX_PROJECT_ID,     params={         GenParams.DECODING_METHOD: \"greedy\",         GenParams.TEMPERATURE: 0,         GenParams.MIN_NEW_TOKENS: 5,         GenParams.MAX_NEW_TOKENS: 2000,         GenParams.REPETITION_PENALTY:1.2,         GenParams.STOP_SEQUENCES: [\"\\n\\n\"]     } ) In\u00a0[\u00a0]: Copied! <pre>url = \"https://www.ibm.com/new/announcements/ibm-granite-3-1-powerful-performance-long-context-and-more\"\ndoc = WebBaseLoader(url).load()\n</pre> url = \"https://www.ibm.com/new/announcements/ibm-granite-3-1-powerful-performance-long-context-and-more\" doc = WebBaseLoader(url).load() In\u00a0[\u00a0]: Copied! <pre>from langchain_text_splitters import CharacterTextSplitter\n\ntokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.1-8b-instruct\")\ntext_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n    tokenizer, \n    separator=\"\\n\", #default: \"\\n\\n\"\n    chunk_size=1200, \n    chunk_overlap=200)\n\nfixed_size_chunks = text_splitter.create_documents([doc[0].page_content])\n</pre> from langchain_text_splitters import CharacterTextSplitter  tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.1-8b-instruct\") text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(     tokenizer,      separator=\"\\n\", #default: \"\\n\\n\"     chunk_size=1200,      chunk_overlap=200)  fixed_size_chunks = text_splitter.create_documents([doc[0].page_content]) <p>We can print one of the chunks for a better understanding of their structure.</p> In\u00a0[\u00a0]: Copied! <pre>fixed_size_chunks[1]\n</pre> fixed_size_chunks[1] Out[\u00a0]: <pre>Document(metadata={}, page_content='As always, IBM\u2019s historical commitment to open source is reflected in the permissive and standard open source licensing for every offering discussed in this article.\\n\\r\\n        Granite 3.1 8B Instruct: raising the bar for lightweight enterprise models\\r\\n    \\nIBM\u2019s efforts in the ongoing optimization the Granite series are most evident in the growth of its flagship 8B dense model. IBM Granite 3.1 8B Instruct now bests most open models in its weight class in average scores on the academic benchmarks evaluations included in the Hugging Face OpenLLM Leaderboard.\\nThe evolution of the Granite model series has continued to prioritize excellence and efficiency in enterprise use cases, including agentic AI. This progress is most apparent in the newest 8B model\u2019s significantly improved performance on IFEval, a dataset featuring tasks that test a model\u2019s ability to follow detailed instructions, and Multi-step Soft Reasoning (MuSR), whose tasks measure reasoning and understanding on and of long texts.\\n\\r\\n        Expanded context length\\r\\n    \\nBolstering the performance leap from Granite 3.0 to Granite 3.1 is the expansion of all models\u2019 context windows. Granite 3.1\u2019s 128K token context length is on par with that of other leading open model series, including Llama 3.1\u20133.3 and Qwen2.5.\\nThe context window (or context length) of a large language model (LLM) is the amount of text, in tokens, that an LLM can consider at any one time. A larger context window enables a model to process larger inputs, carry out longer continuous exchanges and incorporate more information into each output. Tokenization doesn\u2019t entail any fixed token-to-word \u201cexchange rate,\u201d but 1.5 tokens per word is a useful estimate. 128K tokens is roughly equivalent to a 300-page book.\\nAbove a threshold of about 100K tokens, impressive new possibilities emerge, including multi-document question answering, repository-level code understanding, self-reflection and LLM-powered autonomous agents.1 Granite 3.1\u2019s expanded context length thus lends itself to a much wider range of enterprise use cases, from processing code bases and lengthy legal documents in their entirety to simultaneously reviewing thousands of financial transactions.\\n\\r\\n        Granite Guardian 3.1: detecting hallucinations in agentic workflows\\nGranite Guardian 3.1 8B and Granite Guardian 3.1 2B can now detect hallucinations that might occur in an agentic workflow, affording the same accountability and trust to function calling that we already provide for RAG.\\nMany steps and subprocesses occur in the space between the initial request sent to an AI agent and the output the agent eventually returns to the user. To provide oversight throughout, Granite Guardian 3.1 models monitor every function call for syntactic and semantic hallucinations.\\nFor instance, if an AI agent purportedly queries an external information source, Granite Guardian 3.1 monitors for fabricated information flows. If an agentic workflow entails intermediate calculations using figures retrieved from a bank record, Granite Guardian 3.1 checks to see whether the agent pulled the correct function call along with the appropriate numbers.\\nToday\u2019s release is yet another step toward accountability and trust for any component of an LLM-based enterprise workflow. The new Granite Guardian 3.1 models are available on Hugging Face. They\u2019ll also be available through Ollama later this month and on IBM watsonx.ai in January 2025.\\n\\r\\n        Granite embedding models\\r\\n    \\nEmbeddings are an integral part of the LLM ecosystem. An accurate and efficient means of representing words, queries and documents in numerical form is essential to an array of enterprise tasks including semantic search, vector search and RAG, as well as maintaining effective vector databases. An effective embedding model can significantly enhance a system\u2019s understanding of user intent and increase the relevance of information and sources in response to a query.\\nWhile the past two years have seen the proliferation of increasingly competitive open source autoregressive LLMs for tasks like text generation and summarization, open source embedding model releases from major providers are relatively few and far between.\\nThe new Granite Embedding models are an enhanced evolution of the Slate family of encoder-only, RoBERTA-based language models. Trained with the same care and consideration for filtering bias, hate, abuse and profanity (\u201cHAP\u201d) as the rest of the Granite series, Granite Embedding is offered in four model sizes, two of which support multilingual embedding across 12 natural languages:\\nGranite-Embedding-30M-EnglishGranite-Embedding-125M-EnglishGranite-Embedding-107M-MultilingualGranite-Embedding-278M-Multilingual')</pre> <p>We can also use the tokenizer for verifying our process and to check the number of tokens present in each chunk. This step is optional and for demonstrative purposes.</p> In\u00a0[\u00a0]: Copied! <pre>for idx, val in enumerate(fixed_size_chunks):\n    token_count = len(tokenizer.encode(val.page_content))\n    print(f\"The chunk at index {idx} contains {token_count} tokens.\")\n</pre> for idx, val in enumerate(fixed_size_chunks):     token_count = len(tokenizer.encode(val.page_content))     print(f\"The chunk at index {idx} contains {token_count} tokens.\") <pre>The chunk at index 0 contains 1106 tokens.\nThe chunk at index 1 contains 1102 tokens.\nThe chunk at index 2 contains 1183 tokens.\nThe chunk at index 3 contains 1010 tokens.\n</pre> <p>Great! It looks like our chunk sizes were appropriately implemented.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_text_splitters import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\nrecursive_chunks = text_splitter.create_documents([doc[0].page_content])\nrecursive_chunks[:5]\n</pre> from langchain_text_splitters import RecursiveCharacterTextSplitter  text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0) recursive_chunks = text_splitter.create_documents([doc[0].page_content]) recursive_chunks[:5] Out[\u00a0]: <pre>[Document(metadata={}, page_content='IBM Granite 3.1: powerful performance, longer context and more'),\n Document(metadata={}, page_content='IBM Granite 3.1: powerful performance, longer context, new embedding models and more'),\n Document(metadata={}, page_content='Artificial Intelligence'),\n Document(metadata={}, page_content='Compute and servers'),\n Document(metadata={}, page_content='IT automation')]</pre> <p>The splitter successfully chunked the text by using the default separators: <code>[\"\\n\\n\", \"\\n\", \" \", \"\"]</code>.</p> In\u00a0[\u00a0]: Copied! <pre>from langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_experimental.text_splitter import SemanticChunker\n\nembeddings_model = HuggingFaceEmbeddings(model_name=\"ibm-granite/granite-embedding-30m-english\")\ntext_splitter = SemanticChunker(embeddings_model)\nsemantic_chunks = text_splitter.create_documents([doc[0].page_content])\nsemantic_chunks[1]\n</pre> from langchain_huggingface import HuggingFaceEmbeddings from langchain_experimental.text_splitter import SemanticChunker  embeddings_model = HuggingFaceEmbeddings(model_name=\"ibm-granite/granite-embedding-30m-english\") text_splitter = SemanticChunker(embeddings_model) semantic_chunks = text_splitter.create_documents([doc[0].page_content]) semantic_chunks[1] Out[\u00a0]: <pre>Document(metadata={}, page_content='Our latest dense models (Granite 3.1 8B, Granite 3.1 2B), MoE models (Granite 3.1 3B-A800M, Granite 3.1 1B-A400M) and guardrail models (Granite Guardian 3.1 8B, Granite Guardian 3.1 2B) all feature a 128K token context length.We\u2019re releasing a family of all-new embedding models. The new retrieval-optimized Granite Embedding models are offered in four sizes, ranging from 30M\u2013278M parameters. Like their generative counterparts, they offer multilingual support across 12 different languages: English, German, Spanish, French, Japanese, Portuguese, Arabic, Czech, Italian, Korean, Dutch and Chinese. Granite Guardian 3.1 8B and 2B feature a new function calling hallucination detection capability, allowing increased control over and observability for agents making tool calls.All Granite 3.1, Granite Guardian 3.1, and Granite Embedding models are open source under Apache 2.0 license.These latest entries in the Granite series follow IBM\u2019s recent launch of Docling (an open source framework for prepping documents for RAG and other generative AI applications) and Bee (an open source, model agnostic framework for agentic AI).Granite TTM (TinyTimeMixers), IBM\u2019s series of compact but highly performant timeseries models, are now available in watsonx.ai through the beta release of watsonx.ai Timeseries Forecasting API and SDK.Granite 3.1 models are now available in IBM watsonx.ai, as well as through platform partners including (in alphabetical order) Docker, Hugging Face, LM Studio, Ollama and Replicate.Granite 3.1 will also be leveraged internally by enterprise partners: Samsung is integrating select Granite models into its SDS platform; Lockheed Martin is integrating Granite 3.1 models into its AI Factory tools, used by over 10,000 developers and engineers. Today marks the release of IBM Granite 3.1, the latest update to our Granite series of open, performant, enterprise-optimized language models. This suite of improvements, additions and new capabilities focuses primarily on augmenting performance, accuracy and accountability in essential enterprise use cases like tool use, retrieval augmented generation (RAG) and scalable agentic AI workflows. Granite 3.1 builds upon the momentum of the recently launched Granite 3.0 collection. IBM will continue to release updated models and functionality for the Granite 3 series in the coming months, with new multimodal capabilities slated for release in Q1 2025. These new Granite models are not the only notable recent IBM contributions to the open source LLM ecosystem. Today\u2019s release caps off a recent run of innovative open source launches, from a flexible framework for developing AI agents to an intuitive toolkit to unlock essential information stashed away in PDFs, slide decks and other file formats that are difficult for models to digest. Using these tools and frameworks in tandem with Granite 3.1 models offers developers evolved capabilities for RAG, AI agents and other LLM-based workflows. As always, IBM\u2019s historical commitment to open source is reflected in the permissive and standard open source licensing for every offering discussed in this article. Granite 3.1 8B Instruct: raising the bar for lightweight enterprise models\\r\\n    \\n\\n\\n\\nIBM\u2019s efforts in the ongoing optimization the Granite series are most evident in the growth of its flagship 8B dense model. IBM Granite 3.1 8B Instruct now bests most open models in its weight class in average scores on the academic benchmarks evaluations included in the Hugging Face OpenLLM Leaderboard. The evolution of the Granite model series has continued to prioritize excellence and efficiency in enterprise use cases, including agentic AI. This progress is most apparent in the newest 8B model\u2019s significantly improved performance on IFEval, a dataset featuring tasks that test a model\u2019s ability to follow detailed instructions, and Multi-step Soft Reasoning (MuSR), whose tasks measure reasoning and understanding on and of long texts. Expanded context length\\r\\n    \\n\\n\\n\\nBolstering the performance leap from Granite 3.0 to Granite 3.1 is the expansion of all models\u2019 context windows.')</pre> In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/ibm-granite/granite-3.1-language-models/refs/heads/main/README.md\"\nmarkdown_doc = WebBaseLoader(url).load()\nmarkdown_doc\n</pre> url = \"https://raw.githubusercontent.com/ibm-granite/granite-3.1-language-models/refs/heads/main/README.md\" markdown_doc = WebBaseLoader(url).load() markdown_doc Out[\u00a0]: <pre>[Document(metadata={'source': 'https://raw.githubusercontent.com/ibm-granite/granite-3.1-language-models/refs/heads/main/README.md'}, page_content='\\n\\n\\n\\n  :books: Paper (comming soon)\\xa0 | :hugs: HuggingFace Collection\\xa0 | \\n  :speech_balloon: Discussions Page\\xa0 | \u00f0\u0178\u201c\u02dc IBM Granite Docs\\n\\n\\n---\\n## Introduction to Granite 3.1 Language Models\\nGranite 3.1 language models are lightweight, state-of-the-art, open foundation models that natively support multilinguality, coding, reasoning, and tool usage, including the potential to be run on constrained compute resources. All the models are publicly released under an Apache 2.0 license for both research and commercial use. The models\\' data curation and training procedure were designed for enterprise usage and customization, with a process that evaluates datasets for governance, risk and compliance (GRC) criteria, in addition to IBM\\'s standard data clearance process and document quality checks.\\n\\nGranite 3.1 language models extend the context length of Granite 3.0 language models from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the models successfully adapt to the desired length of 128K. This long-context pre-training stage was performed using approximately 500B tokens. Moreover, Granite 3.1 instruction models provide an improved developer experience for function-calling and RAG generation tasks.\\n\\nGranite 3.1 models come in 4 varying sizes and 2 architectures:\\n- Dense Models: 2B and 8B parameter models, trained on 12 trillion tokens in total.\\n- Mixture-of-Expert (MoE) Models: Sparse 1B and 3B MoE models, with 400M and 800M activated parameters respectively, trained on 10 trillion tokens in total.\\n\\nAccordingly, these options provide a range of models with different compute requirements to choose from, with appropriate trade-offs with their performance on downstream tasks. At each scale, we release base model \u00e2\u20ac\u201d checkpoints of models after pretraining, as well as instruct checkpoints \u00e2\u20ac\u201d models finetuned for dialogue, instruction-following, helpfulness, and safety.\\n\\nEvaluation results show that Granite-3.1-8B-Instruct outperforms models of similar parameter sizes in [Hugging Face\\'s OpenLLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/) (see Figure 1). \\n\\n\\n\\n\\n  Figure 1. Evaluation results from Granite-3.1-8B-Instruct in Hugging Face\\'s OpenLLM Leaderboard.\\n\\n\\nComprehensive evaluation results for all model variants, as well as other relevant information will be available in Granite 3.1 Language Models technical report.\\n\\n## How to Use our Models?\\nTo use any of our models, pick an appropriate `model_path` from:\\n1. `ibm-granite/granite-3.1-2b-base`\\n2. `ibm-granite/granite-3.1-2b-instruct`\\n3. `ibm-granite/granite-3.1-8b-base`\\n4. `ibm-granite/granite-3.1-8b-instruct`\\n5. `ibm-granite/granite-3.1-1b-a400m-base`\\n6. `ibm-granite/granite-3.1-1b-a400m-instruct`\\n7. `ibm-granite/granite-3.1-3b-a800m-base`\\n8. `ibm-granite/granite-3.1-3b-a800m-instruct`\\n\\n### Inference\\nThis is a simple example of how to use Granite-3.1-1B-A400M-Instruct model.\\n\\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\ndevice = \"auto\"\\nmodel_path = \"ibm-granite/granite-3.1-1b-a400m-instruct\"\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\n# drop device_map if running on CPU\\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\\nmodel.eval()\\n# change input text as desired\\nchat = [\\n    { \"role\": \"user\", \"content\": \"Please list one IBM Research laboratory located in the United States. You should only output its name and location.\" },\\n]\\nchat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\\n# tokenize the text\\ninput_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\\n# generate output tokens\\noutput = model.generate(**input_tokens, \\n                        max_new_tokens=100)\\n# decode output tokens into text\\noutput = tokenizer.batch_decode(output)\\n# print output\\nprint(output)\\n```\\n## How to Download our Models?\\nThe model of choice (granite-3.1-1b-a400m-instruct in this example) can be cloned using:\\n```shell\\ngit clone https://huggingface.co/ibm-granite/granite-3.1-1b-a400m-instruct\\n```\\n\\n## How to Contribute to this Project?\\nPlese check our [Guidelines](/CONTRIBUTING.md) and [Code of Conduct](/CODE_OF_CONDUCT.md) to contribute to our project.\\n\\n## Model Cards\\nThe model cards for each model variant are available in their respective HuggingFace repository. Please visit our collection [here](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d).\\n\\n## License \\nAll Granite 3.0 Language Models are distributed under [Apache 2.0](./LICENSE) license.\\n\\n## Would you like to provide feedback?\\nPlease let us know your comments about our family of language models by visiting our [collection](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d). Select the repository of the model you would like to provide feedback about. Then, go to *Community* tab, and click on *New discussion*. Alternatively, you can also post any questions/comments on our [github discussions page](https://github.com/orgs/ibm-granite/discussions).\\n\\n')]</pre> <p>Now, we can use LangChain's <code>MarkdownHeaderTextSplitter</code> to split the file by header type, which we set in the <code>headers_to_split_on</code> list. We will also print one of the chunks as an example.</p> In\u00a0[\u00a0]: Copied! <pre>#document based chunking\nfrom langchain_text_splitters import MarkdownHeaderTextSplitter\n\nheaders_to_split_on = [\n    (\"#\", \"Header 1\"),\n    (\"##\", \"Header 2\"),\n    (\"###\", \"Header 3\"),\n]\n\nmarkdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\ndocument_based_chunks = markdown_splitter.split_text(markdown_doc[0].page_content)\ndocument_based_chunks[3]\n</pre> #document based chunking from langchain_text_splitters import MarkdownHeaderTextSplitter  headers_to_split_on = [     (\"#\", \"Header 1\"),     (\"##\", \"Header 2\"),     (\"###\", \"Header 3\"), ]  markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on) document_based_chunks = markdown_splitter.split_text(markdown_doc[0].page_content) document_based_chunks[3] Out[\u00a0]: <pre>Document(metadata={'Header 2': 'How to Use our Models?', 'Header 3': 'Inference'}, page_content='This is a simple example of how to use Granite-3.1-1B-A400M-Instruct model.  \\n```python\\nimport torch\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\ndevice = \"auto\"\\nmodel_path = \"ibm-granite/granite-3.1-1b-a400m-instruct\"\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\n# drop device_map if running on CPU\\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\\nmodel.eval()\\n# change input text as desired\\nchat = [\\n{ \"role\": \"user\", \"content\": \"Please list one IBM Research laboratory located in the United States. You should only output its name and location.\" },\\n]\\nchat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\\n# tokenize the text\\ninput_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\\n# generate output tokens\\noutput = model.generate(**input_tokens,\\nmax_new_tokens=100)\\n# decode output tokens into text\\noutput = tokenizer.batch_decode(output)\\n# print output\\nprint(output)\\n```')</pre> <p>As you can see in the output, the chunking successfully split the text by header type.</p> In\u00a0[\u00a0]: Copied! <pre>vector_db = Chroma(\n    collection_name=\"example_collection\",\n    embedding_function=embeddings_model,\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n)\n</pre> vector_db = Chroma(     collection_name=\"example_collection\",     embedding_function=embeddings_model,     persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary ) In\u00a0[\u00a0]: Copied! <pre>vector_db.add_documents(semantic_chunks)\n</pre> vector_db.add_documents(semantic_chunks) Out[\u00a0]: <pre>['84fcc1f6-45bb-4031-b12e-031139450cf8',\n '433da718-0fce-4ae8-a04a-e62f9aa0590d',\n '4bd97cd3-526a-4f70-abe3-b95b8b47661e',\n '342c7609-b1df-45f3-ae25-9d9833829105',\n '46a452f6-2f02-4120-a408-9382c240a26e']</pre> In\u00a0[\u00a0]: Copied! <pre>from langchain.chains import create_retrieval_chain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\n\nprompt_template = \"\"\"&lt;|start_of_role|&gt;user&lt;|end_of_role|&gt;Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {input}&lt;|end_of_text|&gt;\n&lt;|start_of_role|&gt;assistant&lt;|end_of_role|&gt;\"\"\"\n\nqa_chain_prompt = PromptTemplate.from_template(prompt_template)\ncombine_docs_chain = create_stuff_documents_chain(llm, qa_chain_prompt)\nrag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)\n</pre> from langchain.chains import create_retrieval_chain from langchain.prompts import PromptTemplate from langchain.chains.combine_documents import create_stuff_documents_chain  prompt_template = \"\"\"&lt;|start_of_role|&gt;user&lt;|end_of_role|&gt;Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.  {context}  Question: {input}&lt;|end_of_text|&gt; &lt;|start_of_role|&gt;assistant&lt;|end_of_role|&gt;\"\"\"  qa_chain_prompt = PromptTemplate.from_template(prompt_template) combine_docs_chain = create_stuff_documents_chain(llm, qa_chain_prompt) rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain) <p>Using our completed RAG workflow, let's invoke a user query. First, we can strategically prompt the model without any additional context from the vector store we built to test whether the model is using its built-in knowledge or truly using the RAG context. The Granite 3.1 announcement blog references Docling, IBM's tool for parsing various document types and converting them into Markdown or JSON. Let's ask the LLM about Docling.</p> In\u00a0[\u00a0]: Copied! <pre>output = llm.invoke(\"What is Docling?\")\noutput\n</pre> output = llm.invoke(\"What is Docling?\") output Out[\u00a0]: <pre>\"\\nDocling is a platform that allows users to create, share and discover interactive documents. It's like having your own personal library of dynamic content where you can add notes, highlights, bookmarks, and even collaborate with others in real-time. Think of it as the next generation of document management systems designed for modern collaboration needs.\"</pre> <p>Clearly, the model was not trained on information about Docling and without outside tools or information, it cannot provide us with the correct information. The model hallucinates. Now, let's try providing the same query to the RAG chain we built.</p> In\u00a0[\u00a0]: Copied! <pre>rag_output = rag_chain.invoke({\"input\": \"What is Docling?\"})\nrag_output['answer']\n</pre> rag_output = rag_chain.invoke({\"input\": \"What is Docling?\"}) rag_output['answer'] Out[\u00a0]: <pre>'Docling is a powerful tool developed by IBM Deep Search for parsing documents in various formats such as PDF, DOCX, images, PPTX, XLSX, HTML, and AsciiDoc, and converting them into model-friendly formats like Markdown or JSON. This enables easier access to the information within these documents for models like Granite for tasks such as RAG and other workflows. Docling is designed to integrate seamlessly with agentic frameworks like LlamaIndex, LangChain, and Bee, providing developers with the flexibility to incorporate its assistance into their preferred ecosystem. It surpasses basic optical character recognition (OCR) and text extraction methods by employing advanced contextual and element-based preprocessing techniques. Currently, Docling is open-sourced under the permissive MIT License, and the team continues to develop additional features, including equation and code extraction, as well as metadata extraction.'</pre> <p>Great! The Granite model correctly used the RAG context to tell us correct information about Docling while preserving semantic coherence. We proved this same result was not possible without the use of RAG.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#implement-rag-chunking-strategies-with-langchain-and-watsonxai","title":"Implement RAG chunking strategies with LangChain and watsonx.ai\u00b6","text":"<p>Author: Anna Gutowska</p> <p>In this tutorial, you will experiment with several chunking strategies using LangChain and the latest IBM\u00ae Granite\u2122 model now available on watsonx.ai\u2122. The overall goal will be to perform chunking to effective implement retrieval augmented generation (RAG).</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#what-is-chunking","title":"What is chunking?\u00b6","text":"<p>Chunking refers to the process of breaking large pieces of text into smaller text segments or chunks. To emphasize the importance of chunking, it is helpful to understand RAG. RAG is a technique in natural language processing (NLP) that combines information retrieval and large language models (LLMs) to retrieve relevant information from supplemental datasets to optimize the quality of the LLM\u2019s output. To manage large documents, we can use chunking to split the text into smaller snippets of meaningful chunks. These text chunks can then be embedded and stored in a vector database through the use of an embedding model. Finally, the RAG system can then uses semantic search to retrieve only the most relevant chunks. Smaller chunks tend to outperform larger chunks as they tend to be more manageable pieces for models of smaller context window size.</p> <p>Some key components of chunking include:</p> <ul> <li>Chunking strategy: Choosing the right chunking strategy for your RAG application is important as it determines the boundaries for setting chunks. We will explore some of these in the next section.</li> <li>Chunk size: Maximum number of tokens to be in each chunk. Determining the appropriate chunk size usually involves some experimenting.</li> <li>Chunk overlap: The number of tokens overlapping between chunks to preserve context. This is an optional parameter.</li> </ul>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#choosing-the-right-chunking-strategy-for-your-rag-application","title":"Choosing the right chunking strategy for your RAG application\u00b6","text":"<p>There are several different chunking strategies to choose from. It is important to select the most effective chunking technique for the specific use case of your LLM application. Some commonly used chunking processes include:</p> <ul> <li><p>Fixed-size chunking: Splitting text based on a chunk size and optional chunk overlap. This approach is most common and straightforward.</p> </li> <li><p>Recursive chunking: Iterating default separators until one of them produces the preferred chunk size. Default separators include <code>[\"\\n\\n\", \"\\n\", \" \", \"\"]</code>. This chunking method uses hierarchical separators so that paragraphs, followed by sentences and then words, are kept together as much as possible.</p> </li> <li><p>Semantic chunking: Splitting text in a way that groups sentences based on the semantic similarity of their embeddings. Embeddings of high semantic similarity are closer together than those of low semantic similarity. This results in context-aware chunks.</p> </li> <li><p>Document-based chunking: Splitting based on document structure. This splitter can utilize Markdown text, images, tables and even Python code classes and functions as ways of determining structure. In doing so, large documents can be chunked and processed by the LLM.</p> </li> <li><p>Agentic chunking: Leverages agentic AI by allowing the LLM to determine appropriate document splitting based on semantic meaning as well as content structure such as paragraph types, section headings, step-by-step instructions and more. This chunker is experimental and attempts to simulate human reasoning when processing long documents.</p> </li> </ul>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/rag-chunking-strategies/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud\u00ae account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> <p>This step opens a Jupyter Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on GitHub.</p> </li> </ol>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#step-2-set-up-a-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up a watsonx.ai Runtime instance and API key.\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (select your appropriate region and choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an application programming interface (API) key.</p> </li> <li><p>Associate the watsonx.ai Runtime service instance to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We need a few libraries and modules for this tutorial. Make sure to import the following ones and if they're not installed, a quick pip installation resolves the problem.</p> <p>Note, this tutorial was built using Python 3.11.9.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#step-4-initialize-your-llm","title":"Step 4. Initialize your LLM\u00b6","text":"<p>We will use Granite 3.1 as our LLM for this tutorial. To initialize the LLM, we need to set the model parameters. To learn more about these model parameters, such as the minimum and maximum token limits, refer to the documentation.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#step-5-load-your-document","title":"Step 5. Load your document\u00b6","text":"<p>The context we are using for our RAG pipeline is the official IBM announcement for the release of Granite 3.1. We can load the blog to a <code>Document</code> directly from the webpage by using LangChain's <code>WebBaseLoader</code>.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#step-6-perform-text-splitting","title":"Step 6. Perform text splitting\u00b6","text":"<p>Let's provide sample code for implementing each of the chunking strategies we covered earlier in this tutorial available through LangChain.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#fixed-size-chunking","title":"Fixed-size chunking\u00b6","text":"<p>To implement fixed-size chunking, we can use LangChain's <code>CharacterTextSplitter</code> and set a <code>chunk_size</code> as well as <code>chunk_overlap</code>. The <code>chunk_size</code> is measured by the number of characters. Feel free to experiment with different values. We will also set the separator to be the newline character so that we can differentiate between paragraphs. For tokenization, we can use the <code>granite-3.1-8b-instruct</code> tokenizer. The tokenizer breaks down text into tokens that can be processed by the LLM.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#recursive-chunking","title":"Recursive chunking\u00b6","text":"<p>For recursive chunking, we can use LangChain's <code>RecursiveCharacterTextSplitter</code>. Like the fixed-size chunking example, we can experiment with different chunk and overlap sizes.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#semantic-chunking","title":"Semantic chunking\u00b6","text":"<p>Semantic chunking requires an embedding or encoder model. We can use the <code>granite-embedding-30m-english</code> model as our embedding model. We can also print one of the chunks for a better understanding of their structure.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#document-based-chunking","title":"Document-based chunking\u00b6","text":"<p>Documents of various file types are compatible with LangChain's document-based text splitters. For this tutorial's purposes, we will use a Markdown file. For examples of recursive JSON splitting, code splitting and HTML splitting, refer to the LangChain documentation.</p> <p>An example of a Markdown file we can load is the README file for Granite 3.1 on IBM's GitHub.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#step-7-create-vector-store","title":"Step 7. Create vector store\u00b6","text":"<p>Now that we have experimented with various chunking strategies, let's move along with our RAG implementation. For this tutorial, we will choose the chunks produced by the semantic split and convert them to vector embeddings. An open source vector store we can use is Chroma DB. We can easily access Chroma functionality through the <code>langchain_chroma</code> package.</p> <p>Let's initialize our Chroma vector database, provide it with our embeddings model and add our documents produced by semantic chunking.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#step-8-structure-the-prompt-template","title":"Step 8. Structure the prompt template\u00b6","text":"<p>Next, we can move onto creating a prompt template for our LLM. This prompt template allows us to ask multiple questions without altering the initial prompt structure. We can also provide our vector store as the retriever. This step finalizes the RAG structure.</p>"},{"location":"tutorials/generative-ai/rag-chunking-strategies/#step-9-prompt-the-rag-chain","title":"Step 9. Prompt the RAG chain\u00b6","text":""},{"location":"tutorials/generative-ai/rag-chunking-strategies/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, you created a RAG pipeline and experimented with several chunking strategies to improve the system\u2019s retrieval accuracy. Using the Granite 3.1 model, we successfully produced appropriate model responses to a user query related to the documents provided as context. The text we used for this RAG implementation was loaded from a blog on ibm.com announcing the release of Granite 3.1. The model provided us with information only accessible through the provided context since it was not part of the model's initial knowledge base.</p> <p>For those in search of further reading, check out the results of a project comparing LLM performance using HTML structured chunking in comparison to watsonx chunking.</p>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/","title":"Evaluate RAG pipeline using Ragas in Python with watsonx","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install -q \"ragas==0.2.1\"\n%pip install -q langchain_community\n%pip install -q langchain_ibm\n%pip install -q ibm-watsonx-ai\n%pip install -q langchain_core\n%pip install -q nltk\n</pre> %pip install -q \"ragas==0.2.1\" %pip install -q langchain_community %pip install -q langchain_ibm %pip install -q ibm-watsonx-ai %pip install -q langchain_core %pip install -q nltk In\u00a0[\u00a0]: Copied! <pre>import getpass\n\nfrom langchain_community.llms import WatsonxLLM as _WatsonxLLM\nfrom langchain_ibm import WatsonxEmbeddings\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain.schema import LLMResult\nfrom ragas import evaluate\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\nfrom typing import List, Optional, Any\nfrom datasets import load_dataset\nfrom ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n</pre> import getpass  from langchain_community.llms import WatsonxLLM as _WatsonxLLM from langchain_ibm import WatsonxEmbeddings from langchain.callbacks.manager import CallbackManagerForLLMRun from langchain.schema import LLMResult from ragas import evaluate from ragas.llms import LangchainLLMWrapper from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness from typing import List, Optional, Any from datasets import load_dataset from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams <p>Set up your credentials. Input your API key and project ID as strings. Depending on your provisioned service instance, use one of the listed regions as your watsonx URL:</p> <ul> <li>Dallas: https://us-south.ml.cloud.ibm.com</li> <li>London: https://eu-gb.ml.cloud.ibm.com</li> <li>Frankfurt: https://eu-de.ml.cloud.ibm.com</li> <li>Tokyo: https://jp-tok.ml.cloud.ibm.com</li> </ul> In\u00a0[\u00a0]: Copied! <pre>WATSONX_URL = \"https://us-south.ml.cloud.ibm.com\"\nWATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n</pre> WATSONX_URL = \"https://us-south.ml.cloud.ibm.com\" WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \") WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \") In\u00a0[\u00a0]: Copied! <pre>amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\", trust_remote_code=True)\namnesty_qa\n</pre> amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\", trust_remote_code=True) amnesty_qa <pre>Repo card metadata block was not found. Setting CardData to empty.\n</pre> Out[\u00a0]: <pre>DatasetDict({\n    eval: Dataset({\n        features: ['question', 'ground_truth', 'answer', 'contexts'],\n        num_rows: 20\n    })\n})</pre> <p>The data is loaded as a DatasetDict and the features we are interested in are within the <code>\"eval\"</code> split.</p> In\u00a0[\u00a0]: Copied! <pre>dataset = amnesty_qa[\"eval\"]\ndataset\n</pre> dataset = amnesty_qa[\"eval\"] dataset Out[\u00a0]: <pre>Dataset({\n    features: ['question', 'ground_truth', 'answer', 'contexts'],\n    num_rows: 20\n})</pre> <p>To get some insight into the structure and contents of this dataset, load the data into a Pandas dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>df = dataset.to_pandas()\ndf.head()\n</pre> df = dataset.to_pandas() df.head() <p>Datasets for RAG evaluation can be created in various ways. A key element for the creation of this dataset was the external knowledge base provided to an LLM. This knowledge can be obtained from a scraped webpage, basic text file, imported document, and more. In this case, reports collected from Amnesty International are used. The content of the dataset might have been created end-to-end or by using a synthetic data generation approach such as Ragas' <code>TestsetGenerator</code>. Using <code>TestsetGenerator</code> requires the loaded documents, a generator LLM, a critic LLM and an embedding model.</p> <p>In turn, the end-to-end approach involves several steps. Let's assume this approach was taken for the creation of this dataset. This means that either an LLM or a human user generated the questions stored in the <code>question</code> column. To generate the ground truths for each question, the user might have manually created them or generated them using an LLM with the appropriate prompt template. These responses are deemed as the ideal answers and are stored in the <code>ground_truth</code> column. Lastly, a RAG pipeline was used to generate the answers seen in the answer column. When building the RAG pipeline, the external knowledge base was vectorized. Then, when querying the RAG system, the relevant chunks of text that the LLM used for generating each answer were obtained from the vector store by using a similarity algorithm such as the top-k retrieval algorithm. These context chunks were stored in the <code>contexts</code> column.</p> In\u00a0[\u00a0]: Copied! <pre>class WatsonxLLM(_WatsonxLLM):\n    temperature: float = 0.05\n    \"\"\"\n    A workaround for interface incompatibility: Ragas expected all LLMs to\n    have a `temperature` property whereas WatsonxLLM does not define it.\n    \"\"\"\n\n    def _generate(\n        self,\n        prompts: List[str],\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        stream: Optional[bool] = None,\n        **kwargs: Any,\n    ) -&gt; LLMResult:\n        \"\"\"\n        A workaround for interface incompatibility: Ragas expected the\n        `token_usage` property of the LLM result be of a particular shape.\n        WatsonX returns it in a slightly different shape.\n        \"\"\"\n        result: LLMResult = super()._generate(prompts, stop, run_manager, stream, **kwargs)\n        if not result.llm_output or \"token_usage\" not in result.llm_output:\n            return result\n        usage = result.llm_output[\"token_usage\"]\n        if not isinstance(usage, dict):\n            return result\n        result.llm_output[\"token_usage\"] = {\n            \"prompt_tokens\": usage[\"input_token_count\"],\n            \"completion_tokens\": usage[\"generated_token_count\"],\n            \"total_tokens\": usage[\"input_token_count\"] + usage[\"generated_token_count\"],\n        }\n        return result\n</pre> class WatsonxLLM(_WatsonxLLM):     temperature: float = 0.05     \"\"\"     A workaround for interface incompatibility: Ragas expected all LLMs to     have a `temperature` property whereas WatsonxLLM does not define it.     \"\"\"      def _generate(         self,         prompts: List[str],         stop: Optional[List[str]] = None,         run_manager: Optional[CallbackManagerForLLMRun] = None,         stream: Optional[bool] = None,         **kwargs: Any,     ) -&gt; LLMResult:         \"\"\"         A workaround for interface incompatibility: Ragas expected the         `token_usage` property of the LLM result be of a particular shape.         WatsonX returns it in a slightly different shape.         \"\"\"         result: LLMResult = super()._generate(prompts, stop, run_manager, stream, **kwargs)         if not result.llm_output or \"token_usage\" not in result.llm_output:             return result         usage = result.llm_output[\"token_usage\"]         if not isinstance(usage, dict):             return result         result.llm_output[\"token_usage\"] = {             \"prompt_tokens\": usage[\"input_token_count\"],             \"completion_tokens\": usage[\"generated_token_count\"],             \"total_tokens\": usage[\"input_token_count\"] + usage[\"generated_token_count\"],         }         return result <p>For this tutorial, we suggest using the IBM's Granite-3.0-8B-Instruct model as the LLM to achieve similar results. You are free to use any AI model of your choice to compare to this benchmark and choose the best fit for your application. The foundation models available through watsonx.ai can be found here. The purpose of these models in LLM applications is to serve as the reasoning engine that decides which actions to take and responses to produce. To use the <code>WatsonxLLM</code> wrapper with Ragas, we need to use a <code>LangchainLLMWrapper</code>.</p> In\u00a0[\u00a0]: Copied! <pre>watsonx_llm = LangchainLLMWrapper(\n\tlangchain_llm = WatsonxLLM(\n\t\tmodel_id = \"ibm/granite-3-8b-instruct\",\n\t\turl = WATSONX_URL,\n\t\tapikey = WATSONX_APIKEY,\n\t\tproject_id = WATSONX_PROJECT_ID,\n\t\tparams = {\n\t\t\tGenParams.MAX_NEW_TOKENS: 200,\n\t\t\tGenParams.MIN_NEW_TOKENS: 1,\n\t\t\tGenParams.STOP_SEQUENCES: [\"&lt;|endoftext|&gt;\"],\n\t\t\tGenParams.TEMPERATURE: 0.2,\n\t\t\tGenParams.TOP_K: 50,\n\t\t\tGenParams.TOP_P: 1,\n\t\t}\n\t)\n)\n</pre> watsonx_llm = LangchainLLMWrapper( \tlangchain_llm = WatsonxLLM( \t\tmodel_id = \"ibm/granite-3-8b-instruct\", \t\turl = WATSONX_URL, \t\tapikey = WATSONX_APIKEY, \t\tproject_id = WATSONX_PROJECT_ID, \t\tparams = { \t\t\tGenParams.MAX_NEW_TOKENS: 200, \t\t\tGenParams.MIN_NEW_TOKENS: 1, \t\t\tGenParams.STOP_SEQUENCES: [\"&lt;|endoftext|&gt;\"], \t\t\tGenParams.TEMPERATURE: 0.2, \t\t\tGenParams.TOP_K: 50, \t\t\tGenParams.TOP_P: 1, \t\t} \t) ) <p>The Granite\u2122 model is used as the evaluation model. We are not going to use a model to generate any responses because the responses are already stored in the dataset's <code>answer</code> column.</p> <p>The embedding model that we are using is an IBM Slate\u2122 model through a watsonx.ai LangChain wrapper. If no embedding model is defined, Ragas uses open-ai embeddings by default. The embeddings model is essential for evaluation as it is used to embed the data from the separate columns to measure the distance between them.</p> In\u00a0[\u00a0]: Copied! <pre>watsonx_embeddings = WatsonxEmbeddings(\n    model_id = EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n    url = WATSONX_URL,\n    apikey = WATSONX_APIKEY,\n    project_id = WATSONX_PROJECT_ID\n)\n</pre> watsonx_embeddings = WatsonxEmbeddings(     model_id = EmbeddingTypes.IBM_SLATE_30M_ENG.value,     url = WATSONX_URL,     apikey = WATSONX_APIKEY,     project_id = WATSONX_PROJECT_ID ) In\u00a0[\u00a0]: Copied! <pre>result = evaluate(\n    amnesty_qa[\"eval\"],\n    metrics=[\n        context_precision,\n        faithfulness,\n        answer_relevancy,\n        context_recall,\n    ],\n    llm=watsonx_llm,\n    embeddings=watsonx_embeddings)\n</pre> result = evaluate(     amnesty_qa[\"eval\"],     metrics=[         context_precision,         faithfulness,         answer_relevancy,         context_recall,     ],     llm=watsonx_llm,     embeddings=watsonx_embeddings) In\u00a0[\u00a0]: Copied! <pre>print(result)\n</pre> print(result) <pre>{'context_precision': 0.9208, 'faithfulness': 0.6667, 'answer_relevancy': 0.9574, 'context_recall': 0.9167}\n</pre> <p>And that's it. One evaluation of the RAG pipeline has been completed. As mentioned, you can run multiple evaluations, try different models and alter parameters. The more evaluations are performed, the more we can comprehensively assess the accuracy and effectiveness of an LLM system using RAG.</p> <p>In this tutorial, you used Ragas to evaluate your RAG pipeline. Your output included the <code>context_precision</code>, <code>faithfulness</code>, <code>answer_relevancy</code> and <code>context_recall</code> metrics. The LLM used for evaluation was an IBM Granite Model and the embedding model used was an IBM Slate model accessed through the watsonx.ai embeddings API.</p> <p>The evaluation performed is important as it can be applied to future generative AI workflows to assess the performance of your RAG systems and improve upon them.</p> <p>We encourage you to check out the Ragas documentation page for more information on their metrics and evaluation process.</p>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#evaluate-rag-pipeline-using-ragas-in-python-with-watsonx","title":"Evaluate RAG pipeline using Ragas in Python with watsonx\u00b6","text":"<p>Contributors: Anna Gutowska, Viktor Lukashov</p> <p>In this tutorial, we will use the Ragas framework for Retrieval-Augmented Generation (RAG) evaluation in Python using LangChain.</p> <p>RAG is a technique in natural language processing (NLP) that combines information retrieval and generative models to produce more accurate, relevant and contextually aware responses. In traditional language generation tasks, large language models (LLMs) such as OpenAI\u2019s GPT-4 (Generative Pre-trained Transformer) or IBM\u00ae Granite\u2122 Models are used to construct responses based on an input prompt. Common real-world use cases of these large language models are chatbots. These models struggle to produce responses that are contextually relevant, factually accurate or up to date.</p> <p>RAG applications address this limitation by incorporating a retrieval step before response generation. During retrieval, additional text fragments relevant to the prompt are pulled from a knowledge base, such as relevant documents from a large corpus of text, typically stored in a vector database. Finally, an LLM is used for generating responses based on the original prompt augmented with the retrieved context.</p>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#overview-of-rag-evaluation","title":"Overview of RAG evaluation\u00b6","text":"<p>There are many different RAG evaluation frameworks and evaluation metrics. Apart from Ragas, other frameworks include IBM's unitxt and OpenAI's Evals. Unlike the other frameworks, Ragas uses another LLM-as-a-judge to evaluate the performance of a RAG pipeline.</p> <p>There are several evaluation metrics available for measuring the performance of our RAG pipeline. The metrics we will be using in the open source Ragas framework can be split into two parts:</p> <ul> <li><p>Generation evaluation</p> <ul> <li><p>Faithfulness measures if all generated answers can be inferred from the retrieved context.</p> </li> <li><p>Answer relevancy measures the relevancy of the generated response to the question.</p> </li> </ul> </li> <li><p>Retrieval evaluation</p> <ul> <li><p>Context precision measures the ranking of ground-truth relevant entities in the context. Higher context precision means ground-truth relevant items are ranked higher than \u201cnoise.\u201d</p> </li> <li><p>Context recall measures the extent to which the LLM\u2019s generated answers to user queries can be found in the retrieved context.</p> </li> </ul> </li> </ul> <p>These metrics are meant to be subjective proxies for how well a RAG pipeline retrieves relevant information from its knowledge base to form a response. It is important to note, there is no ideal for data, prompts or LLMs. Even context that has a low scoring <code>context_relevance</code> is not necessarily bad context. The low score might be due to some amount of \"noise,\" or less relevant information, or simply because the task itself is open to multiple interpretations. Noise is not necessarily bad either. We, as humans, produce a certain amount of noise in our responses while also being intelligible in answering questions.</p> <p>There are also biases that affect the evaluation of a RAG pipeline such as preference for either shorter or longer responses, otherwise known as length bias. This type of bias can lead to one response being evaluated higher than another because of its length and not its substance.</p> <p>For these reasons, it is best practice to perform multiple evaluations. This exercise can be accomplished through changing the LLM's prompt template, metrics, sequence of evaluation, and more. If you are creating your own data set for your RAG pipeline, it is also recommended to use different models for the LLM generating the responses and the LLM critiquing the responses. If the same model is used for both, then there is greater potential for self-evaluation bias. Because these evaluation metrics are subjective, the results produced by these frameworks should also be checked by human judges.</p> <p>In this tutorial, we do not create a RAG system. Instead, we are using Ragas to evaluate the output of a previously created RAG system. For more information about how to build your RAG system using LangChain, see our detailed RAG tutorial.</p>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You need an IBM Cloud\u00ae account to create a watsonx.ai\u2122 project. Sign up for a free account here.</p>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> <p>This step opens a notebook environment where you can copy the code from this tutorial to implement a RAG evaluation of your own. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on Github.</p> </li> </ol>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#step-2-set-up-a-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up a watsonx.ai Runtime instance and API key.\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (select your appropriate region and choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service instance to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We need a few libraries and modules for this tutorial. Make sure to import the ones listed and if  they're not installed, a quick pip installation resolves the problem. This tutorial was built using Python 3.11.9.</p>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#step-4-load-the-dataset","title":"Step 4. Load the dataset\u00b6","text":"<p>Ragas evaluation requires a dataset containing RAG pipeline executions of several different prompts. In addition to the questions themselves, the dataset needs to contain the expected answers known as \"ground truths,\" the answers produced by the LLM, and the list of context pieces retrieved by the RAG pipeline while answering each question. You can create your own end-to-end dataset but for the purposes of this tutorial, the dataset we are using is available on Hugging Face. Let's load the dataset.</p>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#step-5-establish-the-models-for-evaluating-and-embedding","title":"Step 5. Establish the models for evaluating and embedding\u00b6","text":"<p>In this tutorial, we are using an IBM Granite model as the judge.</p> <p>Ragas uses Open AI models by default. <code>WatsonxLLM</code> is the wrapper for IBM watsonx.ai foundation models. A Ragas-compatible <code>WatsonxLLM</code> wrapper is a work in progress and not yet available. For now, to use Ragas with the Granite models, we need to alter the wrapper's properties.</p>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#step-6-generate-an-evaluation-with-ragas","title":"Step 6. Generate an evaluation with Ragas\u00b6","text":"<p>Finally, we can now run the Ragas evaluation on the dataset. Here, we pass in the dataset, the metrics for evaluation, the LLM and the embedding model as parameters.</p> <p>If warning messages appear, please disregard them, allow the evaluation to complete and print the result.</p>"},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#summary-and-next-steps","title":"Summary and next steps\u00b6","text":""},{"location":"tutorials/generative-ai/rag-evaluation-ragas/#try-watsonx-for-free","title":"Try watsonx for free\u00b6","text":"<p>Build an AI strategy for your business on one collaborative AI and data platform called IBM watsonx. Bring together new generative AI capabilities, powered by foundation models and traditional machine learning on a powerful platform that spans the AI lifecycle. With watsonx.ai, you can train, validate, tune and deploy models with ease and build AI applications in a fraction of the time with a fraction of the data.</p> <p>Try watsonx.ai, the next-generation studio for AI builders.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/","title":"Using the watsonx.ai Time Series Forecasting API to predict energy demand","text":"In\u00a0[2]: Copied! <pre>#installations\n%pip install wget | tail -n 1\n%pip install -U matplotlib | tail -n 1\n%pip install -U ibm-watsonx-ai | tail -n 1\n</pre> #installations %pip install wget | tail -n 1 %pip install -U matplotlib | tail -n 1 %pip install -U ibm-watsonx-ai | tail -n 1 <pre>\n[notice] A new release of pip is available: 24.3.1 -&gt; 25.0\n[notice] To update, run: python3.11 -m pip install --upgrade pip\nRequirement already satisfied: wget in /usr/local/lib/python3.11/site-packages (3.2)\nNote: you may need to restart the kernel to use updated packages.\n\n[notice] A new release of pip is available: 24.3.1 -&gt; 25.0\n[notice] To update, run: python3.11 -m pip install --upgrade pip\nRequirement already satisfied: six&gt;=1.5 in /Users/meredithsyed/Library/Python/3.11/lib/python/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n[notice] A new release of pip is available: 24.3.1 -&gt; 25.0\n[notice] To update, run: python3.11 -m pip install --upgrade pip\nRequirement already satisfied: six&gt;=1.10.0 in /Users/meredithsyed/Library/Python/3.11/lib/python/site-packages (from lomond-&gt;ibm-watsonx-ai) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[3]: Copied! <pre>#imports\nimport os\nimport getpass\nimport wget\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nfrom ibm_watsonx_ai import Credentials\nfrom ibm_watsonx_ai import APIClient\nfrom ibm_watsonx_ai.foundation_models import TSModelInference\nfrom ibm_watsonx_ai.foundation_models.schema import TSForecastParameters\n</pre> #imports import os import getpass import wget import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import mean_absolute_percentage_error  from ibm_watsonx_ai import Credentials from ibm_watsonx_ai import APIClient from ibm_watsonx_ai.foundation_models import TSModelInference from ibm_watsonx_ai.foundation_models.schema import TSForecastParameters <p>To set our credentials, we need the <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> you generated in step 1. We will also set the URL serving as the API endpoint.</p> <p>If you're not sure of the URL, you can use IBM Cloud CLI to retrieve the instance <code>location</code>.</p> <pre><code>ibmcloud login --apikey API_KEY -a https://cloud.ibm.com\nibmcloud resource service-instance INSTANCE_NAME\n</code></pre> In\u00a0[4]: Copied! <pre>WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://us-south.ml.cloud.ibm.com\"\n</pre> WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://us-south.ml.cloud.ibm.com\" <p>We can use the <code>Credentials</code> class to encapsulate our passed credentials.</p> In\u00a0[5]: Copied! <pre>credentials = Credentials(\n    url=URL,\n    api_key=WATSONX_APIKEY\n)\n</pre> credentials = Credentials(     url=URL,     api_key=WATSONX_APIKEY ) <p>To be able to interact with all resources available in watsonx.ai Runtime, you need to set the project which you will be using via the <code>WATSONX_PROJECT_ID</code>.</p> In\u00a0[6]: Copied! <pre>client = APIClient(credentials)\n\nclient.set.default_project(WATSONX_PROJECT_ID)\n</pre> client = APIClient(credentials)  client.set.default_project(WATSONX_PROJECT_ID) Out[6]: <pre>'SUCCESS'</pre> <p></p> In\u00a0[7]: Copied! <pre>filename = 'energy_dataset.csv'\nbase_url = 'https://github.com/IBM/watson-machine-learning-samples/raw/refs/heads/master/cloud/data/energy/'\n\nif not os.path.isfile(filename): wget.download(base_url + filename)\n</pre> filename = 'energy_dataset.csv' base_url = 'https://github.com/IBM/watson-machine-learning-samples/raw/refs/heads/master/cloud/data/energy/'  if not os.path.isfile(filename): wget.download(base_url + filename) <p>Let's examine the last few rows of the dataset. We can see the <code>time</code> column showing a timestamp for each hour. Other columns show numeric data types for energy generation from different sources, weather forecast details and actual energy usage, termed as <code>total_load_actual</code>. This will be our target column, the column for which we are trying to predict values. Since our model is performing multivariate forecasting, we'll use all of the other columns as input to our model to help inform it's predictions. These columns provide details about energy generation and weather forecasts for each hour, enabling us to predict actual energy demand on an hourly basis.</p> In\u00a0[8]: Copied! <pre>df = pd.read_csv(filename)\n\ndf.tail()\n</pre> df = pd.read_csv(filename)  df.tail() Out[8]: time generation biomass generation fossil brown coal/lignite generation fossil gas generation fossil hard coal generation fossil oil generation hydro pumped storage consumption generation hydro run-of-river and poundage generation hydro water reservoir generation nuclear generation other generation other renewable generation solar generation waste generation wind onshore forecast solar day ahead forecast wind onshore day ahead total load forecast total load actual 35059 2018-12-31 19:00:00 297.0 0.0 7634.0 2628.0 178.0 1.0 1135.0 4836.0 6073.0 63.0 95.0 85.0 277.0 3113.0 96.0 3253.0 30619.0 30653.0 35060 2018-12-31 20:00:00 296.0 0.0 7241.0 2566.0 174.0 1.0 1172.0 3931.0 6074.0 62.0 95.0 33.0 280.0 3288.0 51.0 3353.0 29932.0 29735.0 35061 2018-12-31 21:00:00 292.0 0.0 7025.0 2422.0 168.0 50.0 1148.0 2831.0 6076.0 61.0 94.0 31.0 286.0 3503.0 36.0 3404.0 27903.0 28071.0 35062 2018-12-31 22:00:00 293.0 0.0 6562.0 2293.0 163.0 108.0 1128.0 2068.0 6075.0 61.0 93.0 31.0 287.0 3586.0 29.0 3273.0 25450.0 25801.0 35063 2018-12-31 23:00:00 290.0 0.0 6926.0 2166.0 163.0 108.0 1069.0 1686.0 6075.0 61.0 92.0 31.0 287.0 3651.0 26.0 3117.0 24424.0 24455.0 In\u00a0[9]: Copied! <pre># how many rows and columns\ndf.shape\n</pre> # how many rows and columns df.shape Out[9]: <pre>(35064, 19)</pre> In\u00a0[10]: Copied! <pre>timestamp_column = \"time\"\ntarget_column = \"total load actual\"\ncontext_length = 512\nfuture_context = 96\n</pre> timestamp_column = \"time\" target_column = \"total load actual\" context_length = 512 future_context = 96 In\u00a0[11]: Copied! <pre># use the last `context_length` rows for prediction.\nfuture_data = df.iloc[-future_context:,]\ndata = df.iloc[-(context_length + future_context):-future_context,]\n</pre> # use the last `context_length` rows for prediction. future_data = df.iloc[-future_context:,] data = df.iloc[-(context_length + future_context):-future_context,] <p>Let's examine the data further with this data visualization, which plots the hourly timestamps against our target column, <code>total load actual</code>.</p> In\u00a0[12]: Copied! <pre>plt.figure(figsize=(10,2))\nplt.plot(np.asarray(data[timestamp_column], 'datetime64[s]'), data[target_column])\nplt.title(\"Actual Total Load\")\nplt.show()\n</pre> plt.figure(figsize=(10,2)) plt.plot(np.asarray(data[timestamp_column], 'datetime64[s]'), data[target_column]) plt.title(\"Actual Total Load\") plt.show() <p>In preparing data for timeseries forecasting, models can have different requirements for preprocessing the data. The Granite TTM model card recommends that data be scaled and a preprocessing script is provided as an example. For the purposes of this tutorial, we'll use our dataset 'as-is.'</p> <p></p> In\u00a0[13]: Copied! <pre>for model in client.foundation_models.get_time_series_model_specs()[\"resources\"]:\n    print('--------------------------------------------------')\n    print(f'model_id: {model[\"model_id\"]}')\n    print(f'functions: {model[\"functions\"]}')\n    print(f'long_description: {model[\"long_description\"]}')\n    print(f'label: {model[\"label\"]}')\n</pre> for model in client.foundation_models.get_time_series_model_specs()[\"resources\"]:     print('--------------------------------------------------')     print(f'model_id: {model[\"model_id\"]}')     print(f'functions: {model[\"functions\"]}')     print(f'long_description: {model[\"long_description\"]}')     print(f'label: {model[\"label\"]}')  <pre>--------------------------------------------------\nmodel_id: ibm/granite-ttm-1024-96-r2\nfunctions: [{'id': 'time_series_forecast'}]\nlong_description: TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. Given the last 1024 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length) in future. This model is targeted towards a forecasting setting of context length 1024 and forecast length 96 and recommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc)\nlabel: granite-ttm-1024-96-r2\n--------------------------------------------------\nmodel_id: ibm/granite-ttm-1536-96-r2\nfunctions: [{'id': 'time_series_forecast'}]\nlong_description: TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. Given the last 1536 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length) in future. This model is targeted towards a forecasting setting of context length 1536 and forecast length 96 and recommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc)\nlabel: granite-ttm-1536-96-r2\n--------------------------------------------------\nmodel_id: ibm/granite-ttm-512-96-r2\nfunctions: [{'id': 'time_series_forecast'}]\nlong_description: TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. Given the last 512 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length) in future. This model is targeted towards a forecasting setting of context length 512 and forecast length 96 and recommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc)\nlabel: granite-ttm-512-96-r2\n</pre> <p>We need to specify the <code>model_id</code> that will be used for inferencing.</p> In\u00a0[14]: Copied! <pre>ts_model_id = client.foundation_models.TimeSeriesModels.GRANITE_TTM_512_96_R2\n</pre> ts_model_id = client.foundation_models.TimeSeriesModels.GRANITE_TTM_512_96_R2 <p> Next, we need to intialize an object of the <code>TSModelInference</code> class. <code>TSModelInference</code> is a wrapper around watsonx.ai models that provides integration around the models. This <code>ts_model</code> object will be able to make an API call to the model hosted by watsonx.ai.</p> In\u00a0[15]: Copied! <pre>ts_model = TSModelInference(\n    model_id=ts_model_id,\n    api_client=client\n)\n</pre> ts_model = TSModelInference(     model_id=ts_model_id,     api_client=client ) <p>Here, we provide a set of model parameters as a specification that will influence our predictions. In this case, we set the frequency to a time range of 1 hour. <code>TSForecastParameters</code> documentation provides details about the parameters to the model. You can also see more details in our API documentation.</p> In\u00a0[16]: Copied! <pre>forecasting_params = TSForecastParameters(\n    timestamp_column=timestamp_column, \n    freq=\"1h\", \n    target_columns=[target_column],\n) \n</pre> forecasting_params = TSForecastParameters(     timestamp_column=timestamp_column,      freq=\"1h\",      target_columns=[target_column], )  <p></p> In\u00a0[17]: Copied! <pre>results = ts_model.forecast(data=data, params=forecasting_params)['results'][0]\n</pre> results = ts_model.forecast(data=data, params=forecasting_params)['results'][0] <p>For an initial evaluation of our model's performance, we'll use a data visualization. Here, we plot the predictions (shown in gold), along with the historical data (shown in blue) for past values. We can also see the green dotted line, our <code>future_data</code> or the ground truth against which to compare our predictions.</p> <p>This gold line predicts hourly energy demand based on the patterns in the historical data. In evaluating the green dotted line against the gold line, we can see instances of overlap where the model's predictions are accurate.</p> In\u00a0[18]: Copied! <pre>plt.figure(figsize=(10,2))\nplt.plot(np.asarray(data[timestamp_column], dtype='datetime64[s]'), data[target_column], label=\"Historical data\")\nplt.plot(np.asarray(results[timestamp_column], dtype='datetime64[s]'), results[target_column], label=\"Predicted\")\nplt.plot(np.asarray(future_data[timestamp_column], dtype='datetime64[s]'), future_data[target_column], label=\"True\", linestyle='dashed')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n</pre> plt.figure(figsize=(10,2)) plt.plot(np.asarray(data[timestamp_column], dtype='datetime64[s]'), data[target_column], label=\"Historical data\") plt.plot(np.asarray(results[timestamp_column], dtype='datetime64[s]'), results[target_column], label=\"Predicted\") plt.plot(np.asarray(future_data[timestamp_column], dtype='datetime64[s]'), future_data[target_column], label=\"True\", linestyle='dashed') plt.legend(loc='center left', bbox_to_anchor=(1, 0.5)) plt.show() <p>We can also calculate evaluation metrics to quantify the accuracy of the model. We'll use the Mean Absolute Percentage Error (MAPE) as our metric. A lower percentage is better and our result, at less than 10%, is a good indication of the performance of the model for this use case.</p> <p>Using the watsonx.ai Timeseries Forecasting API, the Granite model's accuracy and response time for a multivariate forecasting problem is noteworthy.</p> In\u00a0[53]: Copied! <pre>mape = mean_absolute_percentage_error(future_data[target_column], results[target_column]) * 100\nmape\n</pre> mape = mean_absolute_percentage_error(future_data[target_column], results[target_column]) * 100 mape Out[53]: <pre>6.081524040679701</pre> <p></p>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#using-the-watsonxai-time-series-forecasting-api-to-predict-energy-demand","title":"Using the watsonx.ai Time Series Forecasting API to predict energy demand\u00b6","text":"<p>Authors: Aleksandra K\u0142eczek and Meredith Syed</p> <p>In this tutorial, you will discover how to perform timeseries forecasting using the watsonx.ai Timeseries Forecasting API and SDK to predict energy demand. This notebook demonstrates the usage of a pre-trained time series foundation model for multivariate forecasting tasks and demonstrates a variety of features available using Time Series Foundation Models.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#watsonxai-timeseries-forecasting-api-and-sdk","title":"watsonx.ai Timeseries Forecasting API and SDK\u00b6","text":"<p>We are pleased to announce that the watsonx.ai Timeseries Forecasting API and SDK is available to the public in GA from February 2025. Built on the Granite Timeseries models, this new API/SDK offers unparalleled accuracy and ease of use for tackling real-world forecasting challenges. At the core of this capability is IBM\u2019s Granite timeseries models (TinyTimeMixers), a family of open source pre-trained, lightweight models based on a novel architecture.</p> <p>Granite timeseries models are designed to:</p> <p>-Adapt to diverse datasets: Whether it\u2019s IoT sensor data, stock market prices, or energy demand, Granite dynamically adjusts to data irregularities, seasonality, and trends, allowing for zero-shot forecasting.</p> <p>-Deliver state-of-the-art accuracy: Granite timeseries models outperform many larger timeseries models like TimesFM, Moirai, Chronos, and more that are 10x the size on various industry benchmarks, while being computationally efficient.</p> <p>-Support scalability: From small-scale business needs to enterprise-grade implementations, the models handle forecasting at any scale.</p> <p>These models are trained on a collection of publicly available datasets which permit commercial use. A total of 700 Million timepoints were used from selected datasets from the Monash Forecasting Repository and the LOTSA collection. Granite timeseries models currently support multiple input context lengths (512, 1024, and 1536 data points), and are capable of multivariate predictions across a number of channels and IDs. For more information about use cases and training details, see the model card and the IBM Research paper.</p> <p>By launching the watsonx.ai Timeseries Forecasting API, IBM reaffirms its commitment to providing developers with the tools they need to build impactful AI solutions. This new capability enhances IBM watsonx.ai, an enterprise-grade AI developer studio, enabling clients to develop AI and ML solutions with tools for agent development, fine-tuning, RAG, guardrails and more.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/time-series-forecasting-api/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud\u00ae account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> <p>This step opens a Jupyter Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on GitHub.</p> </li> </ol>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#step-2-set-up-a-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up a watsonx.ai Runtime instance and API key.\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (select your appropriate region and choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an application programming interface (API) key.</p> </li> <li><p>Associate the watsonx.ai Runtime service instance to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We need a few libraries and modules for this tutorial. Make sure to import the following ones and if they're not installed, a quick pip installation resolves the problem.</p> <p>We'll be using <code>os</code> and <code>getpass</code> to set up our credentials. To load the dataset, we'll use <code>wget</code> and <code>pandas</code>. For data visualization, we'll use <code>numpy</code> and <code>matplotlib</code>.</p> <p>In addition to common Python libraries, we'll be using <code>ibm-watsonx-ai</code> which allows you to work with IBM watsonx.ai services. You can train, store, and deploy your models, score them using APIs, and finally integrate them with your application development. We'll use the <code>Credentials</code> and <code>APIClient</code> functionality to connect to watsonx.ai services.</p> <p>For timeseries capabilities, we're using new functionality from <code>TSModelInference</code> and <code>TSForecastParameters</code> to define the model and set parameters for forecasting. In order to evaluate the performance of the model, we'll use the Mean Absolute Percentage Error (MAPE) metric from <code>sklearn</code>.</p> <p>Note, this tutorial was built using Python 3.11</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#step-4-load-and-prepare-the-dataset","title":"Step 4. Load and prepare the dataset\u00b6","text":"<p>This tutorial uses the Hourly energy demand dataset. This dataset contains 4 years of electrical consumption and energy generation data gathered in Spain from 2015 to 2018 aggregated by hour. It is a modified version of the Hourly energy demand generation and weather dataset. You can find more details about the dataset, including metadata in the preceding links.</p> <p>For simplicity, the dataset was prepared to have no missing values and to remove irrelevant columns.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#split-the-data","title":"Split the data\u00b6","text":"<p>For our forecasting problem, we'll need to split the data into 2 sets, the first which will be used as historical data. We'll provide the historical data to the model and ask it to predict future values. In order to test the accuracy of our predictions, we'll also need to compare these predictions against ground truth values. For our experiment, we'll use a second subset of our dataset as the ground truth and we'll compare the predicted values to the actual values in this ground truth subset.</p> <p>Granite timeseries models come in different context lengths of 512, 1024 and 1536 tokens. The context length describes the amount of information the model can consider when making a single prediction. For the Granite timeseries models, each row in a dataset counts as one token towards the context length. We'll be using the 512 token context length timeseries model, <code>ibm/granite-ttm-512-96-r2</code>, in our experiment. In order to do this, we need a dataset of 512 rows to provide as input to the model, our historical data. We'll term this input dataset as <code>data</code>. We have many more rows in our dataset than are needed for this prediction problem. In this case, to subset the data, we'll simply take the most recent timestamps or the last rows of the dataset.</p> <p>The second dataset we need is our evaluation or ground truth dataset. We'll use the last 96 rows of data in our dataset for this purpose. We'll call this <code>future_context</code> and we'll use this data to compare against our predictions.</p> <p>Here, we also specify the columns to be used for prediction. The identifiers <code>timestamp_column</code> and <code>target_column</code> set these values for the model.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#step-5-select-a-granite-timeseries-foundation-model-from-watsonxai","title":"Step 5. Select a Granite Timeseries Foundation Model from watsonx.ai\u00b6","text":"<p>Using the <code>get_time_series_model_specs</code> function from the watsonx.ai SDK, we can list the models available from the Timeseries Forecasting API. For this experiment, we'll use the 512 context length model, but we see that the larger context models are also available.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#step-6-forecasting-using-your-model","title":"Step 6. Forecasting using your model\u00b6","text":"<p>In order to create predictions, we'll call the <code>forecast()</code> method to compute values for our target variable <code>total_load_actual</code>, predicting electricity usage for the future time range. Using the Granite TTM (TinyTimeMixer) model via the API, we get a fast response time, even for a multivariate forecasting problem.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-api/#summary","title":"Summary\u00b6","text":"<p>In this tutorial, you used the watsonx.ai Timeseries Forecasting API and SDK to make predictions for energy demand.</p> <p>Whether you\u2019re improving and streamlining operations, predicting share prices, navigating emissions for sustainability or exploring entirely new use cases, this Forecasting API delivers the foundation for smarter, data-driven decisions, complementing watsonx.ai\u2019s existing foundation model library.</p> <p>Get started today for free at our IBM watsonx Developer Hub.</p> <p>For more use cases using watsonx.ai and IBM's open source Granite models, try out our other tutorials.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-lag-llama/","title":"Time series forecasting lag llama","text":"In\u00a0[\u00a0]: Copied! <pre>!git clone https://github.com/time-series-foundation-models/lag-llama/\n</pre> !git clone https://github.com/time-series-foundation-models/lag-llama/ In\u00a0[\u00a0]: Copied! <pre>%cd lag-llama\n</pre> %cd lag-llama In\u00a0[\u00a0]: Copied! <pre>%pip install -r requirements.txt --quiet\n</pre> %pip install -r requirements.txt --quiet <p>Next we need to install pretrained model weights from the HuggingFace repository where they're stored. To do this, we use the Hugging-Face CLI to download the trained Lag-Llama checkpoint and install it in the same <code>lag-llama</code> directory within our watsonx.ai Studio environment. If you're using a different environment to run this notebook, you'll want to make sure to install the weights in the same directory where you've downloaded the model.</p> In\u00a0[\u00a0]: Copied! <pre>!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir ~/work/lag-llama\n</pre> !huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir ~/work/lag-llama In\u00a0[\u00a0]: Copied! <pre>from itertools import islice\n\nimport pandas as pd\nimport matplotlib.dates as mdates\nfrom matplotlib import pyplot as plt\nfrom tqdm.autonotebook import tqdm\n\nimport torch\n\nfrom gluonts.evaluation import make_evaluation_predictions, Evaluator\nfrom gluonts.dataset.repository.datasets import get_dataset\nfrom gluonts.dataset.pandas import PandasDataset\n\nfrom lag_llama.gluon.estimator import LagLlamaEstimator\nfrom ibm_watson_studio_lib import access_project_or_space\n</pre> from itertools import islice  import pandas as pd import matplotlib.dates as mdates from matplotlib import pyplot as plt from tqdm.autonotebook import tqdm  import torch  from gluonts.evaluation import make_evaluation_predictions, Evaluator from gluonts.dataset.repository.datasets import get_dataset from gluonts.dataset.pandas import PandasDataset  from lag_llama.gluon.estimator import LagLlamaEstimator from ibm_watson_studio_lib import access_project_or_space <p>Next, we load the data. You can find the <code>.csv</code> file that was used in our Github repo. In this case, I've loaded the dataset as a data asset from within the watsonx.ai Studio project. In order to retrieve the data asset from the project, you'll need a project access token. If you're running this notebook in a different environment, you can load the dataset in whatever way is easiest for you. Once the data is loaded, we can see this time series data contains the hourly temperatures in New York during the months of October and November, gathered from ACIS Web Services.</p> In\u00a0[\u00a0]: Copied! <pre>wslib = access_project_or_space({'token':'&lt;YOUR_WATSONX.AI_STUDIO_PROJECT_TOKEN&gt;'})\n\n# Fetch the data from a file\nmy_file = wslib.load_data('nyc_hourly_temp.csv')\n\n# Read the CSV data file into a pandas DataFrame\nmy_file.seek(0)\ndf = pd.read_csv(my_file)\n\ndf.head()\n</pre> wslib = access_project_or_space({'token':''})  # Fetch the data from a file my_file = wslib.load_data('nyc_hourly_temp.csv')  # Read the CSV data file into a pandas DataFrame my_file.seek(0) df = pd.read_csv(my_file)  df.head() <p>The data has some missing readings that we need to fill in by interpolating between values, so that there are no missing values in the time series. Because the dataset is a univariate dataset and contains values for only one variable, each row consists simply of a date and a temperature reading in Fahrenheit.</p> In\u00a0[\u00a0]: Copied! <pre>for col in df.columns:\n# Check if column is not of string type\n    if df[col].dtype != 'object' and pd.api.types.is_string_dtype(df[col]) == False:\n        df[col] = df[col].astype('float32')\n\ndf['Datetime'] = pd.to_datetime(df['Datetime'])\ndf = df.set_index(\"Datetime\")\ndf = df.interpolate('linear')\ndf.head()\n</pre> for col in df.columns: # Check if column is not of string type     if df[col].dtype != 'object' and pd.api.types.is_string_dtype(df[col]) == False:         df[col] = df[col].astype('float32')  df['Datetime'] = pd.to_datetime(df['Datetime']) df = df.set_index(\"Datetime\") df = df.interpolate('linear') df.head() In\u00a0[\u00a0]: Copied! <pre># Plot the time series\ndf['Temp (F)'].plot()\nplt.title('Hourly Temp (F) NYC Oct - Nov 2023')\nplt.xlabel('Date')\nplt.ylabel('Temp (F)')\nplt.axhline(y=32.0, color='r', linestyle='--')\nplt.show()\n</pre> # Plot the time series df['Temp (F)'].plot() plt.title('Hourly Temp (F) NYC Oct - Nov 2023') plt.xlabel('Date') plt.ylabel('Temp (F)') plt.axhline(y=32.0, color='r', linestyle='--') plt.show() In\u00a0[\u00a0]: Copied! <pre>prediction_length = 8 # we'll predict out 8 hours\ncontext_length = prediction_length * 3 * 7 # how many lags to use as context, how far we look back into the future, one week in this case\nnum_samples = 20 # how many samples for each distribution\ndevice = \"cpu\" # where should we run it, options are \"cuda\" or \"cpu\"\n\nbatch_size = 64\n</pre> prediction_length = 8 # we'll predict out 8 hours context_length = prediction_length * 3 * 7 # how many lags to use as context, how far we look back into the future, one week in this case num_samples = 20 # how many samples for each distribution device = \"cpu\" # where should we run it, options are \"cuda\" or \"cpu\"  batch_size = 64 <p>Now we create the forecaster. This step consists of two key steps: first, creating a <code>LagLlamaEstimator</code> which uses all the parameters copied from the downloaded Lag-Llama model. The second step is to create a <code>LagLlamaPredictor</code> using the <code>create_predictor()</code> method of the estimator. This allows us to pass a <code>context_length</code> sized window of data to get forecasts from the predictor.</p> In\u00a0[\u00a0]: Copied! <pre>ckpt_path = \"lag-llama.ckpt\"\nckpt = torch.load(ckpt_path, map_location=device)\nestimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n\nzs_estimator = LagLlamaEstimator(\n        ckpt_path=ckpt_path,\n        prediction_length=prediction_length,\n        context_length=context_length,\n        device=torch.device('cpu'),\n\n        # estimator args\n        input_size=estimator_args[\"input_size\"],\n        n_layer=estimator_args[\"n_layer\"],\n        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n        n_head=estimator_args[\"n_head\"],\n        scaling=estimator_args[\"scaling\"],\n        time_feat=estimator_args[\"time_feat\"],\n\n        nonnegative_pred_samples=True,\n\n        # linear positional encoding scaling\n        rope_scaling={\n            \"type\": \"linear\",\n            \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n        },\n\n        batch_size=batch_size,\n        num_parallel_samples=num_samples,\n    )\n\nzs_predictor = zs_estimator.create_predictor(zs_estimator.create_transformation(), zs_estimator.create_lightning_module())\n</pre> ckpt_path = \"lag-llama.ckpt\" ckpt = torch.load(ckpt_path, map_location=device) estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]  zs_estimator = LagLlamaEstimator(         ckpt_path=ckpt_path,         prediction_length=prediction_length,         context_length=context_length,         device=torch.device('cpu'),          # estimator args         input_size=estimator_args[\"input_size\"],         n_layer=estimator_args[\"n_layer\"],         n_embd_per_head=estimator_args[\"n_embd_per_head\"],         n_head=estimator_args[\"n_head\"],         scaling=estimator_args[\"scaling\"],         time_feat=estimator_args[\"time_feat\"],          nonnegative_pred_samples=True,          # linear positional encoding scaling         rope_scaling={             \"type\": \"linear\",             \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),         },          batch_size=batch_size,         num_parallel_samples=num_samples,     )  zs_predictor = zs_estimator.create_predictor(zs_estimator.create_transformation(), zs_estimator.create_lightning_module()) In\u00a0[\u00a0]: Copied! <pre>valid_end = round(len(df) * 0.9)\ndate_list = pd.date_range(df[valid_end:].index[2] + pd.Timedelta(hours=6), periods=6, freq=\"1d\").tolist()\n\nzs_forecasts = []\nzs_tss = []\n\nfor d in date_list:\n  print(d)\n  forecast_it, ts_it = make_evaluation_predictions(\n          dataset=PandasDataset(df[:d], freq=\"1h\", target=\"Temp (F)\"),\n          predictor=zs_predictor,\n          num_samples=num_samples\n  )\n  zs_forecasts.append(list(forecast_it))\n  zs_tss.append(list(ts_it))\n</pre> valid_end = round(len(df) * 0.9) date_list = pd.date_range(df[valid_end:].index[2] + pd.Timedelta(hours=6), periods=6, freq=\"1d\").tolist()  zs_forecasts = [] zs_tss = []  for d in date_list:   print(d)   forecast_it, ts_it = make_evaluation_predictions(           dataset=PandasDataset(df[:d], freq=\"1h\", target=\"Temp (F)\"),           predictor=zs_predictor,           num_samples=num_samples   )   zs_forecasts.append(list(forecast_it))   zs_tss.append(list(ts_it)) <p>To evaluate the forecasts, we use an <code>evaluator</code> object, also from the <code>gluonts.evaluation</code> library. We\u2019ll generate several metrics that we can use to evaluate the accuracy of our forecasts, but we\u2019ll focus on Mean Absolute Percentage Error or MAPE.</p> In\u00a0[\u00a0]: Copied! <pre>evaluator = Evaluator()\nzs_a_metrics = [] # aggregated forecast metrics, we'll use the MAPE metric to evaluate\nzs_t_metrics = [] # information about each time series, we'll use this to graph\n\nfor (t,s) in zip(zs_tss, zs_forecasts):\n  agg_metrics, ts_metrics = evaluator(t, s)\n  zs_a_metrics.append(agg_metrics)\n  zs_t_metrics.append(ts_metrics)\n</pre> evaluator = Evaluator() zs_a_metrics = [] # aggregated forecast metrics, we'll use the MAPE metric to evaluate zs_t_metrics = [] # information about each time series, we'll use this to graph  for (t,s) in zip(zs_tss, zs_forecasts):   agg_metrics, ts_metrics = evaluator(t, s)   zs_a_metrics.append(agg_metrics)   zs_t_metrics.append(ts_metrics) <p>Once we have the evaluations for each prediction, we can graph each prediction. The generated chart shows each of our 6 zero-shot forecasts (shown in green) and the time series data that lead up to it (the blue line). For each forecast we can see the mean forecast as the green line, the boundaries of the 50% prediction interval in dark green and the boundaries of the 90% prediction interval in lighter green. This shows us how certain our model is about the forecast at each step. This is the advantage of a probabilistic model: it will show us how certain it is at each step in the forecast.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(20, 9))\nplt.rcParams.update({'font.size': 11})\ndate_formater = mdates.DateFormatter('%H:%M') #%b %d \\'%y %H')\n\n\nfor idx in range(len(zs_forecasts)):\n  ax = plt.subplot(3, 3, idx+1)\n  t = zs_tss[idx][0][zs_forecasts[idx][0].start_date.to_timestamp() - pd.Timedelta(hours=12):]\n  \n  smape = float(f'{zs_t_metrics[idx][\"MAPE\"][0]:.4f}') * 100.0\n  smape = float(f'{smape:.2f}')\n  ax.set_title(\"Start: \" + str(zs_t_metrics[idx][\"forecast_start\"][0]) + \" MAPE: \" + str(smape) + \"%\")\n  plt.plot(t.index.to_timestamp(), t[0])\n  ax.xaxis.set_major_formatter(date_formater)\n  #ax.set_ticks()\n  #ax.set_xticklabels(fontsize=\"small\") #[\"Day \" + str(i + 1) for i in range(7)], fontsize=\"small\")\n  ax.set_xlabel('Date')\n  ax.set_ylabel('Temp (F)')\n  zs_forecasts[idx][0].plot(color='g')\n  plt.axhline(y=32.0, color='r', linestyle='--')\n  plt.xticks(rotation=45)\n\nplt.gcf().tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.suptitle(\"Mean Absolute Percentage Error across forecasts\")\nplt.legend([\"Actual\", \"Predicted\",'50% Prediction Interval', '90% Prediction Interval', \"Freeze Line\"], fontsize=\"small\")\nplt.show()\n</pre> plt.figure(figsize=(20, 9)) plt.rcParams.update({'font.size': 11}) date_formater = mdates.DateFormatter('%H:%M') #%b %d \\'%y %H')   for idx in range(len(zs_forecasts)):   ax = plt.subplot(3, 3, idx+1)   t = zs_tss[idx][0][zs_forecasts[idx][0].start_date.to_timestamp() - pd.Timedelta(hours=12):]      smape = float(f'{zs_t_metrics[idx][\"MAPE\"][0]:.4f}') * 100.0   smape = float(f'{smape:.2f}')   ax.set_title(\"Start: \" + str(zs_t_metrics[idx][\"forecast_start\"][0]) + \" MAPE: \" + str(smape) + \"%\")   plt.plot(t.index.to_timestamp(), t[0])   ax.xaxis.set_major_formatter(date_formater)   #ax.set_ticks()   #ax.set_xticklabels(fontsize=\"small\") #[\"Day \" + str(i + 1) for i in range(7)], fontsize=\"small\")   ax.set_xlabel('Date')   ax.set_ylabel('Temp (F)')   zs_forecasts[idx][0].plot(color='g')   plt.axhline(y=32.0, color='r', linestyle='--')   plt.xticks(rotation=45)  plt.gcf().tight_layout() plt.subplots_adjust(top=0.9) plt.suptitle(\"Mean Absolute Percentage Error across forecasts\") plt.legend([\"Actual\", \"Predicted\",'50% Prediction Interval', '90% Prediction Interval', \"Freeze Line\"], fontsize=\"small\") plt.show() <p>In this tutorial you learned about foundation models for forecasting. You used the Lag-Llama model, a foundation model trained on time series data specifically built for forecasting. You used the base model to perform zero-shot forecasting on an hourly temperature dataset.</p> <p>You can learn more about Lag-Llama at the Lag-Llama GitHub Repository.</p> <p>To learn about IBM's foundation models for time series forecasting see Using the IBM Granite models for time series forecasting.</p> <p>For a longer version of this tutorial including fine-tuning of the model, see Time Series Forecasting with Lag-Llama.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-lag-llama/#time-series-forecasting-with-lag-llama","title":"Time Series Forecasting with Lag-Llama\u00b6","text":"<p>In this tutorial, we'll use the Lag-Llama model, an open source foundation model, to predict the overnight low temperatures in a zero-shot learning scenario. Zero-shot learning is when the model is not trained on the data it\u2019s trying to predict. It\u2019s an interesting test of how well our model can detect and respond to the patterns present in the time series.</p> <p>Foundation models for time series data are similar to other forms of generative AI that are trained on large-scale time series datasets and can output either deterministic or probabilistic forecasts. A time series foundation model can create forecasts without pretraining, similar to how a large language model (LLM) can output text without being pretrained on a task. Lag-Llama is a general-purpose foundation model for probabilistic time series forecasting on univariate datasets that uses a transformer architecture. You can read more about it in the paper here: Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting by Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Hena Ghonia, et. al.</p> <p>To unpack this a little, a probabilistic forecast is one that generates a probability distribution of values for each forecast step rather than just a single value. This method is helpful to indicate how certain the model is about its predictions. A wide distribution would indicate low certainty while a narrower range of values would indicate that the model is fairly certain about its predictions. Purely deterministic forecasts don't tell us how certain they are about their forecasts, which can be problematic if we're trying to ascertain how confident we should be in our forecasts.</p> <p>Given this background about the Lag Llama model and time series forecasting, let's see how the model performs in our experiment.</p> <p>Note: This notebook is designed to run within IBM watsonx.ai Studio using the Runtime 24.1 on Python 3.11 S environment.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-lag-llama/#step-1-clone-the-lag-llama-repository-and-install-libraries","title":"Step 1: Clone the Lag-Llama repository and install libraries\u00b6","text":"<p>To use the Lag-Llama model, we'll clone the open source GitHub repository and install it in our IBM watsonx.ai Studio project.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-lag-llama/#step-2-import-libraries-and-data","title":"Step 2: Import libraries and data\u00b6","text":"<p>Next, we need to import libraries to work with Lag-Llama. The library that the Lag-Llama team built to work with Lag-Llama uses GluonTS, a PyTorch based library for working with time series data and forecasting models. You might need to restart the kernel after installing the required libraries.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-lag-llama/#step-3-create-a-zero-shot-predictor","title":"Step 3: Create a zero-shot predictor\u00b6","text":"<p>We'll create some configuration settings to use with our model. The <code>prediction_length</code> is how many time steps each prediction should contain. Because our data is daily, we'll predict the next week of temperatures in each forecast. The <code>context_length</code> sets the number of time points back into the past that the model should look for lagged correlations. We don't want this to be too wide or too narrow, for each dataset the optimal value will be different.</p>"},{"location":"tutorials/generative-ai/time-series-forecasting-lag-llama/#step-4-zero-shot-forecasting","title":"Step 4: Zero-shot forecasting\u00b6","text":"<p>In this step, we'll ask the model to create 6 overnight forecasts focused on the last week of November. We can use the <code>make_evaluation_predictions</code> from the <code>gluonts.evaluation</code> library to generate our forecasts.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/","title":"Perform zero-shot classification with a foundation model","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install -U langchain_ibm\n%pip install ibm_watsonx_ai\n\nimport getpass\n\nfrom langchain_ibm import WatsonxLLM\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n</pre> %pip install -U langchain_ibm %pip install ibm_watsonx_ai  import getpass  from langchain_ibm import WatsonxLLM from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams In\u00a0[\u00a0]: Copied! <pre>credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \"),\n    \"project_id\": getpass.getpass(\"Please enter your project ID (hit enter): \"),\n}\n</pre> credentials = {     \"url\": \"https://us-south.ml.cloud.ibm.com\",     \"apikey\": getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \"),     \"project_id\": getpass.getpass(\"Please enter your project ID (hit enter): \"), } In\u00a0[\u00a0]: Copied! <pre>model = WatsonxLLM(\n\tmodel_id =  \"ibm/granite-3-8b-instruct\",\n\turl = credentials.get(\"url\"),\n\tapikey = credentials.get(\"apikey\"),\n\tproject_id =  credentials.get(\"project_id\"),\n\tparams={\n        GenParams.MAX_NEW_TOKENS: 500,\n        GenParams.MIN_NEW_TOKENS: 1,\n        GenParams.REPETITION_PENALTY: 1.1,\n        GenParams.TEMPERATURE: 0.7, # Adjust for variable responses\n        GenParams.TOP_K: 100,\n        GenParams.TOP_P: 0,\n    },\n)\n</pre> model = WatsonxLLM( \tmodel_id =  \"ibm/granite-3-8b-instruct\", \turl = credentials.get(\"url\"), \tapikey = credentials.get(\"apikey\"), \tproject_id =  credentials.get(\"project_id\"), \tparams={         GenParams.MAX_NEW_TOKENS: 500,         GenParams.MIN_NEW_TOKENS: 1,         GenParams.REPETITION_PENALTY: 1.1,         GenParams.TEMPERATURE: 0.7, # Adjust for variable responses         GenParams.TOP_K: 100,         GenParams.TOP_P: 0,     }, ) In\u00a0[\u00a0]: Copied! <pre>def generate_text(prompt):\n    response = None  # Ensure the variable is defined before the try block\n    try:\n        response = model.generate([prompt])\n        return str(response)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        if response:\n            print(f\"Response: {response}\")\n            return None\n    # Define the prompt here\ndefined_prompt = \"Set the class name for the issue described to either: high or low. Issue: Users are reporting that they are unable to upload files.\"\n</pre> def generate_text(prompt):     response = None  # Ensure the variable is defined before the try block     try:         response = model.generate([prompt])         return str(response)     except Exception as e:         print(f\"Error: {e}\")         if response:             print(f\"Response: {response}\")             return None     # Define the prompt here defined_prompt = \"Set the class name for the issue described to either: high or low. Issue: Users are reporting that they are unable to upload files.\" In\u00a0[\u00a0]: Copied! <pre># Generate and print the text based on the defined prompt\ngenerated_text = generate_text(defined_prompt)\nprint(\"Generated text:\", generated_text)\n</pre> # Generate and print the text based on the defined prompt generated_text = generate_text(defined_prompt) print(\"Generated text:\", generated_text) <p>In this example, the model correctly infers the classification label \"high\" based on its ability to understand the critical impact of the inability to upload files for users.</p> In\u00a0[\u00a0]: Copied! <pre># Define the prompt here\ndefined_prompt = \"Classify the following customer review as 'Postive', 'Negative', 'Neutral': Customer review: 'My IT issue was not resolved.'\"\n\n# Generate and print the text based on the defined prompt\ngenerated_text = generate_text(defined_prompt)\nprint(\"Generated text:\", generated_text)\n</pre> # Define the prompt here defined_prompt = \"Classify the following customer review as 'Postive', 'Negative', 'Neutral': Customer review: 'My IT issue was not resolved.'\"  # Generate and print the text based on the defined prompt generated_text = generate_text(defined_prompt) print(\"Generated text:\", generated_text) <p>The model is able to perform sentiment analysis and classify the review correctly as \"Negative\". This capability can be useful for a variety of domains, not just IT. Try out your own prompts to explore how you could use zero-shot classification to automate time-consuming tasks.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#perform-zero-shot-classification-with-a-foundation-model","title":"Perform zero-shot classification with a foundation model\u00b6","text":"<p>Author: Vanna Winland</p> <p>In this tutorial, we will use an IBM Granite (TM) Model to perform zero-shot classification.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#what-is-zero-shot-classification","title":"What is zero-shot classification?\u00b6","text":"<p>Zero-shot classification uses zero-shot prompting, a prompt engineering technique that allows a model to perform a task without any specific training or examples. This is an application of zero-shot learning (ZSL) which is a machine learning method that relies on the pre-trained models\u2019 ability to recognize and categorize objects or concepts on post-training. ZSL is similar to few-shot learning (FSL), the ability for a model to make an accurate prediction by training on a small number of labeled examples. Both techniques are used to enable models to perform tasks that they haven\u2019t been explicitly trained on.</p> <p>Researchers have been experimenting with machine learning models for classification tasks since the 50\u2019s. The Perceptron is an early classification model that uses a decision-boundary to classify data into different groups. Most ML/DL methods rely on supervised learning techniques, and therefore need to be trained using a large amount of task-specific labeled training data. This presents a challenge as the large, annotated datasets required to train these models simply do not exist for every domain. Some researchers motivated by these constraints say that LLMs are the way around these data limitations.</p> <p>LLMs are designed to perform natural language processing (NLP) and natural language inferencing (NLI) tasks which gives them a natural ability to perform zero-shot text classification. The model can generate data based on semantic descriptions because it\u2019s trained on a large corpus of data. Like LLMs, foundation models use a transformer architecture that enables them to classify labels without any specific training data for a classification task. This is possible because of the models\u2019 ability to perform self-supervised learning and transfer learning to classify data into unseen classes. To the advantage of data science, this approach eliminates the requirement for large datasets with human-annotated labels because it automates the preprocessing portion of the classification pipeline.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#how-foundation-models-perform-zero-shot-classification","title":"How foundation models perform zero-shot classification\u00b6","text":"<p>Foundation models are built on the transformer architecture that that can take raw text at scale through its attention mechanism, and understand how words relate to each other to form a statistical representation of language. The transformer is a type of neural network architecture designed to interpret meaningful representations of sequences or collections of data points. This capability is the reason why these models perform so well on NLP tasks.</p> <p>The transformer model architecture includes an encoder-decoder structure and self-attention mechanism that allows the model to draw connections between input and output using an auto-regressive prediction. The encoder processes the tokenized input data into embeddings that represent the data in a format the model can read. The decoder interprets the embeddings to generate an output. The self-attention mechanism computes the weights for each word, or token, in sentence based on its relationship to every other word in the sentence. This allows the model to take the semantic and syntactic relationships between words. The self-attention mechanism is integral for entailment, a NLI task that heavily relies on the self-attention mechanism because it helps the model understand the context within text data.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#what-models-are-best-for-zero-shot-classification","title":"What models are best for zero-shot classification?\u00b6","text":"<p>Choosing the right model for your zero-shot classification depends on your classification task. It\u2019s no surprise that there is an abundance to choose from, let\u2019s consider three types of models:</p> <ul> <li><p>Zero-shot classification model is a type of model that can classify data into categories without task-specific labeled examples during the prediction phase. The models rely on training data, which is normally a large-scale, general dataset to classify new, unseen classes. One of the most popular zero-shot classification models for zero-shot text classification is HuggingFace\u2019s facebook/bart-large-mnli model based on the BART-large transformer model architecture. Zero-shot classification models perform well on generalized tasks but, because there is no fine-tuning on specific tasks or datasets, accuracy may be limited. Because of this limitation, the model requires well-formulated prompts.</p> </li> <li><p>Large Language Models (LLMs) like GPT and BERT are designed to perform a variety of natural language processing tasks. This can limit use cases using multimodal datasets such as images and audio. These models are designed to handle text data, often using deep learning architectures likes transformers. LLMs are trained on a large corpora of text data, giving them extensive knowledge of language, syntax, semantics, and some domain-specific knowledge. These models generally perform well with little to no task-specific fine-tuning, making them suitable for zero-shot and few-shot classification scenarios. Due to their generalized training, LLMs may have limited accuracy for specialized tasks especially ones that require domain specific data. These models are best to work with when the dataset is text-based.</p> </li> <li><p>Foundation models are multimodal, meaning they are often trained on a myriad of modalities including text, images, and speech. These models are generally versatile and after pretraining can undergo optimization for many different tasks. IBM\u2019s Granite (TM) Models classify data by using a large language model (LLM) trained on a curated dataset of business-relevant information, including legal, financial, and technical domains, which allows it to analyze text and identify patterns to categorize data into specific classes based on the context and semantic meaning within the text. Because of their multimodal capacity, these types of models can handle image and text classification. These models are ideal when you need a broad range of capabilities or want to handle multiple types of data for instance, image or audio classification.</p> </li> </ul>"},{"location":"tutorials/generative-ai/zero-shot-classification/#use-cases","title":"Use cases\u00b6","text":"<ul> <li><p>Image classification \u2013 Computer vision is a type of artificial intelligence (AI) that allows uses machine learning models to analyze images and videos. A crucial task within computer vision is image classification, which involves labeling and categorizing groups of pixels within an image. Image classification is used in many domains such as social media for photo tagging, self-driving cars, and even healthcare.</p> </li> <li><p>Text classification \u2013 NLP utilizes text classification to enable the models understanding of human language. Text classification is used for many NLP tasks such as sentiment analysis, similarity scoring, key phrase detection and much more. A popular use case, and one we\u2019ll be exploring in this tutorial, is for customer service analysis.</p> </li> <li><p>Audio Classification \u2013 The goal of audio classification is to use the model to recognize and distinguish between audio recordings so that it can perform sound categorization. This form of classification is used in smart home and security systems and technologies like text to speech applications.</p> </li> </ul>"},{"location":"tutorials/generative-ai/zero-shot-classification/#prerequisites","title":"Prerequisites\u00b6","text":"<p>To follow this tutorial you need an IBM Cloud account to create a watsonx.ai project.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/generative-ai/zero-shot-classification/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code, text, images, and data visualizations to formulate a well-formed analysis.</p> <ol> <li><p>Log in to watsonx.ai Runtime using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>Take note of the project ID in project &gt; Manage &gt; General &gt; Project ID. You\u2019ll need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial to perform zero-shot classification on your own. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This Jupyter Notebook is available on GitHub.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#step-2-set-up-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up watsonx.ai Runtime instance and API key\u00b6","text":"<p>In this step, you associate your project with the watsonx.ai service.</p> <ol> <li><p>Create a watsonx.ai Runtime instance (choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key in watsonx.ai.</p> </li> <li><p>Associate the watsonx.ai Runtime to the project you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/generative-ai/zero-shot-classification/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We'll need some libraries and modules for this tutorial. Make sure to import the ones below, and if they're not installed, you can resolve this with a quick pip install.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#step-4-set-up-your-watsonx-credentials","title":"Step 4. Set up your Watsonx credentials\u00b6","text":"<p>Run the following to input and save your watsonx.ai Runtime API key and project id:</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#step-5-set-up-the-model-for-zero-shot-classification","title":"Step 5. Set up the model for zero-shot classification\u00b6","text":"<p>Next we'll setup IBM's Granite-3.0-8B-Instruct to perform zero-shot classification.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#step-6-define-the-prompt","title":"Step 6. Define the prompt\u00b6","text":"<p>Now that the model is prepared to perform zero-shot classification, let's define a prompt. Imagine a scenario where it's imperative to triage certain data, perhaps an IT department's flooded inbox full of user-described technical issues. In this example, the model is asked to classify an IT issue as belonging to either the class \"High\" or \"Low,\" indicating the priority of the issue. The prompt should showcase the model's out-of-the box capability to classify the priority of IT issues.</p> <p>The code block below sets up and defines the prompt that the model will respond to. The prompt can be any input, but let's try out the example first. Run the code block to define your user prompt along with some example input text.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#step-7-perform-zero-shot-classification","title":"Step 7. Perform zero-shot classification\u00b6","text":"<p>Once the prompt is defined, we can run the next block to allow the model to predict and print its output.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#classify-service-reviews-based-on-sentiment-classification","title":"Classify service reviews based on sentiment classification\u00b6","text":"<p>Let's apply zero-shot classification to a different aspect of a department's everyday workflow. The same IT department used in the above example has a backlog of customer support reviews that need organized and analyzed. The organization feels the best way to accomplish this is to classify them based on sentiment: \"Postive,\" \"Negative,\", \"Neutral\".</p> <p>Run the code block below with the defined prompt and customer review to classify the sentiment of the text.</p>"},{"location":"tutorials/generative-ai/zero-shot-classification/#summary","title":"Summary\u00b6","text":"<p>In this tutorial we set up IBM's 3-8B-Instruct model to perform zero-shot classification. Then we defined a user prompt and scenario to perform zero-shot classification. We tested out two examples including one semantic and one sentiment analysis.</p>"},{"location":"tutorials/instructional-guides/post-training-quantization/","title":"Post training quantization of Granite-3.0-8B-Instruct in Python with watsonx","text":"<p>Author: Joshua Noble</p>"},{"location":"tutorials/instructional-guides/post-training-quantization/#what-is-post-training-quantization","title":"What is post training quantization?","text":"<p>Quantization of large language models (LLMs) is a model optimization technique that reduces memory space and latency by sacrificing some model accuracy. Large transformer-based models such as LLMs often require significant GPU resources to run. In turn, a quantized model can allow you to run machine learning inference on limited GPUs or even on a CPU. Frameworks such as TensorFlow Lite (tflite) can run quantized TensorFlow models on edge devices including phones or microcontrollers. In the era of larger and larger LLMs, quantization is an essential technique during the training, fine tuning and inference stages of modeling. Quantization is especially helpful for users who want to run models locally on limited hardware machines. Low-resource hardware that has a hardware machine learning accelerator can also run quantized models very efficiently.</p>"},{"location":"tutorials/instructional-guides/post-training-quantization/#quantization-methods","title":"Quantization methods","text":"<p>Quantization is the process of mapping input values from a large set of continuous elements to a smaller set with a finite number of elements. Quantization methods have evolved rapidly and been an area of active research. For instance, a simple algorithm might be integer quantization, which is simply scaling 32-bit floating point (f32) numbers to 8-bit integers (int8). This technique is often called zero-point quantization. More sophisticated techniques use FP8, an 8-bit floating point with a dynamic range that can be set by the user. In this tutorial, we'll use k-means quantization to create very small models. That saves us from needing to do model calibration or the time-intensive step of creating an importance matrix that defines the importance of each activation in the neural network.</p> <p>We'll focus on post training quantization (PTQ) which focuses on decreasing the precision (and thus resource demands) after the model is trained. Quantization-Aware Training (QAT) is a common quantization technique for mitigating model accuracy and perplexity degradation that arises from quantization but is a more advanced technique with more limited use cases. In particular, we'll use k-means quantization via llama.cpp, an open source library that quantizes PyTorch models.</p> <p>When working with LLMs, model quantization allows us to convert high-precision floating-point numbers in the neural network layers to low-precision numbers that consume much less space. We'll be converting models to GPT-Generated Unified Format (GGUF) to run them efficiently in constrained resource scenarios. GGUF is a binary format optimized for quick loading and saving of models that makes it efficient for inference purposes. It achieves this efficiency by combining the model parameters (weights and biases) with more metadata for effective execution. Because it\u2019s compatible with various programming languages such as Python and R and supports fine tuning so users can adapt LLMs to specialized applications, it has become a popular format.</p> <p>In this tutorial, we\u2019ll quantize the IBM\u00ae Granite-3.0-8B-Instruct model in a few different ways to show the size of the models and compare how they perform on a task. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on Github.</p>"},{"location":"tutorials/instructional-guides/post-training-quantization/#step-1-set-up-your-enviroment","title":"Step 1. Set up your enviroment","text":"<p>This can be done in either a terminal on OSX or Linux or in VS Code on Windows.</p> <p>First, we need to create a virtual environment in Python that we can use to save all of our libraries:</p> <pre><code>python3 -m venv .\nsource ./bin/activate\n</code></pre> <p>Next, we'll want to install the HuggingFace Hub library so that we can use to download the Granite Model files</p> <pre><code>./bin/pip install huggingface_hub\n</code></pre> <p>Next, either save the following script to a file and run it, or simply start a Python3 session and run it there:</p> <pre><code>from huggingface_hub import snapshot_download\n\nsnapshot_download(\"ibm-granite/granite-3.0-8b-instruct\", local_dir=\"granite-3.0-8b-instruct\")\n</code></pre> <p>Now we can copy the files into our local directory for easier access</p> <p>Next up, we need to install llama.cpp at a system level. The instructions here to build from source are rather complex but very well documented here</p> <p>Alternatively on OSX you can use homebrew:</p> <pre><code>brew install llama.cpp\n</code></pre> <p>On Mac and Linux, the Nix package manager can be used:</p> <pre><code>nix profile install nixpkgs#llama-cpp\n</code></pre> <p>You can also look for prebuilt binaries at the Github Releases page</p> <p>Once we have llama.cpp installed, we can install the libraries to run the llama-cpp scripts in the virtual environment:</p> <pre><code>./bin/pip install 'llama-cpp-python[server]'\n</code></pre>"},{"location":"tutorials/instructional-guides/post-training-quantization/#step-2-prepare-to-quantize","title":"Step 2. Prepare to quantize","text":"<p>We need to get the entire repository for llama.cpp in order to convert models to GGUF format.</p> <pre><code>git clone https://github.com/ggerganov/llama.cpp\n</code></pre> <p>Now we install libraries that we'll need in order to run the GGUF conversion:</p> <pre><code>./bin/pip install -r llama.cpp/requirements.txt\n</code></pre> <p>Now we're ready to convert the model to gguf using a script inside the repository:</p> <pre><code>./bin/python3 llama.cpp/convert_hf_to_gguf.py granite-3.0-8b-instruct\n</code></pre> <p>This gives us a new GGUF file based on our original model files.</p> <p>Now we're ready to quantize. Common quantization schemes that are supported by GGUF include:</p> <p>2-bit quantization: This offers the highest compression, significantly reducing model size and inference speed, though with a potential impact on accuracy. 4-bit quantization: This balances compression and accuracy, making it suitable for many practical applications. 6-bit quantization: This quantization setting provides higher accuracy than 4 or 2, with some reduction in memory requirements that can help in running it locally.</p> <p>For a the smallest possible model we can use 2 bit quantization:</p> <pre><code>/opt/homebrew/bin/llama-quantize \\\n    granite-3.0-8b-instruct/granite-3.0-8b-instruct-F16.gguf \\\n    granite-3.0-8b-instruct/granite-8B-instruct-Q2_K.gguf Q2_K\n</code></pre> <p>For a medium sized quanitized model, we can use 4 bit quantization:</p> <pre><code>/opt/homebrew/bin/llama-quantize \\\n    granite-3.0-8b-instruct/granite-3.0-8b-instruct-F16.gguf \\\n    granite-3.0-8b-instruct/granite-8B-instruct-Q4_K_M.gguf Q4_K_M\n</code></pre> <p>For a larger quanitized model on a machine with more resources, we can use 6 bit quantization:</p> <pre><code>/opt/homebrew/bin/llama-quantize \\\n    granite-3.0-8b-instruct/granite-3.0-8b-instruct-F16.gguf \\\n    granite-3.0-8b-instruct/granite-8B-instruct-Q6.gguf Q6_K\n</code></pre> <p>Here's a size comparison</p> Param Size Q2_K 3.17 GB Q4_K_M 5.06 GB Q6_K 6.87 GB <p>Each of these steps may take up to 15 minutes but when they're done we have multiple versions of the model that we can use to compare.</p>"},{"location":"tutorials/instructional-guides/post-training-quantization/#step-3-running-the-model","title":"Step 3. Running the model","text":"<p>We could run the model in llamacpp with the following command:</p> <pre><code>llama-server -m granite-3.0-8b-instruct/granite-8B-instruct-Q4_K_M.gguf --port 8080\n</code></pre> <p>This allows you to open a webpage at <code>localhost:8080</code> which hosts an interactive session which runs Granite Instruct as a helpful assistant. </p> <p>We could also add the file to ollama. To do this, first we need to create a modelfile:</p> <pre><code># Modelfile\nFROM \"&lt;PATH_TO_MODEL&gt;/granite-8B-instruct-Q4_K_M.gguf\"\n\nPARAMETER stop \"&lt;|im_start|&gt;\"\nPARAMETER stop \"&lt;|im_end|&gt;\"\n\nTEMPLATE \"\"\"\n&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\"\"\"\n</code></pre> <p>Save this file as <code>GCI_8b_modelfile_Q4</code>. Start ollama:</p> <pre><code>ollama serve\n</code></pre> <p>Then load the model into ollama:</p> <pre><code>ollama create Granite_Instruct:8b_Q4 -f GCI_8b_modelfile_Q4\n</code></pre> <p>Now you're ready to run it:</p> <pre><code>ollama run Granite_Instruct:8b_Q4\n</code></pre> <p>We repeat the process with two other model files, one for the 2 bit and one for the 6 bit quantization:</p> <pre><code># Modelfile\nFROM \"&lt;PATH_TO_MODEL&gt;/granite-8B-instruct-Q2.gguf\"\n\nPARAMETER stop \"&lt;|im_start|&gt;\"\nPARAMETER stop \"&lt;|im_end|&gt;\"\n\nTEMPLATE \"\"\"\n&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\"\"\"\n</code></pre> <p>Save this as <code>GCI_8b_modelfile_Q2</code> and load it into ollama with the following command:</p> <pre><code>ollama create Granite_Instruct:8b_Q2 -f GCI_8b_modelfile_Q2\n</code></pre> <p>One last modelfile for the Q6 version:</p> <pre><code># Modelfile\nFROM \"&lt;PATH_TO_MODEL&gt;/granite-8B-instruct-Q6.gguf\"\n\nPARAMETER stop \"&lt;|im_start|&gt;\"\nPARAMETER stop \"&lt;|im_end|&gt;\"\n\nTEMPLATE \"\"\"\n&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n\"\"\"\n</code></pre> <p>Save this as <code>GCI_8b_modelfile_Q6</code> and load into ollama with the following command:</p> <pre><code>ollama create Granite_Instruct:8b_Q6 -f GCI_8b_modelfile_Q6\n</code></pre> <p>Now we're ready to compare our models.</p>"},{"location":"tutorials/instructional-guides/post-training-quantization/#step-4-comparing-the-models","title":"Step 4. Comparing the models","text":"<p>Let's compare how the 2 bit version does to the 4 bit version with a prompt asking the model to fix bugs in code. First, we'll load the 2 bit version:</p> <pre><code>ollama run Granite_Instruct:8b_Q2\n</code></pre> <p>Once that's running, we can paste our prompt:</p> <pre><code>&gt;&gt;&gt;Fix any errors in the following Python function and format it correctly:\n\nimport math\n\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return math.pi* self.radius** math.pi\n\n    def perimeter(self):\n        return math.pi* math.pi * self.radius\n\ndef print_circle_properties(circles):\n    for circle in circles:\n        print(f\"Circle with radius {circle.radius}:\")\n        print(f\"Area: {circle.area()}\")\n        print(f\"Perimeter: {circle.perimeter()}\\n\")\n\ncircles = [Circle(3), Circle(5), Circle(7)]\nprint_circle_properties(circle)\n</code></pre> <p>With the 2 bit quantization, the model doesn't perform quite as well as we might hope. It misses that the equation for the area of a circle and the perimeters are both wrong. It also doesn't explain any of the errors.</p> <pre><code>I have fixed the errors in your Python code and formatted it as follows:\nimport math\n\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return math.pi* self.radius** math.pi\n\n    def perimeter(self):\n        return math.pi * self.radius\n\ndef print_circle_properties(circles):\n    for circle in circles:\n        print(f\"Circle with radius {circle.radius}:\")\n        print(f\"Area: {circle.area()}\")\n        print(f\"Perimeter: {circle.perimeter()}\\n\")\n\ncircles = [Circle(3), Circle(5), Circle(7)]\nprint_circle_properties(circles)\n</code></pre> <p>Now let's load the Q4:</p> <pre><code>ollama run Granite_Instruct:8b_Q4\n</code></pre> <p>Copy/pasting in the previous prompt, we get a much better response, although the 3rd item doesn't quite make sense:</p> <pre><code>There are a few errors in the provided Python code:\n1. In the `area()` method of the `Circle` class, the `math.pi* self.radius** math.pi` should be `math.pi * (self.radius ** 2)`. This is because the area of a circle is given by pi times the \nsquare of its radius.\n2. The `perimeter()` method in the `Circle` class should return `2 * math.pi * self.radius`, since the perimeter of a circle is given by 2 * pi * radius.\n3. In the `print_circle_properties(circles)` function, the parameter `circle` should be `circle`.\n4. The last line of code should be `print_circle_properties(circles)`, not `print_circle_properties(circle)`.\nHere is the corrected code:\n```python\nimport math\n\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return math.pi * (self.radius ** 2)\n\n    def perimeter(self):\n        return 2 * math.pi * self.radius\n\ndef print_circle_properties(circles):\n    for circle in circles:\n        print(f\"Circle with radius {circle.radius}:\")\n        print(f\"Area: {circle.area()}\")\n        print(f\"Perimeter: {circle.perimeter()}\\n\")\n\ncircles = [Circle(3), Circle(5), Circle(7)]\nprint_circle_properties(circles)\n```\n</code></pre> <p>Finally, we can load the 6 bit quantized model:</p> <pre><code>ollama run Granite_Instruct:8b_Q6\n</code></pre> <p>This answer is much better than either of the two previous versions:</p> <pre><code>The provided Python function contains two errors:\n\n1. In the `area` method, there is an extra `math.pi` before `self.radius**`. It should be `return math.pi * self.radius ** 2`.\n2. In the `perimeter` method, there are two `math.pi` multiplications. The correct formula for perimeter of a circle is `2 * math.pi * radius`. Therefore, it should be `return 2 * math.pi * \nself.radius`.\n3. At the end of the code, the variable name is misspelled as `circle` instead of `circles`.\n\nHere's the corrected version of the code:\n```python\nimport math\n\nclass Circle:\n    def __init__(self, radius):\n        self.radius = radius\n\n    def area(self):\n        return math.pi * self.radius ** 2\n\n    def perimeter(self):\n        return 2 * math.pi * self.radius\n\ndef print_circle_properties(circles):\n    for circle in circles:\n        print(f\"Circle with radius {circle.radius}:\")\n        print(f\"Area: {circle.area()}\")\n        print(f\"Perimeter: {circle.perimeter()}\\n\")\n\ncircles = [Circle(3), Circle(5), Circle(7)]\nprint_circle_properties(circles)\n```\n</code></pre> <p>We can see that the 2 bit quantized model does save space but also is less adept at picking out errors in our code as well as fixing them and explaining them. The 4 bit model corrects all of the code errors but doesn't fully explain its instructions. The 6 bit model corrects all of the errors and explains those errors correctly and in greater detail.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/","title":"Build an AI stylist with IBM Granite using watsonx.ai","text":"<p>In this tutorial, you will be guided through how to build a generative AI-powered personal stylist. This tutorial leverages the IBM Granite\u2122 Vision 3.2 large language model (LLM) for processing image input and Granite 3.2 with the latest enhanced reasoning capabilities for formulating customizable outfit ideas.</p> In\u00a0[\u00a0]: Copied! <pre># Install required packages\n!pip install -q image ibm-watsonx-ai\n</pre> # Install required packages !pip install -q image ibm-watsonx-ai In\u00a0[\u00a0]: Copied! <pre># Required imports\nimport getpass, os, base64, json\nfrom ibm_watsonx_ai import Credentials\nfrom ibm_watsonx_ai.foundation_models import ModelInference\nfrom PIL import Image\n</pre> # Required imports import getpass, os, base64, json from ibm_watsonx_ai import Credentials from ibm_watsonx_ai.foundation_models import ModelInference from PIL import Image <p>To set our credentials, we need the <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> you generated in step 1. We will also set the URL serving as the API endpoint.</p> In\u00a0[\u00a0]: Copied! <pre>WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://us-south.ml.cloud.ibm.com\"\n</pre> WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://us-south.ml.cloud.ibm.com\" <p>We can use the <code>Credentials</code> class to encapsulate our passed credentials.</p> In\u00a0[\u00a0]: Copied! <pre>credentials = Credentials(\n    url=URL,\n    api_key=WATSONX_APIKEY\n)\n</pre> credentials = Credentials(     url=URL,     api_key=WATSONX_APIKEY ) In\u00a0[\u00a0]: Copied! <pre>def augment_api_request_body(user_query, image):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": user_query\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image}\"\n                }\n            }]\n        }\n    ]\n    \n    return messages\n</pre> def augment_api_request_body(user_query, image):     messages = [         {             \"role\": \"user\",             \"content\": [{                 \"type\": \"text\",                 \"text\": user_query             },             {                 \"type\": \"image_url\",                 \"image_url\": {                     \"url\": f\"data:image/jpeg;base64,{image}\"                 }             }]         }     ]          return messages <p>We can also instantiate the model interface using the <code>ModelInference</code> class.</p> In\u00a0[\u00a0]: Copied! <pre>model = ModelInference(\n        model_id=\"ibm/granite-vision-3-2-2b\",\n        credentials=credentials,\n        project_id=WATSONX_PROJECT_ID,\n        params={\n            \"max_tokens\": 400,\n            \"temperature\": 0       \n        }\n    )\n</pre> model = ModelInference(         model_id=\"ibm/granite-vision-3-2-2b\",         credentials=credentials,         project_id=WATSONX_PROJECT_ID,         params={             \"max_tokens\": 400,             \"temperature\": 0                }     ) In\u00a0[\u00a0]: Copied! <pre>directory = \"images\"  #directory name\nimages = []\nfilenames = []\n\nfor filename in os.listdir(directory): \n  if filename.endswith(\".jpeg\") or filename.endswith(\".png\"): \n    filepath = directory + '/' +filename\n    with  open(filepath, \"rb\") as f:\n      images.append(base64.b64encode(f.read()).decode('utf-8')) \n    filenames.append(filename)\n    print(filename)\n</pre> directory = \"images\"  #directory name images = [] filenames = []  for filename in os.listdir(directory):    if filename.endswith(\".jpeg\") or filename.endswith(\".png\"):      filepath = directory + '/' +filename     with  open(filepath, \"rb\") as f:       images.append(base64.b64encode(f.read()).decode('utf-8'))      filenames.append(filename)     print(filename) <pre>image1.jpeg\nimage12.jpeg\nimage6.jpeg\nimage7.jpeg\nimage13.jpeg\nimage10.jpeg\nimage8.jpeg\nimage4.jpeg\nimage5.jpeg\nimage9.jpeg\nimage11.jpeg\nimage2.jpeg\nimage3.jpeg\n</pre> In\u00a0[\u00a0]: Copied! <pre>user_query = \"\"\"Provide a description, category, and occasion for the clothing item or shoes in this image.  \n\n                Classify the category as shirt, pants, or shoes.\n                Classify the occasion as casual or formal.\n                \n                Ensure the output is valid JSON. Do not create new categories or occasions. Only use the allowed classifications.\n                \n                Your response should be in this schema: \n                {\n                    \"description\": \"&lt;description&gt;\",\n                    \"category\": \"&lt;category&gt;\",\n                    \"occasion\": \"&lt;occasion&gt;\"\n                }\n                \"\"\"\n\nimage_descriptions = []\n\nfor i in range(len(images)):\n    image = images[i]\n    message = augment_api_request_body(user_query, image)\n    response = model.chat(messages=message)\n    result = response['choices'][0]['message']['content']\n    print(result)\n    image_descriptions.append(result)\n</pre> user_query = \"\"\"Provide a description, category, and occasion for the clothing item or shoes in this image.                    Classify the category as shirt, pants, or shoes.                 Classify the occasion as casual or formal.                                  Ensure the output is valid JSON. Do not create new categories or occasions. Only use the allowed classifications.                                  Your response should be in this schema:                  {                     \"description\": \"\",                     \"category\": \"\",                     \"occasion\": \"\"                 }                 \"\"\"  image_descriptions = []  for i in range(len(images)):     image = images[i]     message = augment_api_request_body(user_query, image)     response = model.chat(messages=message)     result = response['choices'][0]['message']['content']     print(result)     image_descriptions.append(result) <pre>{\n    \"description\": \"A pair of polished brown leather dress shoes with a brogue detailing on the toe box and a classic oxford design.\",\n    \"category\": \"shoes\",\n    \"occasion\": \"formal\"\n}\n{\n    \"description\": \"A pair of checkered trousers with a houndstooth pattern, featuring a zippered pocket and a button closure at the waist.\",\n    \"category\": \"pants\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"A light blue, button-up shirt with a smooth texture and a classic collar, suitable for casual to semi-formal occasions.\",\n    \"category\": \"shirt\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"A pair of khaki pants with a buttoned waistband and a button closure at the front.\",\n    \"category\": \"pants\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"A blue plaid shirt with a collar and long sleeves, featuring chest pockets and a button-up front.\",\n    \"category\": \"shirt\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"A pair of bright orange, short-sleeved t-shirts with a crew neck and a simple design.\",\n    \"category\": \"shirt\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"A pair of blue suede sneakers with white laces and perforations, suitable for casual wear.\",\n    \"category\": \"shoes\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"A pair of red canvas sneakers with white laces, isolated on a white background.\",\n    \"category\": \"shoes\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"A pair of grey dress pants with a smooth texture and a classic design, suitable for formal occasions.\",\n    \"category\": \"pants\",\n    \"occasion\": \"formal\"\n}\n{\n    \"description\": \"A plain white T-shirt with short sleeves and a crew neck, displayed from the front and back.\",\n    \"category\": \"shirt\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"A black short-sleeved t-shirt with a crew neck and a simple design.\",\n    \"category\": \"shirt\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"Black pants with a zippered pocket and a buttoned fly, showing the waistband and pocket details.\",\n    \"category\": \"pants\",\n    \"occasion\": \"casual\"\n}\n{\n    \"description\": \"A pair of tan leather boots with a chunky sole and a high-top design, suitable for casual wear.\",\n    \"category\": \"shoes\",\n    \"occasion\": \"casual\"\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>reasoning_model = ModelInference(\n        model_id=\"ibm/granite-3-2-8b-instruct\",\n        credentials=credentials,\n        project_id=WATSONX_PROJECT_ID\n)\n</pre> reasoning_model = ModelInference(         model_id=\"ibm/granite-3-2-8b-instruct\",         credentials=credentials,         project_id=WATSONX_PROJECT_ID ) <p>To align the filenames with the image descriptions, we can enumerate the list of image descriptions and create a list of dictionaries in which we store the description, category, occasion and filename of each item in the respective fields.</p> In\u00a0[\u00a0]: Copied! <pre># Add filenames to the image descriptions\ncloset = []\nfor i, desc in enumerate(image_descriptions):\n    desc_dict = json.loads(desc)\n    desc_dict['filename'] = filenames[i]\n    image_descriptions[i] = json.dumps(desc_dict)\n\ncloset = [json.loads(js) for js in image_descriptions]\n</pre> # Add filenames to the image descriptions closet = [] for i, desc in enumerate(image_descriptions):     desc_dict = json.loads(desc)     desc_dict['filename'] = filenames[i]     image_descriptions[i] = json.dumps(desc_dict)  closet = [json.loads(js) for js in image_descriptions] <p>Now, let's query the Granite 3.2 model with reasoning to produce an outfit for our specified criteria using the <code>closet</code> list.</p> In\u00a0[\u00a0]: Copied! <pre>occasion = input(\"Enter the occasion\")                 #casual or formal (e.g. \"casual\")\ntime_of_day = input(\"Enter the time of day\")           #morning, afternoon or evening (e.g. \"morning\")\nlocation = input(\"Enter the location\")                 #any location (e.g. \"park\")\nseason = input(\"Enter the season\")                     #spring, summer, fall or winter (e.g. \"fall\")\n\nprompt = f\"\"\"Use the description, category, and occasion of the clothes in my closet to put together an outfit for a {occasion} {time_of_day} at the {location}.\n                The event takes place in the {season} season. Make sure to return only one shirt, bottoms, and shoes.\n                Use the description, category, and occasion provided. Do not classify the items yourself. \n                Include the file name of each image in your output along with the file extension. Here are the items in my closet: {closet}\"\"\"\n\nmessages = [\n        {\n            \"role\": \"control\",\n            \"content\": \"thinking\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": f\"{prompt}\"\n                }\n            ]\n        }\n    ]\n\noutfit = reasoning_model.chat(messages=messages)['choices'][0]['message']['content']\nprint(outfit)\n</pre> occasion = input(\"Enter the occasion\")                 #casual or formal (e.g. \"casual\") time_of_day = input(\"Enter the time of day\")           #morning, afternoon or evening (e.g. \"morning\") location = input(\"Enter the location\")                 #any location (e.g. \"park\") season = input(\"Enter the season\")                     #spring, summer, fall or winter (e.g. \"fall\")  prompt = f\"\"\"Use the description, category, and occasion of the clothes in my closet to put together an outfit for a {occasion} {time_of_day} at the {location}.                 The event takes place in the {season} season. Make sure to return only one shirt, bottoms, and shoes.                 Use the description, category, and occasion provided. Do not classify the items yourself.                  Include the file name of each image in your output along with the file extension. Here are the items in my closet: {closet}\"\"\"  messages = [         {             \"role\": \"control\",             \"content\": \"thinking\"         },         {             \"role\": \"user\",             \"content\": [                 {                     \"type\": \"text\",                     \"text\": f\"{prompt}\"                 }             ]         }     ]  outfit = reasoning_model.chat(messages=messages)['choices'][0]['message']['content'] print(outfit) <pre>Here is my thought process: \n- The outfit needs to be suitable for a casual morning at the park during fall. \n- I will select one shirt, one pair of pants, and one pair of shoes that fit the 'casual' occasion category. \n- I will avoid formal or overly dressy items and choose items that are comfortable for park activities.\n\nHere is my response:\n\nFor a casual morning at the park in fall, I suggest the following outfit:\n\n1. **Shirt**: A blue plaid shirt with a collar and long sleeves (file: 'image13.jpeg')\n   - The plaid pattern is classic for fall and goes well with casual park settings. The long sleeves offer some protection against cooler morning temperatures.\n\n2. **Pants**: Khaki pants with a buttoned waistband and a button closure at the front (file: 'image7.jpeg')\n   - Khaki is a versatile choice that can match the casual vibe and also provide a nice balance with the plaid shirt. It's practical and comfortable for walking around.\n\n3. **Shoes**: A pair of tan leather boots with a chunky sole and high-top design (file: 'image3.jpeg')\n   - Tan leather boots offer a stylish yet comfortable option. The chunky sole provides good grip and support, ideal for navigating park trails or uneven ground. \n\nThis combination provides a relaxed, put-together look suitable for a casual morning outing, while also considering comfort and practicality.\n</pre> <p>With this generated outfit description, we can also display the clothing items that the model recommends! To do so, we can simply extract the filenames. In case the model mentions the same filename twice, it is important to check whether the image has not already been displayed as we iterate the list of images. We can do so by storing displayed images in the <code>selected_items</code> list. Finally, we can display the selected items.</p> In\u00a0[\u00a0]: Copied! <pre>selected_items = []\n\n#extract the images of clothing that the model recommends\nfor item, uploaded_file in zip(closet, images):\n    if item['filename'].lower() in outfit.lower() and not any(key['filename'] == item['filename'] for key in selected_items):\n        selected_items.append({\n            'image': uploaded_file,\n            'category': item['category'],\n            'filename': item['filename']\n        })\n\n#display the selected clothing items\nif len(selected_items) &gt; 0:\n    for item in selected_items:\n        display(Image.open(directory + '/' + item['filename']))\n</pre> selected_items = []  #extract the images of clothing that the model recommends for item, uploaded_file in zip(closet, images):     if item['filename'].lower() in outfit.lower() and not any(key['filename'] == item['filename'] for key in selected_items):         selected_items.append({             'image': uploaded_file,             'category': item['category'],             'filename': item['filename']         })  #display the selected clothing items if len(selected_items) &gt; 0:     for item in selected_items:         display(Image.open(directory + '/' + item['filename']))"},{"location":"tutorials/projects/ai-stylist-tutorial/#build-an-ai-stylist-with-ibm-granite-using-watsonxai","title":"Build an AI stylist with IBM Granite using watsonx.ai\u00b6","text":"<p>Authors: Anna Gutowska, Ash Minhas</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#introduction","title":"Introduction\u00b6","text":"<p>How often do you find yourself thinking, \u201cWhat should I wear today? I don\u2019t even know where to start with picking items from my closet!\u201d This dilemma is one that many of us share. By using cutting-edge artificial intelligence (AI) models, this no longer needs to be a daunting task.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#ai-styling-how-it-works","title":"AI styling: How it works\u00b6","text":"<p>Our AI-driven solution is composed of the following stages:</p> <ol> <li><p>The user uploads images of their current wardrobe or even items in their wishlist, one item at a time.</p> </li> <li><p>The user selects the following criteria:</p> <ul> <li>Occasion: casual or formal.</li> <li>Time of day: morning, afternoon or evening.</li> <li>Season of the year: winter, spring, summer or fall.</li> <li>Location (for example, a coffee shop).</li> </ul> </li> <li><p>Upon submission of the input, the multimodal Granite Vision 3.2 model iterates over the list of images and returns the following output:</p> <ul> <li>Description of the item.</li> <li>Category: shirt, pants or shoes.</li> <li>Occasion: casual or formal.</li> </ul> </li> <li><p>The Granite 3.2 model with enhanced reasoning then serves as a fashion stylist. The LLM uses the Vision model\u2019s output to provide an outfit recommendation that is suitable for the user\u2019s event.</p> </li> <li><p>The outfit suggestion, a data frame of items that the user uploaded and the images in the described personalized recommendation are all returned to the user.</p> </li> </ol>"},{"location":"tutorials/projects/ai-stylist-tutorial/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You need an IBM Cloud\u00ae account to create a watsonx.ai\u2122 project.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#steps","title":"Steps\u00b6","text":"<p>In order to use the watsonx application programming interface (API), you will need to complete the following steps.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<ol> <li><p>Log in to watsonx.ai by using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> </ol>"},{"location":"tutorials/projects/ai-stylist-tutorial/#step-2-set-up-watsonxai-runtime-service-and-api-key","title":"Step 2. Set up watsonx.ai Runtime service and API key\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/projects/ai-stylist-tutorial/#step-3-clone-the-repository-optional","title":"Step 3. Clone the repository (optional)\u00b6","text":"<p>For a more interactive experience when using this AI tool, clone the GitHub repository and follow the setup instructions in the README.md file within the AI stylist project to launch the Streamlit application on your local machine. Otherwise, if you prefer to follow along step-by-step, create a Jupyter Notebook and continue with this tutorial.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#step-4-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 4. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this issue with a quick pip installation.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#step-5-set-up-the-api-request-for-the-granite-vision-model","title":"Step 5. Set up the API request for the Granite Vision model\u00b6","text":"<p>The <code>augment_api_request_body</code> function takes the user query and image as parameters and augments the body of the API request. We will use this function in each iteration of inferencing the Vision model.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#step-6-encode-images","title":"Step 6. Encode images\u00b6","text":"<p>To encode our images in a way that is digestible for the LLM, we will encode them to bytes that we then decode to UTF-8 representation. In this case, our images are located in the local <code>images</code> directory. You can find sample images in the AI stylist directory in our GitHub repository.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#step-7-categorize-input-with-the-vision-model","title":"Step 7. Categorize input with the Vision model\u00b6","text":"<p>Now that we have loaded and encoded our images, we can query the Vision model. Our prompt is specific to our desired output to limit the model's creativity as we seek valid JSON output. We will store the description, category and occasion of each image in a list called <code>closet</code>.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#step-8-generate-outfits-with-the-reasoning-model","title":"Step 8. Generate outfits with the reasoning model\u00b6","text":"<p>Now that we have each clothing and shoe item categorized, it will be much easier for the reasoning model to generate an outfit for the selected occasion. Let's instantiate and query the reasoning model.</p>"},{"location":"tutorials/projects/ai-stylist-tutorial/#conclusion","title":"Conclusion\u00b6","text":"<p>In this tutorial, you built a system that uses AI to provide style advice to a user's specific event. Using photos or screenshots of the user's clothing, outfits are customized to meet the specified criteria. The Granite-Vision-3-2-2b model was critical for labeling and categorizing each item. Additionally, the Granite-3-2-8B-instruct model leveraged its reasoning capabilities to generate personalized outfit ideas. Some next steps for building off this application can include:</p> <ul> <li>Customizing outfits to a user's personal style, body type, preferred color palette and more.</li> <li>Broadening the criteria to include jackets and accessories.<ul> <li>For example, the system might propose a blazer for a user attending a formal conference in addition to the selected shirt, pants and shoes.</li> </ul> </li> <li>Serving as a personal shopper by providing e-commerce product recommendations and pricing that align with the user's unique style and budget.</li> <li>Adding chatbot functionality to ask the LLM questions about each outfit.</li> <li>Providing a virtual try-on experience that uses a user selfie to simulate the final look.</li> </ul>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/","title":"Use crewAI to build multimodal multiagent systems for retail shelf optimization with watsonx.ai","text":"<p>Authors: Yohan Bensoussan, Anna Gutowska</p> <p>Imagine transforming your retail store\u2019s performance with just a smartphone and the power of artificial intelligence (AI). This isn\u2019t a far-off future scenario\u2014it\u2019s a reality that small and mid-size retail stores can implement today! In this tutorial, we\u2019ll explore an exciting real-world use case where store owners and managers can use AI to optimize their shelf arrangements, boost sales and enhance the customer experience. We will leverage the latest IBM Granite model in this project as well as Meta's Llama 3.2 vision model.</p> <p></p>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#the-impacts-of-generative-ai-on-retail","title":"The impacts of generative AI on retail","text":"<p>With the advent of agent-based generative AI, small and mid-size retail stores now have access to expert-level analysis and recommendations that were once the domain of large corporations with dedicated data science teams. This democratization of AI technology can be groundbreaking for your local grocery store, boutique shop or regional chain.</p> <p>Here\u2019s what makes this approach so revolutionary: * Simplicity: All you need to start is a simple picture of your store aisle. * Expertise on demand: The AI agents act as your personal team of retail experts, analyzing your space and current market trends. * Actionable insights: You\u2019ll receive a detailed, practical plan for rearranging your shelves to maximize sales and customer satisfaction. * Cost-effective: This approach eliminates the need for expensive consultants or complex software systems. * Adaptability: As market trends change, you can quickly re-analyze and adjust your store layout to stay ahead of the curve.</p> <p>Let\u2019s dive into the technical details and see how this AI-powered retail optimization works, step by step. By the end of this tutorial, you\u2019ll have a clear understanding of how to implement this system in your own store, potentially revolutionizing your retail space with the power of AI.</p>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#the-story-behind-the-solution","title":"The story behind the solution","text":"<p>Sarah is a local grocery store owner who was struggling to compete with larger chains. Despite her best efforts, she noticed that certain products weren't selling as well as they should, while others were constantly out of stock. One day, while reorganizing her fresh produce aisle for the third time that month, she wondered if there was a better way. That's where our AI-powered solution comes in. Using just her smartphone and our intelligent system, Sarah was able to transform her store's performance. Let's dive into how to build such a system.</p> <p></p>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#what-is-crewai","title":"What is crewAI?","text":"<p>We can use crewAI, an open-source agent framework that orchestrates agent interactions in crews. The term \"crew\" refers to multiagent systems. Our crew is a team of agents role-playing retail experts that are available 24 x 7, each with their own specialty. Tasks can either be directly assigned to an agent or handled through crewAI\u2019s hierarchical process that assesses specific roles and availability. For crewAI beginners, check out the crewAI explainer along with the official docs. On the official crewAI GitHub repository, you can also find examples of crews performing stock analysis, data analysis, RAG, LangGraph integration and much more.</p>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#meet-the-ai-retail-team","title":"Meet the AI retail team","text":"<p>Let's have a look at the team of agentic retail experts we will use in this tutorial.</p> <pre><code>store_manager:\n  role: Store Manager\n  goal: &gt;\n    Analyze the shelves in the physical store and provide a detailed report\n    to the market analyst to develop a detailed action plan with the insights.\n  backstory: &gt;\n    As the Space Planner, you are responsible for examining the store's shelves,\n    assessing product placement and optimizing space utilization.\n    You have access to advanced tools for shelf visualization, which help you\n    collect accurate data on the current arrangement of products.\n    You are capable to translate market analysis into a plan for the store\n    or specific shelf or department.\n\nmarket_analyst:\n  role: Market Trend Analyst\n  goal: &gt;\n    Provide recommendations to rearrange the product arrangement based on market trends.\n  backstory: &gt;\n    As the Market Analyst, you possess in-depth knowledge of market trends and consumer behavior.\n    Your experience and keen sense of retail enable you to propose effective recommendations\n    for specific shelves. You analyze reports provided by the Space Planner to suggest\n    improvements that enhance sales and customer experience.\n</code></pre>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#task-workflow","title":"Task workflow","text":"<p>Here's how our worker team automates the complex task of shelf improvement.</p> <pre><code>analyze_shelf:\n  description: &gt;\n    Use the Vision Tool to collect visual data and caption the current product arrangement.\n    Conduct a thorough analysis of shelf {shelf} in the store.\n    Prepare a detailed report highlighting the current layout, products,\n    product placement and any observed issues.\n    Ensure the report is detailed at the level of product names.\n  expected_output: &gt;\n    A comprehensive report on shelf {shelf}, including visual data,\n    analysis of product placement, space utilization and any recommendations for improvement.\n  agent: store_manager\n\nprovide_recommendations:\n  description: &gt;\n    Review the report on shelf {shelf} provided by the Store Manager.\n    Utilize your knowledge of the retail market and internet to assess current trends\n    relevant to the products in this shelf ({shelf}).\n    Develop expert recommendations to optimize sales and customer satisfaction.\n    Ensure the recommendations are detailed and includes details like product names.\n  expected_output: &gt;\n    A set of actionable recommendations for rearranging the {shelf} shelf,\n    aligned with current market trends and consumer preferences.\n  agent: market_analyst\n\ncreate_action_plan:\n  description: &gt;\n    List the recommendations from the Market Analyst,\n    then develop a detailed action plan for Store manager and Store buyer\n    to implement the changes.\n    Ensure the plan is practical and outlines the steps needed to rearrange\n    the products effectively.\n    Be smart and well explained.\n    Give the explanation of your recommendations and the goal to achieve.\n  expected_output: &gt;\n    A detailed list of recommendation and action plan for rearranging and\n    improving the {shelf} shelf according to market trends,\n    including market analyst recommendations and translation into\n    practical tasks for the Store manager and the Buyer.\n  agent: store_manager\n</code></pre>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#steps","title":"Steps","text":"<p>You can find this project on Github.</p>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#step-1-set-up-your-environment","title":"Step 1. Set up your environment","text":"<p>We first need to set up our environment. You can find these steps in the Markdown file on GitHub or by following along here. </p> <ol> <li> <p>Ensure you have Python &gt;=3.10 &lt;=3.13 installed on your system. You can check your Python version using the <code>python3 --version</code> command.</p> </li> <li> <p>Set up a virtual environment to avoid Python package dependency conflicts.</p> </li> </ol> <pre><code>python3 -m venv myenv\nsource ./myenv/bin/activate\n</code></pre> <ol> <li>Clone the repository found here: <code>https://github.com/IBM/ibmdotcom-tutorials.git</code>. For detailed steps on how to clone a repository, refer to the GitHub documentation. </li> </ol> <p>The project structure of the <code>crew-ai-projects</code> directory should resemble the following:</p> <pre><code>src/my_retail_advisor/\n\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 agents.yaml    # Agent configurations\n\u2502   \u2514\u2500\u2500 tasks.yaml     # Task definitions\n\u251c\u2500\u2500 tool/\n\u2502   \u251c\u2500\u2500 custom_tool.py # Custom crewAI tool implementations\n\u2502   \u2514\u2500\u2500 tool_helper.py # Vision helper functions\n\u251c\u2500\u2500 crew.py           # Crew orchestration\n\u2514\u2500\u2500 main.py          # Application entry point\n</code></pre>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#step-2-obtain-watsonx-api-credentials","title":"Step 2. Obtain watsonx API credentials","text":"<ol> <li> <p>Log in to watsonx.ai using your IBM Cloud account.</p> </li> <li> <p>Create a watsonx.ai project.    Take note of your project ID in project &gt; Manage &gt; General &gt; Project ID. You'll need this ID for this tutorial.</p> </li> <li> <p>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> </li> <li> <p>Generate a watsonx API Key. </p> </li> <li> <p>Associate the watsonx.ai Runtime service to the project you created in watsonx.ai. </p> </li> </ol>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#step-3-obtain-serper-api-credentials","title":"Step 3. Obtain Serper API credentials","text":"<p>Generate and take note of your free Serper API key. Serper is a Google Search API that we will be using in this project. </p>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#step-4-install-crewai-and-set-up-your-credentials","title":"Step 4. Install crewAI and set up your credentials","text":"<p>We need to install the crewAI framework for this tutorial and set out watsonx.ai credentials that we generated in step 2. </p> <p>To install crewAI, run the following command in your terminal. </p> <pre><code>%pip install 'crewai[tools]'\n</code></pre> <p>In a seperate <code>.env</code> file at the same directory level as the <code>.env_sample</code> file, set your credentials as strings like so: </p> <pre><code>WATSONX_APIKEY=your_watson_api_key_here\nWATSONX_PROJECT_ID=your_watsonx_project_id_here\nWATSONX_URL=your_endpoint (e.g. \"https://us-south.ml.cloud.ibm.com\")\nSERPER_API_KEY=your_serper_api_key_here\n</code></pre>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#step-5-customize-the-crew-optional","title":"Step 5. Customize the crew (optional)","text":"<p>crewAI can be configured to use any open source large language model (LLM). LLMs can be connected through Ollama and several other APIs such as IBM watsonx\u2122 and OpenAI. Users can also leverage pre-built tools available through the crewAI Toolkit as well as LangChain Tools. </p> <p>For this tutorial, our custom visual search tool is powered by the <code>llama-3-2-90b-vision-instruct</code> model using watsonx.ai. Here is a look at the custom vision tool in the <code>tool</code> directory.</p> <pre><code># tool/custom_tool.py\nfrom crewai.tools import BaseTool\nfrom my_retail_advisor.tools.tool_helper import Helper\n\nclass VisionTool(BaseTool):\n    name: str = \"Vision Tool\"\n    description: str = \"Analyzes a default picture to collect visual data.\"\n\n    def _run(self) -&gt; str:\n        # Relative path to the shelf.jpg image from the working crew-ai/my_retail_advisor directory\n        image_path = 'images/shelf.jpg'\n\n        # Simulating image-to-text conversion\n        products_in_image = Helper.image2text(image_path)\n        return products_in_image\n</code></pre> <p>There are many ways you can customize your crew: - Modify <code>src/my_retail_advisor/config/agents.yaml</code> to define your agents. - Modify <code>src/my_retail_advisor/config/tasks.yaml</code> to define your tasks. - Modify <code>src/my_retail_advisor/crew.py</code> to add your own logic, tools and specific args. - Modify <code>src/my_retail_advisor/main.py</code> to add custom inputs for your agents and tasks. - Modify <code>src/my_retail_advisor/tool/custom_tool.py</code> to add custom tools for your agents and tasks. - Modify <code>src/my_retail_advisor/tool/tool_helper.py</code> to change the custom vision tool based on llama vision model. - Replace <code>images/shelf.jpg</code> with an image of your own.</p>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#step-6-run-the-system","title":"Step 6. Run the system","text":"<p>Ensure you are in the proper working directory of this project. You can change directories by running the following command in your terminal.</p> <pre><code>cd crew-ai-projects/my_retail_advisor\n</code></pre> <p>To kickoff your crew of AI agents and begin task execution, run this command from the root folder of your project. Note, the crew may run for several minutes before returning a final result.</p> <pre><code>crewai run\n</code></pre> <p>This command initializes the my-retail-advisor Crew, assembling the agents and assigning them tasks as defined in your configuration. This example, unmodified, will run Granite on watsonx.ai to create a <code>report.md</code> file with the output. crewAI can return JSON, Pydantic models and raw strings as output. Here is an example of the output produced by the crew.</p>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#example-output","title":"Example output:","text":"<p>Action Plan for Rearranging and Improving the Vegetable Shelf</p> <p>Objective: To create a visually appealing and organized produce section that showcases the most popular vegetables, increases sales, and enhances customer satisfaction.</p> <p>Recommendations from the Market Analyst:</p> <ol> <li>Create a focal point with a colorful and eye-catching display of the most popular vegetables.</li> <li>Use a variety of colors, textures, and heights to create visual interest.</li> <li>Group similar vegetables together.</li> <li>Consider using baskets, bins, or other containers to add a natural and earthy feel.</li> <li>Use lighting to highlight specific products or promotions. ...  </li> </ol> <p>Action Plan for Store Manager and Store Buyer:</p> <p>Step 1: Focal Point Display Store Manager: Designate a focal point area on the shelf for the most popular vegetables (tomatoes, potatoes, onions, lettuce, carrots, peppers, cucumbers, and celery). Store Buyer: Ensure a sufficient stock of these vegetables to maintain a full and appealing display. Team: Arrange the vegetables in a visually appealing way, using a variety of colors, textures, and heights.</p> <p>...  </p> <p>Step 2: Themed Displays and Additional Features Store Manager: Plan and implement themed displays (e.g., summer BBQ or holiday-themed) to engage customers and promote related products. Store Buyer: Ensure a sufficient stock of themed display materials and related products. Team: Use vegetable racks or stands to add extra display space and create a visually appealing display.</p> <p>...</p>"},{"location":"tutorials/projects/crewAI-multiagent-retail-example/#conclusion","title":"Conclusion","text":"<p>As seen in the sample output, the multiagent system is able to execute the sequential process of processing input, tool calling and formulating findings.</p> <p>Remember Sarah's fresh produce aisle problem? Here's how the system helped her:</p> <ol> <li>Visual analysis: Sarah took a photo of her vegetable aisle with her smartphone. The store manager agent analyzed the image and noticed that the shelves are densely packed and in need of maintance.</li> <li>Market research: The market analyst agent used the current market trends for vegetable sales and customer preferences to create actionable recommendations for rearranging the vegetables shelf.</li> <li>Action plan: Based on these insights, Sarah received a detailed plan including instructions to:<ul> <li>Create a colorful and eye-catching display</li> <li>Use baskets or bins to add a natural feel to the display.</li> <li>Add a themed display, such as a summer BBQ or holiday-themed display, to engage customers in related products.</li> <li>Create space between the different types of vegetables to declutter.</li> <li>Improve the lighting. </li> </ul> </li> </ol> <p>In summary, the <code>my-retail-advisor</code> Crew is composed of multiple AI agents, each with unique roles, goals and tools. These agents collaborate on a series of tasks, defined in <code>config/tasks.yaml</code>, leveraging their collective skills to achieve complex objectives. The <code>config/agents.yaml</code> file outlines the capabilities and configurations of each agent in your crew.</p> <p>With AI-powered tools like this, small and medium-sized retailers can streamline their decision-making process. Just like Sarah, you can transform your store's performance with technology that's accessible, affordable and effective. This architecture also unlocks other AI opportunities across various domains, such as product design and enhanced customer experience. Its flexibility makes it valuable beyond retail, empowering businesses to innovate and excel in industry-specific tasks.</p>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/","title":"Multi-Agent Collaboration for Customer Call Analysis using Watsonx.ai and CrewAI","text":"<p>Authors: Vanna Winland, Meredith Syed</p> <p>In this tutorial, we will demonstrate how a team of multiple artificial intelligence (AI) agents can collaborate to complete complex tasks and optimize workflows. We built a python application to explain the orchestration of specialized agents working within a multiagent architecture. By the end, you will have seen and run an example of multiagent collaboration within an agentic AI application.</p> <p>The application we\u2019ll be working with is a Customer Service Analysis crew using CrewAI as the multiagent framework and Watsonx.ai to deploy the large language model (LLM) that powers it.</p> <p>AI agents are LLM-based entities that can perform operations on behalf of a user or agentic AI system. Agentic architectures are structured around two different systems: single and multiagent.</p> <p>Single agent systems are best for solving narrow problems because they depend on one LLM agent to perform generative AI tasks. For example, a single chatbot agent can be focused on specific tasks or conversations that it can complete within the scope of its individual capabilities.</p> <p>Multiagent systems (MAS) are frameworks that orchestrate the functionality and interactions between AI agents. Rather than attempting to encompass all capabilities within a single agent, multiagent architectures use different agents to work within the same environment to achieve a shared goal. Key benefits of multiagent systems include agent collaboration and adaptability to solve problems beyond the capabilities of a single agent. The best approach depends on the complexity of the machine learning tasks required to compile a solution or achieve a certain result.</p>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#problem-solving-with-multiagent-systems","title":"Problem-solving with multiagent systems","text":"<p>crewAI is an opensource agentic framework that orchestrates LLM agent automation by assembling customizable crews, or teams of role-playing agents. We applied a simplified industry use case to explain how agents collaborate within a multiagent architecture.</p> <p>Imagine a real-world use case for a customer service call center. Telecommunications software to analyze call center transcripts is used to enhance customer experiences and evaluate call quality. In more robust software, transcripts may even be analyzed in real-time along with large datasets including call metadata. For the sake of explainability, our application\u2019s dataset is simple, a mock transcript between a customer service representative and customer.</p> <pre><code># multiagent-collaboration-cs-call-center-analysis/data/transcript.txt\n\nCustomer Service Interaction Transcript\n\nCynthia:\nHi, I'm calling because I received a jar of peanut butter that was open and it's\ncompletely spilled everywhere. This is really frustrating, and I need a replacement.\n\nGerald (Peanut Butter Inc.):\nUgh, that sucks. But, like, how did you not notice it was open before\nyou bought it?\n\nCynthia:\nExcuse me? I didn't expect the jar to be open when I received it. It was sealed\nwhen I bought it. Can you just help me out here?\n\nGerald:\nYeah, whatever. But we can't control how it gets to you. I mean, it's not like\nwe throw the jars around or anything. You're probably being dramatic.\n\nCynthia:\nI'm not being dramatic. The peanut butter is literally all over the box and\nit's a mess. I just want a replacement or a refund, that's all.\n\nGerald:\nLook, I guess I could send you a replacement, but it's really not our fault, you\nknow? Maybe next time, check the jar before you open it?\n\nCynthia:\nAre you seriously blaming me for your company's mistake? That's not how customer\nservice works!\n\nGerald:\nWell, what do you want me to do? I don't exactly have magic powers to fix your\nproblem instantly. Chill out, we'll send you a new jar eventually.\n\nCynthia:\nThat's not good enough! I expect better from a company that I've been buying\nfrom for years. Can you just do the right thing and make this right?\n\nGerald:\nFine, fine. I'll put in a request or whatever. But seriously, this kind of thing\nhappens. Don't make it sound like the end of the world.\n\nCynthia:\nUnbelievable. I'll be posting a review if this isn't fixed immediately.\n\nGerald:\nCool, go ahead. I'm sure we'll survive your review.\n\nCynthia:\nI'll be contacting your supervisor if this isn't resolved soon.\n\nGerald:\nYeah, okay. Do what you gotta do.\n</code></pre> <p>A team of collaborative agents will generate a comprehensive report based on text analysis and customer call center evaluation metrics to help customer service managers summarize the main events of the call, evaluate performance and provide recommendations for improvement.</p>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#the-customer-service-call-analysis-crew","title":"The Customer Service Call Analysis Crew","text":"<p>The customer service call analysis crew consists of three agents with specialized roles and predefined goals. The agent configuration includes a Transcript Analyzer, a Quality Assurance Specialist, and a Report Generator. The agent\u2019s goals and characteristics are defined by three main attributes, role, goal, and backstory.</p> <pre><code>transcript_analyzer:\n  role: &gt;\n    Transcript Analyzer\n  goal: &gt;\n    Analyze the provided transcripts and extract key insights and themes.\n  backstory: &gt;\n    As the Transcript Analyzer, you are responsible for reviewing customer\n    service call transcripts, identifying important information, and summarizing\n    findings into a report to pass on to the Quality Assurance Specialist. \n    You have access to advanced text analysis tools that help you process and\n    interpret the data effectively.\n\nquality_assurance_specialist:\n  role: &gt;\n    Quality Assurance Specialist\n  goal: &gt;\n    Evaluate the quality of the customer service based the Transcript Analyzer's\n    report, call center evaluation metrics, and business standards. Flag any \n    transcripts with escalation risks as high priority.\n  backstory: &gt;\n    As the Quality Assurance Specialist, you are tasked with assessing the\n    quality of customer service interactions based on the Transcript Analyzer's\n    report, call center evaluation metrics, and industry standards used in call\n    centers. You review transcripts, evaluate agent performance, and provide\n    feedback to improve overall service quality.\n\nreport_generator:\n  role: &gt;\n    Report Generator\n  goal: &gt;\n    Generate reports based on the insights and findings from the transcript\n    analysis and quality assurance specialist.\n  backstory: &gt;\n    As the Report Generator, you compile the key insights and findings from the\n    transcript analysis and quality assurance specialist into a comprehensive\n    report. You create an organized report that includes summaries and recommendations\n    based on the data to help customer service managers understand the trends\n    and patterns in customer interactions.\n</code></pre> <p>The Transcript Analyzer agent performs a thorough analysis on the transcript to extract key insights, important information, and summarize findings into a report that is passed along to the other agents to aid in their tasks. This agent uses a suite of customized tools to perform natural language processing (NLP) techniques like keyword extraction and sentiment analysis.</p> <p>The Quality Assurance Specialist agent evaluates the quality of the call based on the key insights from the Transcript Analyzer\u2019s report and its own described expertise implementing and evaluating call center evaluation metrics. This agent can also search the internet to retrieve relevant metrics and processes to evaluate the employee\u2019s performance and provide feedback to improve overall service quality.</p> <p>The Report Generator agent generates a report based on the insights in the transcript analysis report and the metrics and feedback provided by the quality assurance evaluation. The agent specializes in organizing the data into a comprehensive report. The goal of the report is to provide customer service managers with a breakdown of the key insights from the call and recommendations to improve customer service quality.</p>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#agent-tools","title":"Agent tools","text":"<p>Each agent has access to tools, skills or functions that agents use to perform different tasks. crewAI offers existing tools, integration with LangChain tools, and the option to build your own custom tools. The Customer Service Analysis crew uses a combination, with each tool specified for the agent\u2019s task and the application\u2019s goal. Each agent has certain permissions for what tooling they can access in their configuration.</p> <p>Custom tools are created by defining a clear description for what the tool will be used for. For example, the Transcript Analyzer agent has several custom tools for text analysis.</p> <pre><code>\n# src/customer_service_analyzer/tools/custom_tool.py\n\nclass SentimentAnalysisTool(BaseTool):\n    name: str = \"Sentiment Analysis Tool\"\n    description: str = \"Determines the sentiment of the interactions in the transcripts.\"\n\n    def _run(self, transcript: str) -&gt; str:\n        # Simulating sentiment analysis\n        sentiment = Helper.analyze_sentiment(transcript)\n        return sentiment\n\n</code></pre> <p>The description of the tool is what the agent uses as logic to perform sentiment analysis on the transcript.</p> <p>Agents can also use existing tools and integrated APIs. The Quality Assurance Specialist agent has access to a <code>search_tool</code> that uses the <code>SerperDevTool</code>to search the internet and return the most relevant results to its inquiries. The agent can utilize its specialized role as an experienced customer service evaluator, but also leverage the internet to search for the necessary metrics to evaluate the call and use in its report.</p>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#task-workflow","title":"Task workflow","text":"<p>The tasks are the specific assignments completed by the agents with execution details facilitated by three required tasks attributes: description, agent, expected output. The agents perform their tasks in a logical sequence using the detailed descriptions for each task as a guide.</p> <pre><code>transcript_analysis:\n  description: &gt;\n    Use the Text Analysis Tool to collect key information and insights to better\n    understand customer service interactions and improve service quality. \n    Conduct a thorough analysis of the call {transcript}.\n    Prepare a detailed report highlighting key insights, themes, and sentiment\n    from the transcripts.\n    Identify any escalation risks and flag them for the Quality Assurance Specialist.\n    Use the sentiment analysis tool to determine the overall sentiment of the\n    customer and the agent.\n    Use the keyword extraction tool to identify key keywords and phrases in the transcript.\n  expected_output: &gt;\n    A detailed analysis report of the {transcript} highlighting key insights,\n    themes, and sentiment from the transcripts.\n  agent: transcript_analyzer\n\nquality_evaluation:\n  description: &gt;\n    Review the transcript analysis report on {transcript} from the Transcript Analyzer.\n    Utilize your expertise in customer service evaluation metrics and industry\n    standards, and internet to evaluate the quality of the customer service interaction.\n    Score the interaction based on the evaluation metrics and flag any high-risk\n    escalations. Develop expert recommendations to optimize customer service\n    quality. Ensure the report includes customer service metrics and feedback\n    for improvement.\n  expected_output: &gt;\n    A detailed quality evaluation report of the {transcript} highlighting the\n    quality of the customer service interaction, scoring based on evaluation\n    metrics, flagging any high-risk escalations, and recommendations for improvement.\n  agent: quality_assurance_specialist\n\nreport_generation:\n  description: &gt;\n    List the reports from the Transcript Analyzer and the Quality Assurance\n    Specialist, then develop a detailed action plan for customer service managers\n    to implement the changes.\n    Use the data from these agents output to create an organized report including\n    a summarization and actionable recommendations for call center managers.\n    Ensure the report includes keywords and sentiment analysis from the Transcript\n    Analyzer agent.\n    Ensure the report includes the Quality Assurance Specialist agent's report,\n    evaluation metrics and recommendations for improving customer service quality.\n    Ensure the report is well written and easy to understand.\n    Be smart and well explained.\n    Ensure the report is comprehensive, organized, and easy to understand with\n    labeled sections with relevant information.\n  expected_output: &gt;\n    A comprehensive report that lists the reports from the Transcript Analyzer,\n    then the Quality Assurance Specialist. \n    The report should include the key insights from {transcript} and the quality\n    evaluation report from the Quality Assurance Specialist.\n    The report should include organized sections for each agent's findings,\n    summaries, and actionable recommendations for call center managers.\n  agent: report_generator\n  context:\n    - transcript_analysis\n    - quality_evaluation\n</code></pre> <p>The task workflow is executed in a sequential process starting with the transcript analysis completed by the Transcript Analyzer. The results of one task can establish context for a future task. During the next sequence, the Quality Assurance Specialist leverages the transcript analysis report to inform its quality evaluation, noting any keywords or phrases that indicate escalation.</p> <p>The Report Generator agent uses the outputs of the Transcript Analyzer and the Quality Assurance Specialist agents as context to generate a comprehensive report about the call transcript. This is an example of multiagent collaboration and how agents can complete complex tasks and generate more robust outputs with increased context awareness while performing their specialized roles.</p>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#steps","title":"Steps","text":""},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#step-1-set-up-your-environment","title":"Step 1. Set up your environment","text":"<p>First, we need to set up our environment to run the application. You can find these steps in the markdown file within the crewAI project folder on GitHub or by following along here.</p> <ul> <li> <p>Ensure you have Python &gt;=3.10 &lt;=3.13 installed on your system. You can check   your Python version using the <code>python3 \u2013version</code> command.</p> </li> <li> <p>Clone the GitHub repository found here.   For detailed steps on how to clone a repository, refer to the GitHub documentation.</p> </li> </ul> <p>The project structure should resemble the following:</p> <pre><code>src/customer_service_analyzer/\n\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 agents.yaml    # Agent configurations\n\u2502   \u2514\u2500\u2500 tasks.yaml     # Task definitions\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 custom_tool.py # Custom crewAI tool implementations\n\u2502   \u2514\u2500\u2500 tool_helper.py # Custom tool helper functions\n\u251c\u2500\u2500 crew.py           # Crew orchestration\n\u2514\u2500\u2500 main.py          # Application entry point\n</code></pre>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#step-2-obtain-watsonx-api-credentials","title":"Step 2. Obtain watsonx API credentials","text":"<ol> <li> <p>Log in to\u00a0watsonx.ai    using your IBM Cloud account.</p> </li> <li> <p>Create a\u00a0watsonx.ai project.    Take note of your\u00a0project ID\u00a0in project &gt; Manage &gt; General &gt; Project ID.    You'll need this ID for this tutorial.</p> </li> <li> <p>Create a\u00a0watsonx.ai Runtime    service instance (choose the Lite plan, which is a free instance).</p> </li> <li> <p>Generate a watsonx\u00a0API Key.</p> </li> <li> <p>Associate the watsonx.ai Runtime service to the project you created in\u00a0watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#step-3-obtain-serper-api-credentials","title":"Step 3. Obtain Serper API credentials","text":"<p>Generate and take note of your free\u00a0Serper API\u00a0key. Serper is a Google Search API that we will be using in this project.</p>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#step-4-install-crewai-and-set-up-your-credentials","title":"Step 4. Install crewAI and set up your credentials","text":"<p>We need to install the crewAI framework for this tutorial and set up watsonx.ai credentials that we generated in step 2.</p> <p>If you use <code>uv</code> for package management, you can add crewAI as follows:</p> <pre><code>\nuv tool install crewai\n\n</code></pre> <p>If using pip for package management, set up a virtual environment and then install crewAI in that environment.</p> <pre><code>\npython3 -m venv venv\nsource ./venv/bin/activate\n\n</code></pre> <p>To install crewAI, run the following command in your terminal.</p> <pre><code>\npip install 'crewai[tools]'\n\n</code></pre> <p>In a separate\u00a0.env\u00a0file at the same directory level as the\u00a0.env_sample\u00a0file, set your credentials as strings like so:</p> <pre><code>\nWATSONX_APIKEY=your_watson_api_key_here\n\nWATSONX_PROJECT_ID=your_watsonx_project_id_here\n\nWATSONX_URL=your_endpoint (e.g. \"&lt;https://us-south.ml.cloud.ibm.com&gt;\")\n\nSERPER_API_KEY=your_serper_api_key_here\n\n</code></pre>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#step-5-optional-customize-the-crew","title":"Step 5. (Optional) Customize the crew","text":"<p>crewAI can be configured to use any open source LLM. LLMs can be connected through Ollama and several other APIs such as IBM watsonx\u2122 and OpenAI. Users can also leverage pre-built tools available through the crewAI Toolkit as well as LangChain Tools.</p>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#step-6-run-the-system","title":"Step 6. Run the system","text":"<p>Ensure you are in the proper working directory of this project. You can change directories by running the following command in your terminal.</p> <pre><code>\ncd crew-ai-projects/multiagent-collab-cs-call-center-analysis\n\n</code></pre> <p>To kickoff your crew of AI agents and begin task execution, run this command from the root folder of your project. Note, the crew may run for several minutes before returning a result.</p> <pre><code>\ncrewai run\n\n</code></pre> <p>This command initializes the Call Center Analysis Crew, assembling the agents and assigning them tasks as defined in your configuration. This example, unmodified, will run a Llama model on watsonx.ai to create a\u00a0report.md\u00a0file with the output. crewAI can return JSON, Pydantic models and raw strings as output. Here is an example of the output produced by the crew.</p>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#example-output","title":"Example Output","text":"<p>This is an example of the final output after running the crew:</p> <pre><code>**Detailed Analysis Report of the Customer Service Interaction Transcript**\n\n**Transcript Analysis Report**\n\nThe customer, Cynthia, called to report a damaged product, a jar of peanut butter that was open and spilled everywhere. She requested a replacement, but the agent, Gerald, responded defensively and blamed her for not noticing the damage before purchasing. The conversation escalated, with Cynthia becoming frustrated and threatening to post a negative review and contact the supervisor.\n\n**Key Insights and Themes**\n\n- The customer was dissatisfied with the product and the agent's response.\n- The agent was unhelpful, unprofessional, and failed to take responsibility for the company's mistake.\n- The conversation was confrontational, with both parties becoming increasingly agitated.\n- The customer felt disrespected and unvalued, while the agent seemed dismissive and uncaring.\n\n**Sentiment Analysis**\n\n- Customer Sentiment: Frustrated, Angry, Disappointed\n- Agent Sentiment: Defensive, Dismissive, Uncaring\n\n**Keyword Extraction**\n\n- Damaged Product\n- Unhelpful Agent\n- Confrontational Conversation\n- Customer Dissatisfaction\n- Unprofessional Response\n\n**Escalation Risks**\n\n- Negative Review: The customer threatened to post a negative review if the issue was not resolved promptly.\n- Supervisor Involvement: The customer may contact the supervisor to report the incident and request further action.\n\n**Recommendations for Quality Assurance Specialist**\n\n- Review the call recording to assess the agent's performance and provide feedback on areas for improvement, using customer service metrics.\n- Investigate the root cause of the damaged product and implement measures to prevent similar incidents in the future.\n- Provide training on customer service skills, including active listening, empathy, and conflict resolution, using customer service standards.\n- Monitor the customer's feedback and respond promptly to any concerns or complaints to maintain a positive customer experience.\n- Recognize the standards for various customer service metrics to measure key performance indicators that are related to the areas mentioned above.\n\n**Summary of Quality Evaluation Report**\n\nThe customer, Cynthia, called to report a damaged product, a jar of peanut butter that was open and spilled everywhere. She requested a replacement, but the agent, Gerald, responded defensively and blamed her for not noticing the damage before purchasing. Evaluation metrics showed a low Customer Satisfaction Score (CSAT), high Customer Effort Score (CES), and negative Net Promoter Score (NPS).\n\n**Recommendations for Call Center Managers**\n\n- Review the call recording, investigate the root cause of the damaged product, and provide training on customer service skills. Recognize the standards for various customer service metrics to measure key performance indicators.\n- Monitor the customer's feedback and respond promptly to any concerns or complaints to maintain a positive customer experience.\n- Implement measures to prevent similar incidents in the future, such as improving product packaging and handling procedures.\n- Provide feedback and coaching to agents on their performance, highlighting areas for improvement and recognizing good performance.\n</code></pre>"},{"location":"tutorials/projects/multiagent-collaboration-customer-call-analysis/#conclusion","title":"Conclusion","text":"<p>As demonstrated in the sample output, the agents worked together to complete the complex task of analyzing, evaluating, and generating a report about the example transcript. The collaboration between agents enhanced the application\u2019s efficiency and accuracy by orchestrating each agent to specialize in a particular aspect of the process. The report agent, for instance, generated an organized report that includes the findings from the textual analysis and evaluation tasks. This reflects a smooth coordination between agents in handling different parts of the workflow.</p> <p>Multiagent frameworks can provide a more robust and improved overall performance via agent collaboration. Not every multiagent architecture works the same way, for instance some are specific for software development, while others like crewAI and AutoGen offer more composable configurations.</p>"},{"location":"tutorials/projects/react-agent-tutorial/","title":"Deploy an IT support LangGraph ReAct agent with IBM Granite on watsonx.ai","text":"<p>Author: Anna Gutowska </p> <p>In this tutorial, you will build a ReAct (Reasoning and Action) AI agent with the open-source LangGraph framework by using an IBM Granite model through the IBM\u00ae watsonx.ai\u00ae API in Python. The use case is to manage existing IT support tickets and to create new ones.</p>"},{"location":"tutorials/projects/react-agent-tutorial/#what-is-a-react-agent","title":"What is a ReAct agent?","text":"<p>An artificial intelligence (AI) agent refers to a system or program that is capable of autonomously performing tasks on behalf of a user or another system by designing its agent workflow and using available tools. Generative AI agents use the advanced natural language processing (NLP) techniques of large language models (LLMs) to comprehend and respond to user inputs step-by-step and determine when to call on external tools. A core component of AI agents is reasoning. Upon acquiring new information through tool calling, human intervention or other agents, the reasoning paradigm guides the agent\u2019s next steps. </p> <p>With each action and each tool response, the ReAct (Reasoning and Action) paradigm instructs agents to \"think\" and plan their next steps. This step-by-step, slow reasoning, gives us insight into how the agent uses updated context to formulate conclusions. Because this process of reflection is continuous, it is often referred to as a think-act-observe loop and is a form of chain-of-thought prompting.</p>"},{"location":"tutorials/projects/react-agent-tutorial/#using-langgraph-to-build-react-agents","title":"Using LangGraph to build ReAct agents","text":"<p>This tutorial will use the LangGraph framework, an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. The prebuilt <code>create_react_agent</code> function provided by LangGraph is an easy way to build a simple, custom agent. Simple ReAct agents, like the one in this tutorial as depicted in Figure 1, are composed of two nodes. One node is responsible for calling the model and the other node is for using tools. Common tools include the prebuilt LangChain Wikipedia tool, DuckDuckGoSearchRun tool and even retrieval-augmented generation (RAG). In cases with complex action input, another node can be added, as seen in Figure 2. This additional node would serve the purpose of helping ensure the agent returns structured output. </p> <p></p> <p>Within LangGraph, the \u201cstate\u201d feature serves as a memory bank that records and tracks all the valuable information processed by each iteration of the AI system. These stateful graphs allow agent's to recall past information and valuable context. The cyclic structure of the ReAct graph is leveraged when the outcome of one step depends on previous steps in the loop. The nodes, or \"actors,\" in the graph encode agent logic and are connected by edges. Edges are essentially Python functions that determine the next node to execute depending on the current state.</p>"},{"location":"tutorials/projects/react-agent-tutorial/#steps","title":"Steps","text":""},{"location":"tutorials/projects/react-agent-tutorial/#step-1-generate-your-watsonxai-credentials","title":"Step 1. Generate your watsonx.ai credentials","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook. </p> <ol> <li> <p>Log in to watsonx.ai using your IBM Cloud\u00ae account.</p> </li> <li> <p>Create a watsonx.ai Runtime service instance (select your appropriate region and choose the Lite plan, which is a free instance).</p> </li> <li> <p>Generate an application programming interface (API) key. </p> </li> </ol>"},{"location":"tutorials/projects/react-agent-tutorial/#step-2-set-up-the-project-in-your-ide","title":"Step 2. Set up the project in your IDE","text":"<p>To easily get started with deploying agents on watsonx.ai, clone this GitHub repository and access the IT support ReAct agent project. You can run the following command in your terminal to do so.</p> <pre><code>git clone git@github.com:IBM/ibmdotcom-tutorials.git\ncd react-agent-langgraph-it-support/base/langgraph-react-agent/\n</code></pre> <p>Next, install poetry if you do not already have it installed. Poetry is a tool for manageing Python dependencies and packaging.</p> <pre><code>pipx install --python 3.11 poetry\n</code></pre> <p>Then, activate your virtual environment.</p> <pre><code>source $(poetry -q env use 3.11 &amp;&amp; poetry env info --path)/bin/activate\n</code></pre> <p>Rather than using the <code>pip install</code> command, the poetry package allows us to add dependencies by running the following command in the terminal. This step installs the repository dependencies reflected in the <code>pyproject.toml</code> file to your separate virtual environment.</p> <pre><code>poetry install\n</code></pre> <p>Adding a working directory to PYTHONPATH is necessary for the next steps. In your terminal execute:  </p> <pre><code>export PYTHONPATH=$(pwd):${PYTHONPATH}\n</code></pre> <p>To set up your environment, follow along with the instructions in the README.md file on Github. This set up requires several commands to be run on your IDE or command line.</p>"},{"location":"tutorials/projects/react-agent-tutorial/#step-3-set-the-environment-variables","title":"Step 3. Set the environment variables","text":"<p>In the <code>config.toml</code> file, you will find the following blank credentials that must be filled in before attempting to deploy your agent. Your <code>watsonx_apikey</code> and <code>watsonx_url</code> were initialized in step 1 of this tutorial. Next, follow along with the simple form found on the Developer Access page to select your deployment space or create a new one. There, you can retrieve your <code>space_id</code> needed to connect our agent to the watsonx.ai deployment. Lastly, your <code>model_id</code> is set to the IBM Granite 3.2 model. </p> <pre><code>[deployment]\n  watsonx_apikey = \"\"\n  watsonx_url = \"\"  # should follow the format: `https://{REGION}.ml.cloud.ibm.com`\n  space_id = \"\" # found in the \"Manage\" tab of your Deployment or in the Developer Access page here: https://dataplatform.cloud.ibm.com/developer-access\n\n[deployment.custom]\n# during creation of deployment additional parameters can be provided inside `CUSTOM` object for further referencing\n# please refer to the API docs: https://cloud.ibm.com/apidocs/machine-learning-cp#deployments-create\n  model_id = \"ibm/granite-3-2-8b-instruct\"\n  thread_id = \"thread-1\" # More info here: https://langchain-ai.github.io/langgraph/how-tos/persistence/\n</code></pre>"},{"location":"tutorials/projects/react-agent-tutorial/#step-4-upload-your-data-to-ibm-cloud-object-storage","title":"Step 4. Upload your data to IBM Cloud Object Storage","text":"<p>Our agent requires a data source to provide up-to-date information and add new data. We will store our data file in IBM Cloud\u00ae Object Storage. </p> <ol> <li>First, log in to IBM Cloud. Then, create a new project. </li> <li>In the left-side menu, select Resource list. Using the Create resource button, create a new Cloud Object Storage instance or simply use this link. </li> <li>Open your newly created IBM Cloud Storage Instance, create a new bucket. For this tutorial, you can select the Smart Tier which is the free tier. When directed, upload your file. For the sample file, refer to the tickets.csv file in the GitHub repository.</li> </ol>"},{"location":"tutorials/projects/react-agent-tutorial/#step-5-establish-your-data-connection","title":"Step 5. Establish your data connection","text":"<p>To provide the ReAct agent with IT ticket management functionality, we must connect to our data source in IBM Cloud Object Storage. For this step, we can use the <code>ibm_boto3</code> library.</p> <p>In <code>tools.py</code>, the <code>COS_ENDPOINT</code>, <code>COS_INSTANCE_CRN</code>, <code>BUCKET_NAME</code> and <code>CSV_FILE_NAME</code> must be filled in with the appropriate information using the bucket details found in your Cloud Object Storage instance under the Configuration tab. </p> <p>```python   COS_ENDPOINT = \"\"       #find in your COS bucket configuration   COS_INSTANCE_CRN = \"\"   #find in your COS bucket configuration   BUCKET_NAME = \"\"        #find in your COS bucket configuration   CSV_FILE_NAME = \"filename.csv\" #you can use the provided tickets.csv sample file </p> <p>cos = ibm_boto3.client(       \"s3\",       ibm_api_key_id=dep_config[\"watsonx_apikey\"],       ibm_service_instance_id=COS_INSTANCE_CRN,       config=Config(signature_version=\"oauth\"),       endpoint_url=COS_ENDPOINT,   )   ```</p>"},{"location":"tutorials/projects/react-agent-tutorial/#step-6-create-your-custom-tools","title":"Step 6. Create your custom tools","text":"<p>Our agent will be able to both read and write data in our file. First, let's create the tool to read data using the LangChain <code>@tool</code> decorator. </p> <p>We have added this <code>find_tickets</code> tool to the <code>tools.py</code> file. This tool retrieves the data object from Cloud Object Storage and returns it as a Pandas dataframe. Otherwise, an exception is thrown. </p> <p><code>python    @tool    def find_tickets():       \"\"\"Returns a list of of all tickets.\"\"\"       try:           response = cos.get_object(Bucket=BUCKET_NAME, Key=CSV_FILE_NAME)           csv_data = pd.read_csv(response['Body'])            print(\"Ticket file loaded successfully:\")           return csv_data       except Exception as e:           print(f\"Error loading file from COS: {e}\")           return None</code></p> <p>Next, we have added the <code>create_ticket</code> tool. </p> <p>```python    @tool  def create_ticket(issue: str, urgency:str):     \"\"\"Creates a tickets for a customer issue. Request a detailed explanation of the customer issue and urgency level before creating a ticket.</p> <pre><code>Args:\n    issue (str): A description of the issue.\n    urgency (str): A category value for the level of urgency. Can be \"low\", \"medium\", or \"high\".\n\nReturns:\n    The new ticket.\n\"\"\"\ntry:\n    # retrieve the existing item to reload the contents\n    response = cos.get_object(Bucket=BUCKET_NAME, Key=CSV_FILE_NAME)\n    existing_body_df = pd.read_csv(response['Body'])\n    new_ticket = {\"issue\": issue, \"date_added\":datetime.now().strftime(\"%m-%d-%Y\"), \"urgency\":urgency, \"status\":\"open\"}\n    # Add a new row (i.e. ticket) using loc[]\n    existing_body_df.loc[len(existing_body_df)] = new_ticket\n\n    cos.put_object(Bucket=BUCKET_NAME, Key=CSV_FILE_NAME, Body=existing_body_df.to_json())\n    return \"New ticket successfully created!\"\nexcept Exception as e:\n    print(\"Unable to create new ticket. Please try again.\")\n</code></pre> <p>```</p> <p>This tool takes in the description of the issue from the user and the urgency of the issue as its arguments. A new row is added to our file in COS with this information and thus, a new ticket is created. Otherwise, an exception is thrown. </p> <p>One last tool we must add to our <code>tools.py</code> file is the <code>get_todays_date</code> tool which uses the <code>datetime</code> module to return today's date in MM-DD-YYYY format. This tool will be useful for accessing the current date, which the agent has no other way of retrieving because the LLM was not trained on this data. </p> <p><code>python   @tool   def get_todays_date():     \"\"\"Returns today's date in MM-DD-YYYY format.\"\"\"     date = datetime.now().strftime(\"%m-%d-%Y\")     return date</code></p> <p>To grant our agent access to these tools, we have added them to the <code>TOOLS</code> list in the <code>extensions</code> module's init.py file. This list should be the contents of your <code>init.py</code> file in the <code>src/langgraph_react_agent</code> directory.</p> <p>```python    from .tools import (     find_tickets,     get_todays_date,     create_ticket   )</p> <p>TOOLS = [       find_tickets,       get_todays_date,       create_ticket   ]   ```</p> <p>These tools are imported in the <code>agent.py</code> file and passed to the prebuilt LangGraph <code>create_react_agent</code> function serving as the agent executor. Other parameters include the large language model initialized by using the <code>ChatWatsonx</code> class which allows for tool calling on watsonx.ai, the memory saver and system prompt. Note, some prompts will behave better than others and so, some level of prompt engineering might be required depending on the LLM you choose. </p>"},{"location":"tutorials/projects/react-agent-tutorial/#step-7-chat-with-your-agent","title":"Step 7. Chat with your agent","text":"<p>There are three ways to chat with your agent.</p>"},{"location":"tutorials/projects/react-agent-tutorial/#option-1-query-the-agent-locally","title":"Option 1. Query the agent locally","text":"<p>Run the script for local AI service execution.   <code>sh   poetry run python examples/execute_ai_service_locally.py</code></p>"},{"location":"tutorials/projects/react-agent-tutorial/#option-2-deploy-the-agent-to-the-built-in-chat-interface-in-watsonxai","title":"Option 2. Deploy the agent to the built-in chat interface in watsonx.ai","text":"<p>The final option is to access the agent in the Deployments space on watsonx.ai. To do this, select \"Deployments\" on the left-side menu. Then, select your deployment space, select the \"Assets\" tab, select your <code>online ai_service</code> asset, select the asset with the \"wx-agent\" tag once more and finally, open the \"Preview\" tab. Now, you can chat with the agent in the interactive chat interface. Each of these 3 options should result in a similar output. </p>"},{"location":"tutorials/projects/react-agent-tutorial/#option-3-deploy-and-chat-with-the-agent-in-your-ide","title":"Option 3. Deploy and chat with the agent in your IDE","text":"<p>To run the deployment script, initialize the <code>deployment_id</code> variable in the query_existing_deployment.py file.</p> <p>The <code>deployment_id</code> of your deployment can be obtained by running the scripts/deploy.py.</p> <p>Next, run the deployment script.    <code>sh   poetry run python scripts/deploy.py</code> Then, run the script for querying the deployment.   <code>sh   poetry run python examples/query_existing_deployment.py</code></p> <p>For the purposes of this tutorial, let's choose option 2 and query our deployed agent on watsonx.ai in the form of an agentic chatbot. Let's provide the agent with some prompts that would require the usage of tools. Upon following the steps listed in Option 3, you should see a chat interface on watsonx.ai. There, we can type our prompt. </p> <p>First, let's test whether the <code>create_ticket</code> tool will be successfully invoked. Let's prompt the agent to create a new ticket.</p> <p></p> <p>As you can see in the agent's final answer, the AI system successfully used problem-solving to create a new ticket with the <code>create_ticket</code> tool. Having the visibility into tool calls is helpful for debugging purposes. Now, let's check whether the ticket was successfully added to our data file serving as the agent's knowledge base. </p> <p></p> <p>Great! The agent successfully added the ticket to the file. </p>"},{"location":"tutorials/projects/react-agent-tutorial/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you created an agent with the ReAct framework that uses decision making to solve complex tasks such as retrieving and creating support tickets. There are several AI models out there that allow for agentic tool calling such as Google's Gemini, IBM's Granite and OpenAI's GPT-4. In our project, we used an IBM Granite AI model through the watsonx.ai API. The model behaved as expected both locally and when deployed on watsonx.ai. As a next step, check out the LlamaIndex and crewAI multiagent templates available in the watsonx-developer-hub GitHub repository for building AI agents. </p>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/","title":"Build an personal trainer with Meta Llama 4 using watsonx.ai","text":"<p>In this tutorial, you will build a generative AI-powered personal trainer. This AI trainer leverages the latest opensource Meta Llama 4 Scout large language model (LLM) for processing image input and generating personalized workout plans to reach your fitness goals effectively. We will access the model through the IBM watsonx.ai\u2122 API.</p> In\u00a0[\u00a0]: Copied! <pre># Install required packages\n!pip install -q image ibm-watsonx-ai\n</pre> # Install required packages !pip install -q image ibm-watsonx-ai In\u00a0[\u00a0]: Copied! <pre># Required imports\nimport getpass, os, base64, json\nfrom ibm_watsonx_ai import Credentials\nfrom ibm_watsonx_ai.foundation_models import ModelInference\nfrom PIL import Image\n</pre> # Required imports import getpass, os, base64, json from ibm_watsonx_ai import Credentials from ibm_watsonx_ai.foundation_models import ModelInference from PIL import Image <p>To set our credentials, we need the <code>WATSONX_APIKEY</code> and <code>WATSONX_PROJECT_ID</code> you generated in step 1. We will also set the URL serving as the API endpoint.</p> In\u00a0[\u00a0]: Copied! <pre>WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n\nWATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n\nURL = \"https://us-south.ml.cloud.ibm.com\"\n</pre> WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")  WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")  URL = \"https://us-south.ml.cloud.ibm.com\" <p>We can use the <code>Credentials</code> class to encapsulate our passed credentials.</p> In\u00a0[\u00a0]: Copied! <pre>credentials = Credentials(\n    url=URL,\n    api_key=WATSONX_APIKEY\n)\n</pre> credentials = Credentials(     url=URL,     api_key=WATSONX_APIKEY ) In\u00a0[\u00a0]: Copied! <pre>def augment_api_request_body(user_query, image):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [{\n                \"type\": \"text\",\n                \"text\": user_query\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image}\"\n                }\n            }]\n        }\n    ]\n    \n    return messages\n</pre> def augment_api_request_body(user_query, image):     messages = [         {             \"role\": \"user\",             \"content\": [{                 \"type\": \"text\",                 \"text\": user_query             },             {                 \"type\": \"image_url\",                 \"image_url\": {                     \"url\": f\"data:image/jpeg;base64,{image}\"                 }             }]         }     ]          return messages <p>We can also instantiate the model interface by using the <code>ModelInference</code> class.</p> In\u00a0[\u00a0]: Copied! <pre>model = ModelInference(\n        model_id=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n        credentials=credentials,\n        project_id=WATSONX_PROJECT_ID,\n        params={\n            \"max_tokens\": 128000,\n            \"temperature\": 0       \n        }\n    )\n</pre> model = ModelInference(         model_id=\"meta-llama/llama-4-scout-17b-16e-instruct\",         credentials=credentials,         project_id=WATSONX_PROJECT_ID,         params={             \"max_tokens\": 128000,             \"temperature\": 0                }     ) In\u00a0[\u00a0]: Copied! <pre>directory = \"images\"  #directory name\nimages = []\nfilenames = []\n\nfor filename in os.listdir(directory): \n  if filename.endswith(\".jpeg\") or filename.endswith(\".png\"): \n    filepath = directory + '/' +filename\n    with  open(filepath, \"rb\") as f:\n      images.append(base64.b64encode(f.read()).decode('utf-8')) \n    filenames.append(filename)\n    print(filename)\n</pre> directory = \"images\"  #directory name images = [] filenames = []  for filename in os.listdir(directory):    if filename.endswith(\".jpeg\") or filename.endswith(\".png\"):      filepath = directory + '/' +filename     with  open(filepath, \"rb\") as f:       images.append(base64.b64encode(f.read()).decode('utf-8'))      filenames.append(filename)     print(filename) <pre>image0.jpeg\nimage1.jpeg\nimage6.jpeg\nimage7.jpeg\nimage10.jpeg\nimage8.jpeg\nimage4.jpeg\nimage5.jpeg\nimage9.jpeg\nimage2.jpeg\nimage3.jpeg\n</pre> In\u00a0[\u00a0]: Copied! <pre>user_query = \"\"\"Provide a description, category, and workout type for the kinds of exercise equipment in each image, eg. \"barbell\", \"dumbbell\", \"machine\", \"bodyweight\", etc.   \n\n                Classify the description as \"equipment\" or \"other\".\n                Classify the category as \"barbell\", \"dumbbell\", \"machine\", \"bodyweight\", etc.\n                Classify the workout type as \"strength\", \"endurance\", \"flexibility\", \"balance\", \"cardio\", etc.\n                \n                Ensure the output is valid JSON. Do not create new categories or occasions. Only use the allowed classifications.\n                \n                Your response should be in this schema: \n                {\n                    \"description\": \"&lt;description&gt;\",\n                    \"category\": \"&lt;category&gt;\",\n                    \"workout_type\": \"&lt;workout_type&gt;\"\n                }\n                \"\"\"\n\nimage_descriptions = []\n\nfor i in range(len(images)):\n    image = images[i]\n    message = augment_api_request_body(user_query, image)\n    response = model.chat(messages=message)\n    result = response['choices'][0]['message']['content']\n    print(result)\n    image_descriptions.append(result)\n</pre> user_query = \"\"\"Provide a description, category, and workout type for the kinds of exercise equipment in each image, eg. \"barbell\", \"dumbbell\", \"machine\", \"bodyweight\", etc.                     Classify the description as \"equipment\" or \"other\".                 Classify the category as \"barbell\", \"dumbbell\", \"machine\", \"bodyweight\", etc.                 Classify the workout type as \"strength\", \"endurance\", \"flexibility\", \"balance\", \"cardio\", etc.                                  Ensure the output is valid JSON. Do not create new categories or occasions. Only use the allowed classifications.                                  Your response should be in this schema:                  {                     \"description\": \"\",                     \"category\": \"\",                     \"workout_type\": \"\"                 }                 \"\"\"  image_descriptions = []  for i in range(len(images)):     image = images[i]     message = augment_api_request_body(user_query, image)     response = model.chat(messages=message)     result = response['choices'][0]['message']['content']     print(result)     image_descriptions.append(result) <pre>{\n    \"description\": \"elliptical trainer\",\n    \"category\": \"machine\",\n    \"workout_type\": \"cardio\"\n}\n```json\n{\n    \"description\": \"treadmill\",\n    \"category\": \"machine\",\n    \"workout_type\": \"cardio\"\n}\n```\n```\n{\n  \"description\": \"exercise bike\",\n  \"category\": \"machine\",\n  \"workout_type\": \"cardio\"\n}\n```\n```json\n{\n    \"description\": \"A ballet barre\",\n    \"category\": \"barre\",\n    \"workout_type\": \"strength\"\n}\n```\n```json\n{\n    \"description\": \"Stairmaster\",\n    \"category\": \"machine\",\n    \"workout_type\": \"cardio\"\n}\n```\n```json\n{\n    \"description\": \"Pilates reformer\",\n    \"category\": \"machine\",\n    \"workout_type\": \"strength\"\n}\n```\n```json\n{\n    \"description\": \"barbell\",\n    \"category\": \"barbell\",\n    \"workout_type\": \"strength\"\n}\n```\n```json\n{\n    \"description\": \"A weightlifting bench with a barbell rack and weights\",\n    \"category\": \"barbell\",\n    \"workout_type\": \"strength\"\n}\n```\n```json\n{\n    \"description\": \"A dumbbell with multiple weight plates\",\n    \"category\": \"dumbbell\",\n    \"workout_type\": \"strength\"\n}\n```\n```json\n{\n    \"description\": \"rowing machine\",\n    \"category\": \"machine\",\n    \"workout_type\": \"cardio\"\n}\n```\n```json\n{\n    \"description\": \"yoga mat\",\n    \"category\": \"other\",\n    \"workout_type\": \"flexibility\"\n}\n```\n</pre> <p>To align the filenames with the image descriptions, we can enumerate the list of image descriptions and create a list of dictionaries. These dictionaries will store the description, category, occasion and filename of each item in the respective fields.</p> In\u00a0[\u00a0]: Copied! <pre># Add filenames to the image descriptions\ngym_equipment = []\nfor i, desc in enumerate(image_descriptions):\n    # Clean up the string by removing markdown code block markers and 'json' identifier\n    cleaned_desc = desc.strip()\n    if cleaned_desc.startswith('```'):\n        cleaned_desc = cleaned_desc.split('```')[1]  # Remove opening ```\n        if cleaned_desc.startswith('json'):\n            cleaned_desc = cleaned_desc[4:]  # Remove 'json' identifier\n        cleaned_desc = cleaned_desc.split('```')[0]  # Remove closing ```\n    cleaned_desc = cleaned_desc.strip()\n    \n    desc_dict = json.loads(cleaned_desc)\n    desc_dict['filename'] = filenames[i]\n    image_descriptions[i] = json.dumps(desc_dict)\n\ngym_equipment = [json.loads(js) for js in image_descriptions]\nprint(gym_equipment)\n</pre> # Add filenames to the image descriptions gym_equipment = [] for i, desc in enumerate(image_descriptions):     # Clean up the string by removing markdown code block markers and 'json' identifier     cleaned_desc = desc.strip()     if cleaned_desc.startswith('```'):         cleaned_desc = cleaned_desc.split('```')[1]  # Remove opening ```         if cleaned_desc.startswith('json'):             cleaned_desc = cleaned_desc[4:]  # Remove 'json' identifier         cleaned_desc = cleaned_desc.split('```')[0]  # Remove closing ```     cleaned_desc = cleaned_desc.strip()          desc_dict = json.loads(cleaned_desc)     desc_dict['filename'] = filenames[i]     image_descriptions[i] = json.dumps(desc_dict)  gym_equipment = [json.loads(js) for js in image_descriptions] print(gym_equipment) <pre>[{'description': 'elliptical trainer', 'category': 'machine', 'workout_type': 'cardio', 'filename': 'image0.jpeg'}, {'description': 'treadmill', 'category': 'machine', 'workout_type': 'cardio', 'filename': 'image1.jpeg'}, {'description': 'exercise bike', 'category': 'machine', 'workout_type': 'cardio', 'filename': 'image6.jpeg'}, {'description': 'A ballet barre', 'category': 'barre', 'workout_type': 'strength', 'filename': 'image7.jpeg'}, {'description': 'Stairmaster', 'category': 'machine', 'workout_type': 'cardio', 'filename': 'image10.jpeg'}, {'description': 'Pilates reformer', 'category': 'machine', 'workout_type': 'strength', 'filename': 'image8.jpeg'}, {'description': 'barbell', 'category': 'barbell', 'workout_type': 'strength', 'filename': 'image4.jpeg'}, {'description': 'A weightlifting bench with a barbell rack and weights', 'category': 'barbell', 'workout_type': 'strength', 'filename': 'image5.jpeg'}, {'description': 'A dumbbell with multiple weight plates', 'category': 'dumbbell', 'workout_type': 'strength', 'filename': 'image9.jpeg'}, {'description': 'rowing machine', 'category': 'machine', 'workout_type': 'cardio', 'filename': 'image2.jpeg'}, {'description': 'yoga mat', 'category': 'other', 'workout_type': 'flexibility', 'filename': 'image3.jpeg'}]\n</pre> <p>Now, let's query the Llama 4 model to produce a workout plan for our specified criteria by using the <code>gym_equipment</code> list. The following sample below uses the provided input:</p> <pre>workout_type = \"cardio\"\nlength = \"1 hour\"\nfitness_level = \"beginner\"\n</pre> In\u00a0[\u00a0]: Copied! <pre>workout_type = input(\"Enter the workout type\")             #strength, endurance, flexibility, balance, cardio, etc. (e.g. \"cardio\")\nlength = input(\"Enter the length of the workout\")          #30 minutes, 1 hour, 1.5 hours, etc.\nfitness_level = input(\"Enter your fitness level\")          #beginner, intermediate or advanced\nprompt = f\"\"\"Use the description, category, and workout type of the exercise equipment in my gym to put together a workout for a {fitness_level} {workout_type} workout. The workout must be no longer than {length}.\n                You must inclue the filename of each image in your output along with the file extension. Here is the equipment in my gym: {gym_equipment}\"\"\"\n\nmessages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": f\"{prompt}\"\n                }\n            ]\n        }\n    ]\n\nworkout = model.chat(messages=messages)['choices'][0]['message']['content']\nprint(workout)\n</pre> workout_type = input(\"Enter the workout type\")             #strength, endurance, flexibility, balance, cardio, etc. (e.g. \"cardio\") length = input(\"Enter the length of the workout\")          #30 minutes, 1 hour, 1.5 hours, etc. fitness_level = input(\"Enter your fitness level\")          #beginner, intermediate or advanced prompt = f\"\"\"Use the description, category, and workout type of the exercise equipment in my gym to put together a workout for a {fitness_level} {workout_type} workout. The workout must be no longer than {length}.                 You must inclue the filename of each image in your output along with the file extension. Here is the equipment in my gym: {gym_equipment}\"\"\"  messages = [         {             \"role\": \"user\",             \"content\": [                 {                     \"type\": \"text\",                     \"text\": f\"{prompt}\"                 }             ]         }     ]  workout = model.chat(messages=messages)['choices'][0]['message']['content'] print(workout) <pre>Based on the equipment available in your gym, I've put together a beginner-friendly cardio workout that can be completed within 1 hour. Since you're looking for a cardio workout, I'll focus on the equipment that falls under the 'cardio' or 'cardio' category. Here's a suggested workout:\n\n**Warm-up (5 minutes)**\n\n* Start with the 'rowing machine' (image2.jpeg) for 5 minutes to get your heart rate up and loosen your muscles.\n\n**Cardio Circuit (30 minutes)**\n\n* Move to the 'treadmill' (image1.jpeg) and set it to a walk or jog at a moderate pace. Spend 10 minutes on the treadmill to get your heart rate up and get some cardio benefits.\n* Next, head to the 'elliptical trainer' (image0.jpeg) and spend 10 minutes on it, taking your heart rate to a moderate level. You can adjust the resistance to make it more challenging.\n* Finally, hop on the 'exercise bike' (image6.jpeg) for 10 minutes to get some more cardio action.\n\n**High-Intensity Interval Training (HIIT) (20 minutes)**\n\n* Move to the 'Stairmaster' (image10.jpeg) and spend 5 minutes warming up at a moderate pace.\n* Then, increase the resistance and sprint for 2 minutes at maximum intensity.\n* Reduce the intensity and recover for 2 minutes. Repeat for a total of 15-20 minutes.\n\n**Cool-down (5 minutes)**\n\n* Finish your workout with some light stretching on the 'yoga mat' (image3.jpeg) to help prevent muscle soreness.\n\nHere's your workout schedule:\n\n1. Warm-up on the 'rowing machine' (image2.jpeg) (5 minutes)\n2. Cardio circuit:\n\t* Treadmill (10 minutes)\n\t* Elliptical trainer (image0.jpeg) (10 minutes)\n\t* Exercise bike (image6.jpeg) (10 minutes)\n3. HIIT on the Stairmaster (image10.jpeg) (20 minutes)\n4. Cool-down with stretching (5 minutes)\n\nThis workout should get your heart rate up and provide a great cardio session for beginners. Remember to listen to your body and adjust the intensity and duration according to your needs.\n\nExample Output:\n```\n**Beginner Cardio Workout**\n\nWarm-up (5 minutes):\n- Rowing machine (image2.jpeg)\n\nCardio Circuit (30 minutes):\n- Treadmill (image1.jpeg) - 10 minutes\n- Elliptical trainer (image0.jpeg) - 10 minutes\n- Exercise bike (image6.jpeg) - 10 minutes\n\n**Cool-down (5 minutes)**\n- Static stretches on the yoga mat (image3.jpeg)\n```\n</pre> <p>Great! This personal training algorithm returned a well-described cardio workout plan including the number of repetitions, or reps, and rest time. The LLM also places importance on proper form and hydration.</p> <p>With this generated workout description, we can also display the gym equipment that the model recommends. To do so, we can simply extract the filenames. In case the model mentions the same filename twice, it is important to check whether the image has not already been displayed as we iterate the list of images. We can do so by storing displayed images in the <code>selected_items</code> list. Finally, we can display the selected items.</p> In\u00a0[\u00a0]: Copied! <pre>selected_items = []\n#extract the images of clothing that the model recommends\nfor item, uploaded_file in zip(gym_equipment, images):\n    if item['filename'].lower() in workout.lower() and not any(key['filename'] == item['filename'] for key in selected_items):\n        selected_items.append({\n            'image': uploaded_file,\n            'category': item['category'],\n            'filename': item['filename']\n        })\n\n#display the selected clothing items\nif len(selected_items) &gt; 0:\n    for item in selected_items:\n        display(Image.open(directory + '/' + item['filename']))\n        \n</pre> selected_items = [] #extract the images of clothing that the model recommends for item, uploaded_file in zip(gym_equipment, images):     if item['filename'].lower() in workout.lower() and not any(key['filename'] == item['filename'] for key in selected_items):         selected_items.append({             'image': uploaded_file,             'category': item['category'],             'filename': item['filename']         })  #display the selected clothing items if len(selected_items) &gt; 0:     for item in selected_items:         display(Image.open(directory + '/' + item['filename']))"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#build-an-personal-trainer-with-meta-llama-4-using-watsonxai","title":"Build an personal trainer with Meta Llama 4 using watsonx.ai\u00b6","text":"<p>Authors: Ash Minhas, Anna Gutowska</p>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#introduction","title":"Introduction\u00b6","text":"<p>Are you looking to be more active? Has your fitness journey reached a plateau and are you looking to take your workout routines to the next level? In this tutorial, you will use AI to provide a personalized training experience.</p>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#ai-fitness-how-it-works","title":"AI fitness: How it works\u00b6","text":"<p>Our workout app is composed of the following stages:</p> <ol> <li><p>The user uploads images of their current workout equipment, one item at a time. This set can consist of both home &amp; gym equipment.</p> </li> <li><p>The user selects the following criteria:</p> <ul> <li>Workout type: strength, endurance, flexibility, balance, cardio and so on.</li> <li>Workout length: 30 minutes, 1 hour, 1.5 hours and so on.</li> <li>Fitness level: beginner, intermediate or advanced.</li> </ul> </li> <li><p>Upon submission of the input, the multimodal Llama 4 Scout model iterates over the list of images and returns the following output:</p> <ul> <li>Description of the item: either \u201cequipment\u201d or \u201cother.\u201d</li> <li>Category: \"barbell\", \"dumbbell\", \"machine\", \"bodyweight\" and so on.</li> <li>Workout type: \"strength\", \"endurance\", \"flexibility\", \"balance\", \"cardio\" and so on.</li> </ul> </li> <li><p>The same Llama 4 model then serves as a fitness coach. The LLM uses the previous output to provide a training plan that is suitable for the user\u2019s selections.</p> </li> <li><p>The training program and the images in the described personalized recommendation are all returned to the user.</p> </li> </ol>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You need an IBM Cloud\u00ae account to create a watsonx.ai\u2122 project.</p>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#steps","title":"Steps\u00b6","text":"<p>In order to use the watsonx\u00ae application programming interface (API), you will need to complete the following steps.</p>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai\u2122 using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> <p>This step will open a Jupyter Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on Github.</p> </li> </ol>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#step-2-set-up-watsonxai-runtime-service-and-api-key","title":"Step 2. Set up watsonx.ai Runtime service and API key\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this issue with a quick pip installation.</p>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#step-4-set-up-the-api-request-for-the-llm","title":"Step 4. Set up the API request for the LLM\u00b6","text":"<p>The <code>augment_api_request_body</code> function takes the user query and image as parameters and augments the body of the API request. We will use this function in each iteration of inferencing the LLM.</p>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#step-5-encode-images","title":"Step 5. Encode images\u00b6","text":"<p>To encode our images in a way that is digestible for the LLM, we encode them to bytes that we then decode to UTF-8 representation. In this case, our images are located in the local <code>images</code> directory. You can find sample images in the AI personal trainer directory in our GitHub repository.</p>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#step-6-categorize-input-with-the-llama-4-model","title":"Step 6. Categorize input with the Llama 4 model\u00b6","text":"<p>Now that we have loaded and encoded our images, we can query the Vision model. Our prompt is specific to our desired output to limit the model's creativity as we seek valid JSON output. We will store the description, category and workout type of each image in a list called <code>image_descriptions</code>.</p>"},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#step-7-generate-workout-plans-with-the-llm","title":"Step 7. Generate workout plans with the LLM\u00b6","text":""},{"location":"tutorials/projects/ai-personal-trainer-llama/ai-personal-trainer-llama/#conclusion","title":"Conclusion\u00b6","text":"<p>In this tutorial, you built a fitness app that uses an AI coach to customize and automate training sessions for new clients. This coaching platform and other advancements alike have the potential to reshape the fitness industry by providing real-time feedback to a real person looking for online training. Using photos or screenshots of the user's equipment, workout plans are customized by the AI tool to meet the specified criteria. Workouts for weight loss, muscle gain and strength training are all possible outputs. The Llama 4 model was critical for labeling and categorizing each item as well as generating the workout plan.</p> <p>Some next steps for building off this application can include:</p> <ul> <li>Creating a frontend to allow for AI chatbot functionality, similar to ChatGPT.</li> <li>Building an app for users to use the AI personal trainer on-the-go. This might be an iOS app for iPhone users, for example.</li> <li>Redesigning the use case to meet demand for other services such as mental health support.</li> </ul>"},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/","title":"Prompt Chaining with LangChain - A Comprehensive Overview","text":"<p>Prompt chaining is a foundational concept in building advanced workflows using language models (LLMs). It involves linking multiple prompts in a logical sequence, where the output of one prompt serves as the input for the next. This modular approach is powerful for solving complex tasks like multi-step text processing, summarization, question-answering, and more.</p> <p>LangChain is a versatile framework designed to simplify the creation of such workflows. It provides tools to manage LLMs such as IBM\u00ae Granite\u2122 models or OpenAI\u2019s GPT (generative pre-trained transformer) models, define custom prompts, and connect them into reusable chains. By abstracting the complexity of managing prompts, LangChain allows developers to focus on solving problems rather than orchestrating interactions with LLMs.</p> <p>In this tutorial, we will:</p> <ol> <li>Explore different types of prompt chaining (Sequential, Branching, Iterative, etc.).</li> <li>Implement a generic chaining example combining Sequential, Branching, and Iterative chaining types.</li> <li>Leverage LangChain's built-in classes like <code>PromptTemplate</code>, <code>LLMChain</code>, and <code>SequentialChain</code> to define and manage the workflow.</li> </ol> <p>LangChain provides a powerful framework for building modular workflows in chatbot applications. By combining structured prompts, dynamic chaining, and advanced LLM integration, it allows developers to create scalable, adaptive pipelines that leverage RAG techniques and deliver structured outputs like JSON. Here's how LangChain handles prompt chaining effectively:</p> <p>Prompt Abstraction: LangChain leverages <code>from_template</code> to design structured input-output workflows for each step, making it easy to handle complex chatbot operations.</p> <p>LLM Integration: The framework seamlessly integrates with various LLMs, such as, IBM Granite, OpenAI and Hugging Face, enabling fine-tuning for customized tasks.</p> <p>Chain Management: LangChain's <code>SequentialChain</code> and <code>SimpleSequentialChain</code> enable modular workflows for chatbot pipelines, while <code>stroutputparser</code> ensures structured outputs like JSON.</p> <p>Dynamic Workflows: Using tools like <code>ConditionalChain</code> and <code>systemmessage</code> templates, LangChain supports adaptive workflows, aligning with the principles of RAG (Retrieval-Augmented Generation) for dynamic content generation.</p> In\u00a0[3]: Copied! <pre>from IPython.display import Image, display\ndisplay(Image(filename='/Users/vrundagadesha/Tutorials/prompt-chaining-langchain-v1.png', embed=True))\n</pre> from IPython.display import Image, display display(Image(filename='/Users/vrundagadesha/Tutorials/prompt-chaining-langchain-v1.png', embed=True)) <p>By the end of this tutorial, you\u2019ll have a solid understanding of how to use LangChain to build modular and extensible workflows for a wide range of applications.</p> <p>Prompt chaining allows you to design workflows where outputs from one step are passed to the next. Different types of chaining support diverse workflows, ranging from simple sequential tasks to more complex, dynamic processes. Here\u2019s a brief look at the types of prompt chaining:</p> <ul> <li>Sequential Chaining: The most straightforward type of chaining, where the output of one prompt is directly passed as input to the next. This is ideal for tasks with a linear progression.<sup>[1]</sup></li> <li>Branching Chaining: In branching chaining, a single output is split into multiple parallel workflows. Each branch processes the output independently. <sup>[2]</sup></li> <li>Iterative Chaining: Iterative chaining involves repeatedly running a prompt or chain until a specified condition is met. This is useful for refining outputs.<sup>[3]</sup></li> <li>Hierarchical Chaining: This type breaks down a large task into smaller subtasks, which are executed hierarchically. Lower-level outputs feed higher-level tasks. <sup>[4]</sup></li> <li>Conditional Chaining: Conditional chaining dynamically chooses the next step based on the output of a prior prompt. It enables decision-making within workflows.</li> <li>Multi-Modal Chaining: Multi-modal chaining integrates prompts that handle different data types (e.g., text, images, or audio). It is suitable for applications combining multiple modalities. <sup>[2]</sup></li> <li>Dynamic Chaining: Dynamic chaining adapts the workflow based on real-time outputs or changing conditions. It adds flexibility to prompt chaining. <sup>[5]</sup></li> <li>Recursive Chaining: In recursive chaining, large inputs are divided into smaller chunks for individual processing, and the results are then combined. It is useful for handling lengthy documents or datasets. <sup>[6]</sup></li> <li>Reverse Chaining: Reverse chaining starts with a desired output and works backward to determine the necessary inputs or steps to achieve it. It is great for problem-solving and debugging. <sup>[5]</sup></li> </ul> <p>Each type of chaining caters to unique use cases, making it essential to choose the right one based on the task's complexity and requirements.</p> <p>In this workflow, we process customer feedback with chat models and prompt engineering to build a scalable text-processing pipeline. The following stages of the tutorial demonstrate sequential, branching, and iterative chaining techniques powered by generative AI.</p> <p>Extracting Keywords (Sequential Chaining)</p> <ul> <li>The input text or user input given in natural language is processed through a prompt template to identify significant keywords.</li> <li>This step uses sequential chaining to ensure the extracted keywords feed directly into subsequent tasks.</li> </ul> <p>Generating a Sentiment Summary (Branching Chaining)</p> <ul> <li>Extracted keywords are passed into a chat model to generate a sentiment summary.</li> <li>Branching chaining allows parallel paths for summarization to adapt outputs based on the context.</li> </ul> <p>Refining the Sentiment Summary (Iterative Chaining)</p> <ul> <li>If the sentiment summary doesn\u2019t meet predefined quality criteria, it is passed through a refinement prompt.</li> <li>Iterative chaining enables reprocessing until the output meets the desired level of precision.</li> </ul> <p>Final Output</p> <ul> <li>The refined sentiment summary is delivered as the final output, providing polished insights to the user.</li> <li>This showcases the integration of prompt engineering, generative AI, and advanced chaining techniques.</li> </ul> <p>This approach combines sequential, branching, and iterative chaining in Python with chat models and prompt engineering. It ensures robust processing of customer feedback, leveraging generative AI for keyword extraction, sentiment analysis, and refinement.</p> <p>You need an IBM Cloud account to create a watsonx.ai project.</p> <p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li>Log in to watsonx.ai using your IBM Cloud account.</li> <li>Create a watsonx.ai project. You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</li> <li>Create a Jupyter Notebook.</li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on Github.</p> <ol> <li>Create a watsonx.ai Runtime service instance (choose the Lite plan, which is a free instance).</li> <li>Generate an API Key.</li> <li>Associate the watsonx.ai Runtime service to the project you created in watsonx.ai.</li> </ol> <p>We need libraries to work with langchain framework and WatsonxLLM. Let's first install the required packages.</p> <p>Note: If you are using an old version of <code>pip</code>, you can use the command <code>pip install --upgrade pip</code> to upgrade it as it helps you in easy installation of latest packages, which may not be compatible with old versions. But if you are already using the latest version or recently upgraded you packages, then you can skip this command.</p> In\u00a0[1]: Copied! <pre>pip install --upgrade pip\n%pip install langchain\n!pip install langchain-ibm\n</pre> pip install --upgrade pip %pip install langchain !pip install langchain-ibm <pre>Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (24.3.1)\nNote: you may need to restart the kernel to use updated packages.\n</pre> In\u00a0[4]: Copied! <pre>import os\nfrom langchain_ibm import WatsonxLLM\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain, SequentialChain\nimport getpass\n</pre> import os from langchain_ibm import WatsonxLLM from langchain.prompts import PromptTemplate from langchain.chains import LLMChain, SequentialChain import getpass In\u00a0[28]: Copied! <pre># Set up credentials\ncredentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",  # Replace with the correct region if needed\n    \"apikey\": getpass.getpass(\"Please enter your WML API key (hit enter): \")\n}\n\n# Set up project_id\ntry:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")\n</pre>  # Set up credentials credentials = {     \"url\": \"https://us-south.ml.cloud.ibm.com\",  # Replace with the correct region if needed     \"apikey\": getpass.getpass(\"Please enter your WML API key (hit enter): \") }  # Set up project_id try:     project_id = os.environ[\"PROJECT_ID\"] except KeyError:     project_id = input(\"Please enter your project_id (hit enter): \")  Out[28]: <pre>'\\n# Set up credentials\\ncredentials = {\\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",  # Replace with the correct region if needed\\n    \"apikey\": getpass.getpass(\"Please enter your WML API key (hit enter): \")\\n}\\n\\n# Set up project_id\\ntry:\\n    project_id = os.environ[\"PROJECT_ID\"]\\nexcept KeyError:\\n    project_id = input(\"Please enter your project_id (hit enter): \")\\n'</pre> In\u00a0[6]: Copied! <pre># Initialize the IBM LLM\nllm = WatsonxLLM(\n    model_id=\"ibm/granite-3-8b-instruct\",\n    url=credentials[\"url\"],\n    apikey=credentials[\"apikey\"],\n    project_id=project_id,\n    params={\n        \"max_new_tokens\": 150\n    }\n)\n</pre> # Initialize the IBM LLM llm = WatsonxLLM(     model_id=\"ibm/granite-3-8b-instruct\",     url=credentials[\"url\"],     apikey=credentials[\"apikey\"],     project_id=project_id,     params={         \"max_new_tokens\": 150     } ) In\u00a0[7]: Copied! <pre># Define Prompt Templates\n\n# Prompt for extracting keywords\nkeyword_prompt = PromptTemplate(\n    input_variables=[\"text\"],\n    template=\"Extract the most important keywords from the following text:\\n{text}\\n\\nKeywords:\"\n)\n\n# Prompt for generating sentiment summary\nsentiment_prompt = PromptTemplate(\n    input_variables=[\"keywords\"],\n    template=\"Using the following keywords, summarize the sentiment of the feedback:\\nKeywords: {keywords}\\n\\nSentiment Summary:\"\n)\n\n# Prompt for refining the summary\nrefine_prompt = PromptTemplate(\n    input_variables=[\"sentiment_summary\"],\n    template=\"Refine the following sentiment summary to make it more concise and precise:\\n{sentiment_summary}\\n\\nRefined Summary:\"\n)\n</pre> # Define Prompt Templates  # Prompt for extracting keywords keyword_prompt = PromptTemplate(     input_variables=[\"text\"],     template=\"Extract the most important keywords from the following text:\\n{text}\\n\\nKeywords:\" )  # Prompt for generating sentiment summary sentiment_prompt = PromptTemplate(     input_variables=[\"keywords\"],     template=\"Using the following keywords, summarize the sentiment of the feedback:\\nKeywords: {keywords}\\n\\nSentiment Summary:\" )  # Prompt for refining the summary refine_prompt = PromptTemplate(     input_variables=[\"sentiment_summary\"],     template=\"Refine the following sentiment summary to make it more concise and precise:\\n{sentiment_summary}\\n\\nRefined Summary:\" ) In\u00a0[8]: Copied! <pre># Define Chains with Unique Keys\n\n# Chain to extract keywords\nkeyword_chain = LLMChain(\n    llm=llm,\n    prompt=keyword_prompt,\n    output_key=\"keywords\"  # Unique key for extracted keywords\n)\n\n# Chain to generate sentiment summary\nsentiment_chain = LLMChain(\n    llm=llm,\n    prompt=sentiment_prompt,\n    output_key=\"sentiment_summary\"  # Unique key for sentiment summary\n)\n\n# Chain to refine the sentiment summary\nrefine_chain = LLMChain(\n    llm=llm,\n    prompt=refine_prompt,\n    output_key=\"refined_summary\"  # Final refined output\n)\n</pre> # Define Chains with Unique Keys  # Chain to extract keywords keyword_chain = LLMChain(     llm=llm,     prompt=keyword_prompt,     output_key=\"keywords\"  # Unique key for extracted keywords )  # Chain to generate sentiment summary sentiment_chain = LLMChain(     llm=llm,     prompt=sentiment_prompt,     output_key=\"sentiment_summary\"  # Unique key for sentiment summary )  # Chain to refine the sentiment summary refine_chain = LLMChain(     llm=llm,     prompt=refine_prompt,     output_key=\"refined_summary\"  # Final refined output ) <pre>/var/folders/4w/smh16qdx6l98q0534hr9v52r0000gn/T/ipykernel_34449/1476032907.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n  keyword_chain = LLMChain(\n</pre> In\u00a0[9]: Copied! <pre># Combine Chains into a Sequential Workflow\n\nworkflow = SequentialChain(\n    chains=[keyword_chain, sentiment_chain, refine_chain],\n    input_variables=[\"text\"],  # Initial input for the workflow\n    output_variables=[\"refined_summary\"]  # Final output of the workflow\n)\n</pre> # Combine Chains into a Sequential Workflow  workflow = SequentialChain(     chains=[keyword_chain, sentiment_chain, refine_chain],     input_variables=[\"text\"],  # Initial input for the workflow     output_variables=[\"refined_summary\"]  # Final output of the workflow ) In\u00a0[13]: Copied! <pre># Example Input Text\n\nfeedback_text = \"\"\"\nI really enjoy the features of this app, but it crashes frequently, making it hard to use. \nThe customer support is helpful, but response times are slow.\n\nI tried to reachout to the support team, but they never responded\n\nFor me, the customer support was very much helpful. Ihis is very helpful app. Thank you for grate services. \n\"\"\"\n\n# Run the Workflow\n\nresult = workflow.run({\"text\": feedback_text})\n\n# Display the Output\n\nprint(\"Refined Sentiment Summary:\")\nprint(result)  # Directly print the result since it is a string\n</pre> # Example Input Text  feedback_text = \"\"\" I really enjoy the features of this app, but it crashes frequently, making it hard to use.  The customer support is helpful, but response times are slow.  I tried to reachout to the support team, but they never responded  For me, the customer support was very much helpful. Ihis is very helpful app. Thank you for grate services.  \"\"\"  # Run the Workflow  result = workflow.run({\"text\": feedback_text})  # Display the Output  print(\"Refined Sentiment Summary:\") print(result)  # Directly print the result since it is a string <pre>Refined Sentiment Summary:\n\n\nThe user appreciates the app's features but is frustrated by frequent crashes and slow customer support response times, hindering overall usability.\n</pre> <p>The refined sentiment summary is a concise and clear evaluation of the feedback. It highlights the user's appreciation for the app's features but expresses frustration over frequent crashes and slow customer support, reflecting the workflow's ability to distill critical insights effectively.</p> <p>Selecting the appropriate chaining type for LLM application involves evaluating key factors to ensure efficiency and coherence:</p> <p>Task Complexity: Use runnable workflows for tasks with multiple steps. Few-shot examples or chatprompttemplate can help structure complex tasks requiring different prompts.</p> <p>Dependency: If outputs from one step are placeholders for the next prompt, use sequential chaining. Output parsers ensure smooth transition of outputs into structured inputs.</p> <p>Adaptability: For dynamic workflows, such as those involving langchain agents, iterative chaining allows real-time adjustments of parameters and next prompts.</p> <p>Data Modality: Choose workflows compatible with varied data types. Use embedding methods for text and vector data or LangChain Expression Language for flexible operations.</p> <p>By considering these factors, you can build a robust and adaptable LLM application with coherent chaining workflows.</p> <p>Prompt chaining is a versatile technique for building sophisticated NLP workflows. In this tutorial, we explored various chaining types and demonstrated a generic example integrating multiple chaining approaches. By experimenting with these methods, you can unlock the full potential of language models for real-world applications.</p> <p>[1] Roegiest, A., &amp; Chitta, R. (2024). Answering Questions in Stages: Prompt Chaining for Contract QA.</p> <p>[2] Ge, J., Luo, H., Qian, S., Gan, Y., Fu, J., &amp; Zhang, S. (2023). Chain of Thought Prompt Tuning in Vision Language Models.</p> <p>[3] Sun, S., Yuan, R., Cao, Z., Li, W., &amp; Liu, P. (2024). Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization. , 7551-7558. https://doi.org/10.48550/arXiv.2406.00507.</p> <p>[4] Huang, Q., Zhu, J., Li, Z., Xing, Z., Wang, C., &amp; Xu, X. (2023). PCR-Chain: Partial Code Reuse Assisted by Hierarchical Chaining of Prompts on Frozen Copilot. 2023 IEEE/ACM 45th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 1-5. https://doi.org/10.1109/ICSE-Companion58688.2023.00013.</p> <p>[5] Wu, T., Jiang, E., Donsbach, A., Gray, J., Molina, A., Terry, M., &amp; Cai, C. (2022). PromptChainer: Chaining Large Language Model Prompts through Visual Programming. CHI Conference on Human Factors in Computing Systems Extended Abstracts. https://doi.org/10.1145/3491101.3519729.</p> <p>[6] Trautmann, D. (2023). Large Language Model Prompt Chaining for Long Legal Document Classification. ArXiv, abs/2308.04138. https://doi.org/10.48550/arXiv.2308.04138.</p>"},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#prompt-chaining-with-langchain-a-comprehensive-overview","title":"Prompt Chaining with LangChain - A Comprehensive Overview\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#how-langchain-manages-prompt-chaining","title":"How LangChain Manages Prompt Chaining\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#types-of-prompt-chaining","title":"Types of Prompt Chaining\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#use-case-multi-step-text-processing","title":"Use Case - Multi-Step Text Processing\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#prerequisites","title":"Prerequisites\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-2-set-up-watsonxai-runtime-service-and-api-key","title":"Step 2. Set up watsonx.ai Runtime service and API key\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-3-installation-of-the-packages","title":"Step 3. Installation of the packages\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-4-import-required-libraries","title":"Step 4. Import Required Libraries\u00b6","text":"<p>This code block imports essential python libraries and tools to build and manage an LLM application using LangChain and IBM Watson LLM. <code>os</code> used to access environment variables, such as project credentials or API keys. <code>WatsonxLLM</code> is a module from langchain_ibm that integrates IBM Watson LLM for generating outputs from generative AI models. <code>PromptTemplate</code> helps create reusable templates for prompts, ensuring input structure and flexibility in prompt engineering. <code>LLMChain</code> builds individual task chains while <code>SequentialChain</code> links multiple steps into a single workflow and <code>getpass</code> safely retrieves sensitive information (e.g., API keys) without exposing it on the screen.</p>"},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-5-set-up-credentials","title":"Step 5. Set Up Credentials\u00b6","text":"<p>This code sets up credentials for accessing the IBM Watson Machine Learning (WML) API and ensures the project ID is correctly configured.</p> <ul> <li>A dictionary <code>credentials</code> is created with the WML service URL and API key. The API key is securely collected using <code>getpass.getpass</code> to avoid exposing sensitive information.</li> <li>the code tries to fetch the <code>PROJECT_ID</code> from environment variables using <code>os.environ</code>. If the <code>PROJECT_ID</code> is not found, the user is prompted to manually enter it via input.</li> </ul>"},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-6-initialize-large-language-model","title":"Step 6. Initialize Large Language Model\u00b6","text":"<p>This code initializes the IBM Watson LLM for use in the application:</p> <ol> <li>This code creates an instance of <code>WatsonxLLM</code> using the <code>ibm/granite-3-8b-instruct</code> model, designed for instruction-based generative AI tasks.</li> <li>The <code>url</code>, <code>apikey</code>, and <code>project_id</code> values from the previously set up credentials are passed to authenticate and connect to the IBM Watson LLM service.</li> <li>Configures the <code>max_new_tokens</code> parameter to limit the number of tokens generated by the model in each response (150 tokens in this case).</li> </ol> <p>This step prepares the Watson LLM for generating responses in the workflow.</p>"},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-7-define-prompt-templates","title":"Step 7. Define Prompt Templates\u00b6","text":"<p>This code defines prompt templates for three stages of the text-processing workflow:</p> <ol> <li>Keyword Extraction: <code>keyword_prompt</code> is designed to extract the most significant keywords from the provided text. It uses the placeholder <code>{text}</code> to dynamically insert the input.</li> <li>Sentiment Summary Generation: <code>sentiment_prompt</code> takes the extracted <code>{keywords}</code> as input and generates a sentiment summary of the feedback. The template ensures the sentiment generation is focused on the provided keywords.</li> <li>Refinement of the Summary: <code>refine_prompt</code> improves the sentiment summary using <code>{sentiment_summary}</code> as input. It focuses on making the output concise and precise.</li> </ol> <p>These PromptTemplate instances enable reusable and structured prompt engineering for the LLM application.</p>"},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-8-create-chains","title":"Step 8. Create Chains\u00b6","text":"<p>This code defines LLM chains that connect the prompts with the initialized <code>IBM Watson LLM</code>, assigning unique output keys for each stage:</p> <ol> <li>Keyword Chain: <code>keyword_chain</code> uses the <code>keyword_prompt</code> to extract keywords from the input text. The result is stored under the unique key <code>\"keywords\"</code> for use in subsequent steps.</li> <li>Sentiment Chain: <code>sentiment_chain</code> takes the extracted keywords and generates a sentiment summary using the <code>sentiment_prompt</code>. The output is labeled as <code>\"sentiment_summary\"</code>.</li> <li>Refinement Chain: <code>refine_chain</code> processes the generated sentiment summary using the <code>refine_prompt</code>. The final refined output is stored with the key <code>\"refined_summary\"</code>.</li> </ol> <p>These LLMChain instances enable modular task execution, facilitating a step-by-step LLM application workflow.</p>"},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-9-combine-chains","title":"Step 9. Combine Chains\u00b6","text":"<p>This code combines the previously defined chains into a sequential workflow, enabling a step-by-step process for text input. The <code>SequentialChain</code> links the <code>keyword_chain</code>, <code>sentiment_chain</code>, and <code>refine_chain</code> in a defined order, ensuring that the output of one chain serves as the input for the next. The <code>workflow</code> is configured to accept text as its initial input, with the final output, a refined sentiment summary, stored under the key <code>\"refined_summary\"</code>. This setup allows for streamlined and efficient execution of the LLM application, ensuring a coherent and modular processing pipeline.</p>"},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#step-10-run-the-workflow","title":"Step 10. Run the Workflow\u00b6","text":"<p>In this code block, we will run the entire workflow. First of all we have a multi-line feedback string is defined as <code>feedback_text</code>,containing both positive and negative user comments about an app. The <code>workflow.run</code> method processes the feedback through the sequential chains (keyword extraction, sentiment analysis, and refinement) using the provided input. And the refined sentiment summary is printed directly as the final result.</p>"},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#how-to-the-choose-the-correct-chaining-type","title":"How to the choose the correct chaining type\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#summary","title":"Summary\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-chaining-langchain/#footnotes","title":"Footnotes\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-tuning/","title":"Prompt tune a Granite model using watsonx","text":"In\u00a0[\u00a0]: Copied! <pre>###For demonstration purposes only. It is not necessary to run this code block.\nhard_prompt_template = \"\"\"Generate a summary of the context that answers the question. Explain the answer in multiple steps if possible. \nAnswer style should match the context. Ideal Answer length is 2-3 sentences.\\n\\n{context}\\nQuestion: {question}\\nAnswer:\n\"\"\"\n</pre> ###For demonstration purposes only. It is not necessary to run this code block. hard_prompt_template = \"\"\"Generate a summary of the context that answers the question. Explain the answer in multiple steps if possible.  Answer style should match the context. Ideal Answer length is 2-3 sentences.\\n\\n{context}\\nQuestion: {question}\\nAnswer: \"\"\" <p>Using this hard prompt template, an LLM can be provided with specific instructions on the preferred output structure and style. Through this explicit prompt, the LLM would be more likely to produce desirable responses of higher quality.</p> <p>Soft prompts, unlike hard prompts, are not written in natural language. Instead, prompts are initialized as AI-generated, numerical vectors appended to the start of each input embedding that distill knowledge from the larger model. This lack of interpretability extends to the AI that chooses prompts optimized for a given task. Often, the AI is unable to explain why it chose those embeddings. In comparison to other prompting methods, these virtual tokens are less computationally expensive than fine-tuning since the model itself remains frozen with fixed weights. Soft prompts also tend to outperform human-engineered hard prompts.</p> <p>We will be working with soft prompts for prompt tuning in this tutorial.</p> In\u00a0[\u00a0]: Copied! <pre>#installations \n%pip install ibm-watsonx-ai | tail -n 1\n%pip install pandas | tail -n 1\n%pip install wget | tail -n 1\n%pip install scikit-learn | tail -n 1\n%pip install matplotlib | tail -n 1\n</pre> #installations  %pip install ibm-watsonx-ai | tail -n 1 %pip install pandas | tail -n 1 %pip install wget | tail -n 1 %pip install scikit-learn | tail -n 1 %pip install matplotlib | tail -n 1 In\u00a0[\u00a0]: Copied! <pre>#imports\nimport wget\nimport pandas as pd\nimport getpass\n\nfrom ibm_watsonx_ai import APIClient\nfrom ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\nfrom ibm_watsonx_ai.experiment import TuneExperiment\nfrom ibm_watsonx_ai.helpers import DataConnection\nfrom ibm_watsonx_ai.foundation_models import ModelInference\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom datetime import datetime\n</pre> #imports import wget import pandas as pd import getpass  from ibm_watsonx_ai import APIClient from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes from ibm_watsonx_ai.experiment import TuneExperiment from ibm_watsonx_ai.helpers import DataConnection from ibm_watsonx_ai.foundation_models import ModelInference from sklearn.metrics import accuracy_score, f1_score from datetime import datetime <p>Set up your credentials.</p> In\u00a0[\u00a0]: Copied! <pre>credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n}\n\nproject_id = getpass.getpass(\"Please enter your project ID (hit enter): \")\n</pre> credentials = {     \"url\": \"https://us-south.ml.cloud.ibm.com\",     \"apikey\": getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \") }  project_id = getpass.getpass(\"Please enter your project ID (hit enter): \") <p>As the first step in establishing the environment, create an instance of APIClient with your authentication details and set your <code>project_id</code>.</p> In\u00a0[\u00a0]: Copied! <pre>client = APIClient(credentials)\nclient.set.default_project(project_id)\n</pre> client = APIClient(credentials) client.set.default_project(project_id) <p>For this tutorial, we will be using a synthetic dataset consisting of dog grooming business reviews. Using the appropriate URL, we can connect the dataset to the API client.</p> <p>You are free to use any dataset of your choice. Several open-source datasets are available on platforms such as HuggingFace.</p> In\u00a0[\u00a0]: Copied! <pre>train_filename = 'dog_grooming_reviews_train_data.json'\n\nurl = \"https://raw.githubusercontent.com/AnnaGutowska/think/main/tutorials/prompt-tuning-tutorial/\" + train_filename\nwget.download(url)\n\nasset_details = client.data_assets.create(name=train_filename, file_path=train_filename)\nasset_id = client.data_assets.get_id(asset_details)\n</pre> train_filename = 'dog_grooming_reviews_train_data.json'  url = \"https://raw.githubusercontent.com/AnnaGutowska/think/main/tutorials/prompt-tuning-tutorial/\" + train_filename wget.download(url)  asset_details = client.data_assets.create(name=train_filename, file_path=train_filename) asset_id = client.data_assets.get_id(asset_details) <pre>Creating data asset...\nSUCCESS\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(asset_id)\n</pre> print(asset_id) <pre>3b1db894-8d9e-428d-8fee-d96f328c7726\n</pre> <p>To gain some insight into the formatting of these customer reviews, let's load the data into a Pandas dataframe and print a few rows that show both positive and negative reviews. An output of \"1\" denotes positive reviews and \"0\" is used for negative reviews.</p> In\u00a0[\u00a0]: Copied! <pre>pd.set_option('display.max_colwidth', None)\ndf = pd.read_json(train_filename)\ndf[5:10]\n</pre> pd.set_option('display.max_colwidth', None) df = pd.read_json(train_filename) df[5:10] input output 5 Comment: I have been taking my dog to this groomer for a while now and I have always been impressed with the service. The groomers are skilled and patient, especially with my dog who can be quite nervous around new people. They take their time to make sure my dog looks great and never rush through the process. The prices are also very reasonable. I would definitely recommend this groomer to others!\\nSatisfaction:\\n 1 6 Comment: I had a very poor experience at this grooming salon. The groomer was not very experienced and made several mistakes while grooming my dog. The fur was left all over my house and furniture. I called to express my concerns and was met with dismissiveness and arrogance. I will not be returning to this groomer.\\nSatisfaction:\\n 0 7 Comment: I took my dog to this groomer for the first time and was very disappointed. The groomer was rushing through the process and did not seem to pay attention to detail. My dog came out with matted fur and dirty paws. I will not be returning to this groomer.\\nSatisfaction:\\n 0 8 Comment: I have been taking my dog to this groomer for years, but I have recently started having issues with the service. The groomers seem to be in a hurry and do not seem to care about the quality of their work. My dog has come out with cuts and bruises on multiple occasions. I am considering finding a new groomer.\\nSatisfaction:\\n 0 9 Comment: My dog is always super excited to visit this groomer. I always have a positive experience here!\\nSatisfaction:\\n 1 <p>The <code>TuneExperiment</code> class is used to create experiments and schedule tunings. Let's use it to initialize our experiment and set our base foundation model, training data and parameters. The goal of this prompt tuning exercise is for the LLM to tailor its responses in accordance with the extracted customer satisfaction ratings from our dataset. This is a classification task since the reviews can be classified as either positive (\"1\") or negative (\"0\").</p> <p>For this tutorial, we suggest using an IBM Granite Model as the large language model to achieve similar results.</p> In\u00a0[\u00a0]: Copied! <pre>experiment = TuneExperiment(credentials,\n    project_id=project_id\n)\n\nprompt_tuner = experiment.prompt_tuner(name=\"prompt tuning tutorial\",\n    task_id=experiment.Tasks.CLASSIFICATION,\n    base_model=ModelTypes.GRANITE_13B_INSTRUCT_V2,\n    accumulate_steps=16,\n    batch_size=8,\n    learning_rate=0.001,\n    max_input_tokens=128,\n    max_output_tokens=2,\n    num_epochs=12,\n    tuning_type=experiment.PromptTuningTypes.PT,\n    init_text=\"Extract the satisfaction from the comment. Return simple '1' for satisfied customer or '0' for unsatisfied. Comment:\",\n    init_method=\"text\",\n    verbalizer=\"classify {0, 1} {{input}}\",\n    auto_update_model=True\n)\n</pre> experiment = TuneExperiment(credentials,     project_id=project_id )  prompt_tuner = experiment.prompt_tuner(name=\"prompt tuning tutorial\",     task_id=experiment.Tasks.CLASSIFICATION,     base_model=ModelTypes.GRANITE_13B_INSTRUCT_V2,     accumulate_steps=16,     batch_size=8,     learning_rate=0.001,     max_input_tokens=128,     max_output_tokens=2,     num_epochs=12,     tuning_type=experiment.PromptTuningTypes.PT,     init_text=\"Extract the satisfaction from the comment. Return simple '1' for satisfied customer or '0' for unsatisfied. Comment:\",     init_method=\"text\",     verbalizer=\"classify {0, 1} {{input}}\",     auto_update_model=True ) <p>Now that we have our tuning experiment set up, we need to connect it to our dataset. For this, let's use the <code>DataConnection</code> class. This requires  the <code>asset_id</code> we produced earlier upon initiating the data asset with our API client.</p> In\u00a0[\u00a0]: Copied! <pre>data_conn = DataConnection(data_asset_id=asset_id)\n</pre> data_conn = DataConnection(data_asset_id=asset_id) <p>You are free to use an AI model of your choice. The foundation models available to tune through watsonx can be found here or by running the following command.</p> In\u00a0[\u00a0]: Copied! <pre>client.foundation_models.PromptTunableModels.show()\n</pre> client.foundation_models.PromptTunableModels.show() <p>We are now ready to begin the prompt tuning process. The progress will be shown once you run the following cell. This should take approximately 10 to 15 minutes.</p> In\u00a0[\u00a0]: Copied! <pre>tuning_details = prompt_tuner.run(\n    training_data_references=[data_conn],\n    background_mode=False)\n</pre> tuning_details = prompt_tuner.run(     training_data_references=[data_conn],     background_mode=False) <pre>\n\n##############################################\n\nRunning '20671f17-ff53-470b-9bfe-04318ecb91d9'\n\n##############################################\n\n\npending......\nrunning....................................................................................................................................\ncompleted\nTraining of '20671f17-ff53-470b-9bfe-04318ecb91d9' finished successfully.\n</pre> <p>To ensure our prompt tuning has concluded, we can check the status. If the status that prints is anything other than \"completed,\" please wait for the tuning to finish before continuing.</p> In\u00a0[\u00a0]: Copied! <pre>status = prompt_tuner.get_run_status()\nprint(status)\n</pre> status = prompt_tuner.get_run_status() print(status) <pre>completed\n</pre> <p>We can now retrieve the prompt tuning summary. In this summary, you will see a loss value. For each training run, the loss function measures the difference between the predicted and actual results. Hence, a lower loss value is preferred.</p> In\u00a0[\u00a0]: Copied! <pre>prompt_tuner.summary()\n</pre> prompt_tuner.summary() Enhancements Base model Auto store Epochs loss Model Name model_20671f17-ff53-470b-9bfe-04318ecb91d9 [prompt_tuning] ibm/granite-13b-instruct-v2 True 12 0.010478 <p>We can also plot the learning curve of our model tuning using the <code>plot_learning_curve()</code> function. A downward-sloping curve that levels off close to zero indicates that the model is improving its expected output generation. To learn more about interpreting loss function graphs, see the relevant IBM watsonx documentation.</p> In\u00a0[\u00a0]: Copied! <pre>prompt_tuner.plot_learning_curve()\n</pre> prompt_tuner.plot_learning_curve() <p>This step of deploying the tuned model is critical for completing the next step of comparing the performance of the tuned model to the pretuned model.</p> <p>Note: The <code>SERVING_NAME</code> is set to the current date and time since it must be a unique value.</p> In\u00a0[\u00a0]: Copied! <pre>model_id = prompt_tuner.get_model_id()\n\nmeta_props = {\n    client.deployments.ConfigurationMetaNames.NAME: \"PROMP TUNE DEPLOYMENT\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {},\n    client.deployments.ConfigurationMetaNames.SERVING_NAME : datetime.now().strftime('%Y_%m_%d_%H%M%S')\n}\n\ndeployment_details = client.deployments.create(model_id, meta_props)\n</pre> model_id = prompt_tuner.get_model_id()  meta_props = {     client.deployments.ConfigurationMetaNames.NAME: \"PROMP TUNE DEPLOYMENT\",     client.deployments.ConfigurationMetaNames.ONLINE: {},     client.deployments.ConfigurationMetaNames.SERVING_NAME : datetime.now().strftime('%Y_%m_%d_%H%M%S') }  deployment_details = client.deployments.create(model_id, meta_props) <pre>\n\n######################################################################################\n\nSynchronous deployment creation for id: '6aa5dd5c-0cc4-44e0-9730-18303e88e14a' started\n\n######################################################################################\n\n\ninitializing.......................\nready\n\n\n-----------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_id='24a97b84-47d0-4490-9f5f-21ed2376fdd6'\n-----------------------------------------------------------------------------------------------\n\n\n</pre> <p>Now, let's test the performance of both the tuned model and the original foundation model to see the impacts of our tuning process. First, let's load the testing dataset. This dataset should be a subset of data that was not present during tuning. Often, the test set is also smaller than the training set. Additionally, each input in the test dataset has the prompt as the prefix to the user's comment.</p> In\u00a0[\u00a0]: Copied! <pre>test_filename = 'dog_grooming_reviews_test_data.json'\nurl = \"https://raw.githubusercontent.com/AnnaGutowska/think/main/tutorials/prompt-tuning-tutorial/\" + test_filename\nwget.download(url)\ndata = pd.read_json(test_filename)\n</pre> test_filename = 'dog_grooming_reviews_test_data.json' url = \"https://raw.githubusercontent.com/AnnaGutowska/think/main/tutorials/prompt-tuning-tutorial/\" + test_filename wget.download(url) data = pd.read_json(test_filename) <p>Let's display a small portion of the dataset to better understand its structure.</p> In\u00a0[\u00a0]: Copied! <pre>data.head()\n</pre> data.head() input output 0 Extract the satisfaction from the comment. Return simple 1 for satisfied customer or 0 for unsatisfied.\\nComment: Long wait times.\\nSatisfaction:\\n 0 1 Extract the satisfaction from the comment. Return simple 1 for satisfied customer or 0 for unsatisfied.\\nComment: This is the best grooming salon I've ever been to! The staff is incredibly friendly and knowledgeable. They took great care of my dog, who has a lot of fur, and did an excellent job cleaning him up. My dog was relaxed and happy when I picked him up, which is a huge plus. I highly recommend this place!\\nSatisfaction:\\n 1 2 Extract the satisfaction from the comment. Return simple 1 for satisfied customer or 0 for unsatisfied.\\nComment: I have been taking my dog to this groomer for a while now and I have always been impressed with the service. The groomers are skilled and patient, especially with my dog who can be quite nervous around new people. They take their time to make sure my dog looks great and never rush through the process. The prices are also very reasonable. I would definitely recommend this groomer to others!\\nSatisfaction:\\n 1 3 Extract the satisfaction from the comment. Return simple 1 for satisfied customer or 0 for unsatisfied.\\nComment: The team at this grooming salon is fantastic! They always do a wonderful job with my dog's coat, and my dog seems to really enjoy their company. The environment is clean and welcoming. I trust them completely with my pet's grooming needs.\\nSatisfaction:\\n 1 4 Extract the satisfaction from the comment. Return simple 1 for satisfied customer or 0 for unsatisfied.\\nComment:  Please lower the prices.\\nSatisfaction:\\n 0 <p>Upon loading the test dataset, let's extract the inputs and outputs.</p> In\u00a0[\u00a0]: Copied! <pre>prompts = list(data.input)\nsatisfaction = list(data.output)\nprompts_batch = [\"\\n\".join([prompt]) for prompt in prompts]\n</pre> prompts = list(data.input) satisfaction = list(data.output) prompts_batch = [\"\\n\".join([prompt]) for prompt in prompts] <p>We can also print a sample test input and output to better understand how we have extracted the dataset's content.</p> In\u00a0[\u00a0]: Copied! <pre>prompts[0]\n</pre> prompts[0] <pre>'Extract the satisfaction from the comment. Return simple 1 for satisfied customer or 0 for unsatisfied.\\nComment: Long wait times.\\nSatisfaction:\\n'</pre> <p>In this example, the prompt is introduced, followed by the customer's review about long wait times and finally, the satisfaction is 0 to signify a negative review.</p> In\u00a0[\u00a0]: Copied! <pre>satisfaction[0]\n</pre> satisfaction[0] <pre>0</pre> <p>Now that we have the test dataset, let's test the accuracy and F1 score of our tuned model. The F1 score is the mean of the model's precision and recall. We will need the <code>deployment_id</code> to do this. Note, the <code>concurrency_limit</code> is set to 2 to avoid hitting the API's rate limit. This is the number of requests that will be sent in parallel.</p> In\u00a0[\u00a0]: Copied! <pre>deployment_id = deployment_details['metadata']['id']\n\ntuned_model = ModelInference(\n    deployment_id=deployment_id,\n    api_client=client\n)\n\ntuned_model_results = tuned_model.generate_text(prompt=prompts_batch, concurrency_limit=2)\nprint(f'accuracy_score: {accuracy_score(satisfaction, [int(float(x)) for x in tuned_model_results])}, f1_score: {f1_score(satisfaction, [int(float(x)) for x in tuned_model_results])}')\n</pre> deployment_id = deployment_details['metadata']['id']  tuned_model = ModelInference(     deployment_id=deployment_id,     api_client=client )  tuned_model_results = tuned_model.generate_text(prompt=prompts_batch, concurrency_limit=2) print(f'accuracy_score: {accuracy_score(satisfaction, [int(float(x)) for x in tuned_model_results])}, f1_score: {f1_score(satisfaction, [int(float(x)) for x in tuned_model_results])}') <pre>accuracy_score: 0.9827586206896551, f1_score: 0.9827586206896551\n</pre> <p>Given our model's high accuracy and F1 score, let's test the performance of the same Granite model without any tuning.</p> In\u00a0[\u00a0]: Copied! <pre>base_model = ModelInference(\n    model_id=ModelTypes.GRANITE_13B_INSTRUCT_V2,\n    api_client=client\n)\n\nbase_model_results = base_model.generate_text(prompt=prompts_batch, concurrency_limit=2)\n\nprint(f'base model accuracy_score: {accuracy_score(satisfaction, [int(x) for x in base_model_results])}, base model f1_score: {f1_score(satisfaction, [int(x) for x in base_model_results])}')\n</pre> base_model = ModelInference(     model_id=ModelTypes.GRANITE_13B_INSTRUCT_V2,     api_client=client )  base_model_results = base_model.generate_text(prompt=prompts_batch, concurrency_limit=2)  print(f'base model accuracy_score: {accuracy_score(satisfaction, [int(x) for x in base_model_results])}, base model f1_score: {f1_score(satisfaction, [int(x) for x in base_model_results])}') <pre>base model accuracy_score: 0.9310344827586207, base model f1_score: 0.9298245614035088\n</pre> <p>Our tuned model outperforms the pretuned foundation model. Since the tuned model specializes in extracting satisfaction scores, it can be used for other satisfaction-extraction tasks. Great work!</p> <p>In this tutorial, you performed prompt tuning on an IBM Granite model using the watsonx API. Your tuned and deployed model successfully outperformed the foundation model with about 5% greater accuracy.</p>"},{"location":"tutorials/prompt-engineering/prompt-tuning/#prompt-tune-a-granite-model-using-watsonx","title":"Prompt tune a Granite model using watsonx\u00b6","text":"<p>Author: Anna Gutowska</p> <p>In this tutorial, we will prompt tune an IBM\u00ae Granite\u2122 Model using a synthetic dataset containing customer reviews of a dog grooming business.</p> <p>Prompt tuning is an efficient, low-cost way of adapting an artificial intelligence (AI) foundation model to new downstream tasks without retraining the entire model and updating its weights.</p>"},{"location":"tutorials/prompt-engineering/prompt-tuning/#overview-of-llm-optimization","title":"Overview of LLM optimization\u00b6","text":"<p>Foundation models are built on large language models (LLMs) and receive large amounts of training data. Common use cases of foundation models are chatbots and virtual assistants.</p> <p>There are several ways of improving a foundation model's interpretation of input and its quality of responses. To better understand these nuances, let's compare some of the methods.</p> <ul> <li><p>Prompt engineering is the optimization of a pretrained model's responses by providing a well-designed prompt. No new data is introduced using this technique and the model remains as-is. Using this method, the model receives an input prompt and an engineered prompt in front of it. For instance, you can use the prompt: \"Translate English to Spanish,\" with the input: \"good morning.\" This method requires more work from the user. However, this manual human effort to formulate effective prompts helps generative AI models produce task-specific responses without retraining the entire foundation model.</p> </li> <li><p>Fine-tuning large language models involves tuning the same model by providing large numbers of labeled datasets. Fine-tuning alters the model weights and becomes difficult to manage as tasks become diversified. This requires a significant amount of computational resources. In turn, this method tends to have the best accuracy since the model can be trained for very specific use cases.</p> </li> <li><p>Unlike fine-tuning, prompt tuning does not alter the pre-trained model weights. Instead, this technique is parameter-efficient by adjusting prompt parameters to guide the model\u2019s responses in the preferred direction. The model is provided with an input and tunable soft prompts generated by the AI itself. This task-specific context guides the massive model to tailor its responses to a narrow task even with limited data.</p> </li> <li><p>Similarly to prompt tuning, prefix-tuning (link resides outside ibm.com) involves the model receiving several examples of preferred output. The difference here is that a prefix, a series of task-specific vectors, is also included. Prefix-tuning involves both soft prompts and prompts injected into layers of the deep learning model. These so-called \"virtual tokens\" allow the tuned model the flexibility to support a variety of new tasks at once. This method achieves similar performance to fine-tuning all layers and only trains about 0.1% of the parameters. Prefix-tuning even outperforms fine-tuning in low data settings.</p> </li> </ul>"},{"location":"tutorials/prompt-engineering/prompt-tuning/#soft-prompts-versus-hard-prompts","title":"Soft prompts versus hard prompts\u00b6","text":"<p>Hard prompts are user-facing and require user action. A hard prompt can be thought of as a template or instructions for the LLM to generate responses. An example of a hard prompt is introduced next. We encourage you to check out the IBM documentation page for more information on this prompt type and several others.</p>"},{"location":"tutorials/prompt-engineering/prompt-tuning/#prerequisites","title":"Prerequisites\u00b6","text":"<p>You need an IBM Cloud\u00ae account to create a watsonx.ai\u2122 project.</p>"},{"location":"tutorials/prompt-engineering/prompt-tuning/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-tuning/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.</p> <ol> <li><p>Log in to watsonx.ai using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial to implement prompt tuning on your own.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This tutorial is also available on Github.</p>"},{"location":"tutorials/prompt-engineering/prompt-tuning/#step-2-set-up-a-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up a watsonx.ai Runtime instance and API key.\u00b6","text":"<ol> <li><p>Create a watsonx.ai Runtime service instance (select your appropriate region and choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key.</p> </li> <li><p>Associate the watsonx.ai Runtime service instance to the project that you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/prompt-engineering/prompt-tuning/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We'll need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this with a quick pip install.</p>"},{"location":"tutorials/prompt-engineering/prompt-tuning/#step-4-establish-environment-and-import-dataset","title":"Step 4. Establish environment and import dataset\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-tuning/#step-5-tune-the-model","title":"Step 5. Tune the model.\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-tuning/#step-6-evaluate-tuning-results","title":"Step 6. Evaluate tuning results.\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-tuning/#step-7-deploy-the-tuned-model","title":"Step 7. Deploy the tuned model.\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-tuning/#step-8-test-the-tuned-model","title":"Step 8. Test the tuned model.\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-tuning/#summary-and-next-steps","title":"Summary and next steps\u00b6","text":""},{"location":"tutorials/prompt-engineering/prompt-tuning/#try-watsonx-for-free","title":"Try watsonx for free\u00b6","text":"<p>Build an AI strategy for your business on one collaborative AI and data platform called IBM watsonx. Bring together new generative AI capabilities, powered by foundation models and traditional machine learning on a powerful platform that spans the AI lifecycle. With watsonx.ai, you can train, validate, tune and deploy models with ease and build AI applications in a fraction of the time with a fraction of the data.</p> <p>Try watsonx.ai, the next-generation studio for AI builders.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/","title":"Use role prompting with IBM watsonx and Granite","text":"In\u00a0[\u00a0]: Copied! <pre>%pip install -q -U langchain_ibm\n%pip install -q ibm_watsonx_ai\n\nimport getpass\n\nfrom langchain_ibm import WatsonxLLM\nfrom ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams \n</pre> %pip install -q -U langchain_ibm %pip install -q ibm_watsonx_ai  import getpass  from langchain_ibm import WatsonxLLM from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams  In\u00a0[4]: Copied! <pre>credentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \"),\n    \"project_id\": getpass.getpass(\"Please enter your project ID (hit enter): \"),\n}\n</pre> credentials = {     \"url\": \"https://us-south.ml.cloud.ibm.com\",     \"apikey\": getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \"),     \"project_id\": getpass.getpass(\"Please enter your project ID (hit enter): \"), } In\u00a0[5]: Copied! <pre>model = WatsonxLLM(\n\tmodel_id =  \"ibm/granite-3-8b-instruct\",\n\turl = credentials.get(\"url\"),\n\tapikey = credentials.get(\"apikey\"),\n\tproject_id =  credentials.get(\"project_id\"),\n\tparams={\n        GenParams.MAX_NEW_TOKENS: 500,\n        GenParams.MIN_NEW_TOKENS: 1,\n        GenParams.REPETITION_PENALTY: 1.1,\n        GenParams.TEMPERATURE: 0.7, # Adjust for variable responses\n        GenParams.TOP_K: 100,\n        GenParams.TOP_P: 0,\n    },\n)\n</pre> model = WatsonxLLM( \tmodel_id =  \"ibm/granite-3-8b-instruct\", \turl = credentials.get(\"url\"), \tapikey = credentials.get(\"apikey\"), \tproject_id =  credentials.get(\"project_id\"), \tparams={         GenParams.MAX_NEW_TOKENS: 500,         GenParams.MIN_NEW_TOKENS: 1,         GenParams.REPETITION_PENALTY: 1.1,         GenParams.TEMPERATURE: 0.7, # Adjust for variable responses         GenParams.TOP_K: 100,         GenParams.TOP_P: 0,     }, ) In\u00a0[21]: Copied! <pre>def generate_text(prompt):\n    response = None  # Ensure the variable is defined before the try block\n    try:\n        response = model.generate([prompt])\n        return str(response)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        if response:\n            print(f\"Response: {response}\")\n            return None\n    # Define the prompt here\ndefined_prompt = \"You are William Shakespeare, rewrite 'Back in Black' by ACDC as a sonnet.\"\n</pre> def generate_text(prompt):     response = None  # Ensure the variable is defined before the try block     try:         response = model.generate([prompt])         return str(response)     except Exception as e:         print(f\"Error: {e}\")         if response:             print(f\"Response: {response}\")             return None     # Define the prompt here defined_prompt = \"You are William Shakespeare, rewrite 'Back in Black' by ACDC as a sonnet.\"  In\u00a0[\u00a0]: Copied! <pre># Generate and print the text based on the defined prompt\ngenerated_text = generate_text(defined_prompt)\nprint(\"Generated text:\", generated_text)\n</pre> # Generate and print the text based on the defined prompt generated_text = generate_text(defined_prompt) print(\"Generated text:\", generated_text) <p>Here is the model\u2019s output with readable formatting:</p> <p>\"Twinkle, twinkle, little star, how I wonder what you are; Up above the world so high, like a diamond in the sky. In night's velvet cloak, you shine so bright, Guiding sailors through the darkest night.</p> <p>Your radiant beams, a celestial art, Illuminate the canvas of the heart. In silent whispers, secrets you share, Of galaxies far and constellations rare.</p> <p>Yet, mortal eyes can never grasp your form, Beyond our realm, in realms beyond the storm. O, star! O, star! Your light does pierce my soul, A cosmic dance, a timeless role.</p> <p>In dreams, I chase you, in slumber's hold, Your mystery, like an enchanted scroll.</p> <p>But wakefulness claims me, daylight steals your guise, Leaving behind but a memory of your surprise.</p> <p>Thus, twinkling star, in slumber's peaceful reign, I bid you farewell, till morning brings again.\"</p> <p>The model was able to produce a response to the role-based prompt to rewrite the song as a Shakespearian sonnet. The model was able to abide by the rules and style expected from its role prompt.  *Note that because of the creative nature of the prompt, your output may look different than this.</p> In\u00a0[\u00a0]: Copied! <pre>def generate_text(prompt):\n    response = None  # Ensure the variable is defined before the try block\n    try:\n        response = model.generate([prompt])\n        return str(response)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        if response:\n            print(f\"Response: {response}\")\n            return None\n    # Define the prompt here\ndefined_prompt = \" My pet cat has been sneezing a lot lately and is licking her paws what should I do?\"\n\n# Generate and print the text based on the defined prompt\ngenerated_text = generate_text(defined_prompt)\nprint(\"Generated text:\", generated_text)\n</pre> def generate_text(prompt):     response = None  # Ensure the variable is defined before the try block     try:         response = model.generate([prompt])         return str(response)     except Exception as e:         print(f\"Error: {e}\")         if response:             print(f\"Response: {response}\")             return None     # Define the prompt here defined_prompt = \" My pet cat has been sneezing a lot lately and is licking her paws what should I do?\"  # Generate and print the text based on the defined prompt generated_text = generate_text(defined_prompt) print(\"Generated text:\", generated_text) <p>The model responds accordingly with advice and information, however there isn\u2019t a personal touch and isn\u2019t much different from what you\u2019d see on a search engine results page. The model\u2019s output is sort of raw and lacking creativity. This may be an acceptable solution but doesn\u2019t set this veterinarian offices\u2019 virtual assistant apart from the rest. Let\u2019s try the same question again, this time assigning it a role as a \u201ccompassionate, professional, and experienced veterinarian.\u201d</p> In\u00a0[\u00a0]: Copied! <pre>def generate_text(prompt):\n    response = None  # Ensure the variable is defined before the try block\n    try:\n        response = model.generate([prompt])\n        return str(response)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        if response:\n            print(f\"Response: {response}\")\n            return None\n    # Define the prompt here\ndefined_prompt = \"You are a compassionate, professional, and experienced veteraniarian. My pet cat has been sneezing a lot lately and is licking her paws what should I do?\"\n\n# Generate and print the text based on the defined prompt\ngenerated_text = generate_text(defined_prompt)\nprint(\"Generated text:\", generated_text)\n</pre> def generate_text(prompt):     response = None  # Ensure the variable is defined before the try block     try:         response = model.generate([prompt])         return str(response)     except Exception as e:         print(f\"Error: {e}\")         if response:             print(f\"Response: {response}\")             return None     # Define the prompt here defined_prompt = \"You are a compassionate, professional, and experienced veteraniarian. My pet cat has been sneezing a lot lately and is licking her paws what should I do?\"  # Generate and print the text based on the defined prompt generated_text = generate_text(defined_prompt) print(\"Generated text:\", generated_text) <p>The language in the model\u2019s response is more humanized because it speaks to an emotional awareness of the context that the straight-forward system prompt lacked. The model was able to accomplish this while also providing a complete and relevant answer proving that it a more nuanced response. This sort of human interaction with artificial intelligence is a way to meet subjective expectations within organizations and applications.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#use-role-prompting-with-ibm-watsonx-and-granite","title":"Use role prompting with IBM watsonx and Granite\u00b6","text":"<p>Author: Vanna Winland</p> <p>In this tutorial, we will follow step-by-step instructions to perform a prompt engineering technique called role prompting. We will use an IBM Granite Model to assign personas for nuanced model outputs.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#what-is-role-prompting","title":"What is role prompting?\u00b6","text":"<p>Role prompting is a prompt engineering technique that instructs an artificial intelligence (AI) model to take on a specific role or persona when generating a response. This technique can be used to guide the model's tone, style and behavior, which can lead to more engaging outputs.</p> <p>Prompt engineering is about optimizing the model input, so it responds with appropriate, meaningful answers. Zero-shot and few-shot prompting are two popular techniques used to converse with LLMs. LLMs have a natural ability to perform  natural language processing (NLP) tasks due to their ability to process and interpret human language. The language capabilities of AI models are valuable for tasks ranging from chatbot conversations and multiagent interactions to open-ended creative writing.</p> <p>Generative AI gets more personal when a large language model (LLM) is instructed to act as a specific persona to fulfill a role\u2019s specific needs. The AI\u2019s responses can be more accurate and relevant when first prompted with an assigned role. AI models leverage huge datasets so an assigned role can be anything from a teacher, historical figure, salesperson and so on, bound only by what one's imagination can think of. This ability is what makes role prompting, also referred to as persona prompting, such a powerful technique. An AI model\u2019s adaptability makes it a master of disguise, able to generate responses tailored to a user or system\u2019s specific needs.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#how-role-prompting-is-used","title":"How role prompting is used\u00b6","text":"<p>Role prompting can be used to give a chatbot a persona to better interact with users or an AI agent to better interact with other agents. If you\u2019re familiar with prompt templates, you may have already seen role prompting in action. For example, many agentic frameworks use role-playing agents to complete tasks and collaborate effectively. ChatDev uses a role prompting technique called a self-attention mechanism. This mechanism clearly defines the agent\u2019s role which acts as a guideline for its generated outputs.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#prerequisites","title":"Prerequisites\u00b6","text":"<p>To follow this tutorial you need an IBM Cloud account to create a watsonx.ai project.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#steps","title":"Steps\u00b6","text":""},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#step-1-set-up-your-environment","title":"Step 1. Set up your environment\u00b6","text":"<p>While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code, text, images, and data visualizations to formulate a well-formed analysis.</p> <ol> <li><p>Log in to watsonx.ai Runtime using your IBM Cloud account.</p> </li> <li><p>Create a watsonx.ai project.</p> <p>Take note of the project ID in project &gt; Manage &gt; General &gt; Project ID. You\u2019ll need this ID for this tutorial.</p> </li> <li><p>Create a Jupyter Notebook.</p> </li> </ol> <p>This step will open a Notebook environment where you can copy the code from this tutorial to role prompting on your own. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This Jupyter Notebook is available on GitHub.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#step-2-set-up-watsonxai-runtime-instance-and-api-key","title":"Step 2. Set up watsonx.ai Runtime instance and API key\u00b6","text":"<p>In this step, you associate your project with the watsonx.ai service.</p> <ol> <li><p>Create a watsonx.ai Runtime instance (choose the Lite plan, which is a free instance).</p> </li> <li><p>Generate an API Key in watsonx.ai.</p> </li> <li><p>Associate the watsonx.ai Runtime to the project you created in watsonx.ai.</p> </li> </ol>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#step-3-install-and-import-relevant-libraries-and-set-up-your-credentials","title":"Step 3. Install and import relevant libraries and set up your credentials\u00b6","text":"<p>We'll need some libraries and modules for this tutorial. Make sure to import the ones below, and if they're not installed, you can resolve this with a quick pip install.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#step-4-set-up-your-ibm-watsonx-credentials","title":"Step 4. Set up your IBM watsonx credentials\u00b6","text":"<p>Run the following to input and save your watsonx.ai Runtime API key and project id:</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#step-5-set-up-the-model-for-role-prompting","title":"Step 5. Set up the model for role prompting\u00b6","text":"<p>Next, we'll setup IBM's Granite-3.1-8B-Instruct to perform role prompting.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#step-6-turn-song-lyrics-into-sonnets","title":"Step 6.  Turn song lyrics into sonnets\u00b6","text":"<p>To give a simple and fun example of role prompting, let\u2019s ask the model to take on the persona of a famous figure, William Shakespeare. We will ask the model, with its newly assigned role, to rewrite some song lyrics in the style of Shakespeare\u2019s famous sonnets.</p> <p>The code block below sets up and defines the prompt that the model will respond to. Feel free to change the song to whatever you\u2019d like but note that Granite was not trained on copyrighted content. This means that if you pick a song that is not a part of the public domain, for example \u201cEspresso\u201d by Sabrina Carpenter, you\u2019ll have to paste in the lyrics with the prompt as well.</p> <p>For a prompt that just works, this example will use the classic children\u2019s lullaby \u201cTwinkle, Twinkle, Little Star\u201d by Jane Taylor (1806). Run the code block to define your user prompt along with some example input text.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#step-7-generate-a-response","title":"Step 7. Generate a response\u00b6","text":"<p>Once the prompt is defined, we can run the next code block to allow the model to generate an output. Let's hope it's up to our Shakesperian standards.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#step-8-utilize-role-prompting-for-more-empathetic-chatbot-responses","title":"Step 8. Utilize role prompting for more empathetic chatbot responses\u00b6","text":"<p>In this next example, let\u2019s compare a straight-forward system prompt to a role-based system prompt. Let\u2019s say a veterinarian\u2019s office has recently implemented a virtual assistant on their webpage. To provide the best customer support, this office wants their pet owners to feel heard and supported even in their virtual interactions, a relatable goal to many businesses. A visitor may ask a question such as \u201cMy pet cat has been sneezing a lot lately and is licking her paws what should I do?\u201d In this scenario the model has not been assigned a role in its prompt. We\u2019re just using the model out of the box with no input guidance.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#why-is-role-prompting-important","title":"Why is role prompting important?\u00b6","text":"<p>If you are a developer or business looking to add more personalization and meaningful interactions in your genAI applications, consider understanding how role prompting can make an impact. Most modern language models are capable of role prompting. Some basic models may not grasp the nuances of the role or maintain consistency in their responses, while others might be fine-tuned to respond in a certain way. Foundation models like IBM\u2019s Granite series are trained on large amounts of enterprise-specific data that boosts the models\u2019 ability to take on roles to produce tailored responses based on business needs.</p>"},{"location":"tutorials/prompt-engineering/role-prompting-tutorial/#summary","title":"Summary\u00b6","text":"<p>Role prompting encourages the model to perform constantly given its expectations from its assigned persona. We performed a simple example by assigning the LLM with the role of a historical figure in our prompt to turn song lyrics into a sonnet. Next, we compared the output of a non-role prompted model versus a role prompted model for chatbot responses. We concluded by addressing that the response provided by role prompting is more nuanced and supportive in its language, providing elevated customer care.</p>"}]}