{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an AI stylist with IBM Granite using watsonx.ai\n",
    "**Authors:** Anna Gutowska, Ash Minhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you will be guided through how to build a generative AI-powered personal stylist. This tutorial leverages the [IBM Granite™ Vision 3.2](https://www.ibm.com/granite) [large language model (LLM)](https://www.ibm.com/think/topics/large-language-models) for processing image input and [Granite 3.2](https://www.ibm.com/granite) with the latest enhanced reasoning capabilities for formulating customizable outfit ideas.\n",
    "\n",
    "## Introduction\n",
    "How often do you find yourself thinking, “What should I wear today? I don’t even know where to start with picking items from my closet!” This dilemma is one that many of us share. By using cutting-edge [artificial intelligence (AI)](https://www.ibm.com/think/topics/artificial-intelligence) models, this no longer needs to be a daunting task. \n",
    "\n",
    "## AI styling: How it works\n",
    "Our AI-driven solution is composed of the following stages:\n",
    "\n",
    "1. The user uploads images of their current wardrobe or even items in their wishlist, one item at a time.\n",
    "\n",
    "2. The user selects the following criteria:\n",
    "    * Occasion: casual or formal.\n",
    "    * Time of day: morning, afternoon or evening.\n",
    "    * Season of the year: winter, spring, summer or fall.\n",
    "    * Location (for example, a coffee shop).\n",
    "\n",
    "3. Upon submission of the input, the [multimodal](https://www.ibm.com/think/topics/multimodal-ai) Granite Vision 3.2 model iterates over the list of images and returns the following output:\n",
    "    * Description of the item.\n",
    "    * Category: shirt, pants or shoes.\n",
    "    * Occasion: casual or formal.\n",
    "\n",
    "4. The Granite 3.2 model with enhanced reasoning then serves as a fashion stylist. The LLM uses the Vision model’s output to provide an outfit recommendation that is suitable for the user’s event.\n",
    "\n",
    "5. The outfit suggestion, a data frame of items that the user uploaded and the images in the described personalized recommendation are all returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "You need an [IBM Cloud® account](https://cloud.ibm.com/registration) to create a [watsonx.ai™](https://www.ibm.com/products/watsonx-ai) project.\n",
    "\n",
    "# Steps\n",
    "\n",
    "In order to use the watsonx [application programming interface (API)](https://www.ibm.com/think/topics/api), you will need to complete the following steps.\n",
    "\n",
    "## Step 1. Set up your environment\n",
    "\n",
    "1. Log in to [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?context=wx&apps=all) by using your IBM Cloud account.\n",
    "\n",
    "2. Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project).\n",
    "\n",
    "\tYou can get your project ID from within your project. Click the **Manage** tab. Then, copy the project ID from the **Details** section of the **General** page. You need this ID for this tutorial.\n",
    "\n",
    "## Step 2. Set up watsonx.ai Runtime service and API key\n",
    "\n",
    "1. Create a [watsonx.ai Runtime](https://cloud.ibm.com/catalog/services/watsonxai-runtime) service instance (choose the Lite plan, which is a free instance).\n",
    "\n",
    "2. Generate an [API Key](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-authentication.html). \n",
    "\n",
    "3. Associate the watsonx.ai Runtime service to the project that you created in [watsonx.ai](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html?context=cpdaas). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Clone the repository (optional)\n",
    "\n",
    "For a more interactive experience when using this AI tool, clone the [GitHub repository](https://github.com/IBM/ibmdotcom-tutorials) and follow the setup instructions in the README.md file within the AI stylist project to launch the Streamlit application on your local machine. Otherwise, if you prefer to follow along step-by-step, create a Jupyter Notebook and continue with this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Install and import relevant libraries and set up your credentials\n",
    "\n",
    "We need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this issue with a quick pip installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q image ibm-watsonx-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import getpass, os, base64, json\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set our credentials, we need the `WATSONX_APIKEY` and `WATSONX_PROJECT_ID` you generated in step 1. We will also set the URL serving as the API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n",
    "\n",
    "WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n",
    "\n",
    "URL = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `Credentials` class to encapsulate our passed credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = Credentials(\n",
    "    url=URL,\n",
    "    api_key=WATSONX_APIKEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Set up the API request for the Granite Vision model\n",
    "\n",
    "The `augment_api_request_body` function takes the user query and image as parameters and augments the body of the API request. We will use this function in each iteration of inferencing the Vision model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_api_request_body(user_query, image):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\": user_query\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{image}\"\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also instantiate the model interface using the `ModelInference` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelInference(\n",
    "        model_id=\"ibm/granite-vision-3-2-2b\",\n",
    "        credentials=credentials,\n",
    "        project_id=WATSONX_PROJECT_ID,\n",
    "        params={\n",
    "            \"max_tokens\": 400,\n",
    "            \"temperature\": 0       \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Encode images\n",
    "\n",
    "To encode our images in a way that is digestible for the LLM, we will encode them to bytes that we then decode to UTF-8 representation. In this case, our images are located in the local `images` directory. You can find sample images in the AI stylist directory in our [GitHub repository](https://github.com/IBM/ibmdotcom-tutorials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image1.jpeg\n",
      "image12.jpeg\n",
      "image6.jpeg\n",
      "image7.jpeg\n",
      "image13.jpeg\n",
      "image10.jpeg\n",
      "image8.jpeg\n",
      "image4.jpeg\n",
      "image5.jpeg\n",
      "image9.jpeg\n",
      "image11.jpeg\n",
      "image2.jpeg\n",
      "image3.jpeg\n"
     ]
    }
   ],
   "source": [
    "directory = \"images\"  #directory name\n",
    "images = []\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory): \n",
    "  if filename.endswith(\".jpeg\") or filename.endswith(\".png\"): \n",
    "    filepath = directory + '/' +filename\n",
    "    with  open(filepath, \"rb\") as f:\n",
    "      images.append(base64.b64encode(f.read()).decode('utf-8')) \n",
    "    filenames.append(filename)\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Categorize input with the Vision model\n",
    "\n",
    "Now that we have loaded and encoded our images, we can query the Vision model. Our prompt is specific to our desired output to limit the model's creativity as we seek valid JSON output. We will store the description, category and occasion of each image in a list called `closet`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"description\": \"A pair of polished brown leather dress shoes with a brogue detailing on the toe box and a classic oxford design.\",\n",
      "    \"category\": \"shoes\",\n",
      "    \"occasion\": \"formal\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A pair of checkered trousers with a houndstooth pattern, featuring a zippered pocket and a button closure at the waist.\",\n",
      "    \"category\": \"pants\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A light blue, button-up shirt with a smooth texture and a classic collar, suitable for casual to semi-formal occasions.\",\n",
      "    \"category\": \"shirt\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A pair of khaki pants with a buttoned waistband and a button closure at the front.\",\n",
      "    \"category\": \"pants\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A blue plaid shirt with a collar and long sleeves, featuring chest pockets and a button-up front.\",\n",
      "    \"category\": \"shirt\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A pair of bright orange, short-sleeved t-shirts with a crew neck and a simple design.\",\n",
      "    \"category\": \"shirt\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A pair of blue suede sneakers with white laces and perforations, suitable for casual wear.\",\n",
      "    \"category\": \"shoes\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A pair of red canvas sneakers with white laces, isolated on a white background.\",\n",
      "    \"category\": \"shoes\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A pair of grey dress pants with a smooth texture and a classic design, suitable for formal occasions.\",\n",
      "    \"category\": \"pants\",\n",
      "    \"occasion\": \"formal\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A plain white T-shirt with short sleeves and a crew neck, displayed from the front and back.\",\n",
      "    \"category\": \"shirt\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A black short-sleeved t-shirt with a crew neck and a simple design.\",\n",
      "    \"category\": \"shirt\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"Black pants with a zippered pocket and a buttoned fly, showing the waistband and pocket details.\",\n",
      "    \"category\": \"pants\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n",
      "{\n",
      "    \"description\": \"A pair of tan leather boots with a chunky sole and a high-top design, suitable for casual wear.\",\n",
      "    \"category\": \"shoes\",\n",
      "    \"occasion\": \"casual\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_query = \"\"\"Provide a description, category, and occasion for the clothing item or shoes in this image.  \n",
    "\n",
    "                Classify the category as shirt, pants, or shoes.\n",
    "                Classify the occasion as casual or formal.\n",
    "                \n",
    "                Ensure the output is valid JSON. Do not create new categories or occasions. Only use the allowed classifications.\n",
    "                \n",
    "                Your response should be in this schema: \n",
    "                {\n",
    "                    \"description\": \"<description>\",\n",
    "                    \"category\": \"<category>\",\n",
    "                    \"occasion\": \"<occasion>\"\n",
    "                }\n",
    "                \"\"\"\n",
    "\n",
    "image_descriptions = []\n",
    "\n",
    "for i in range(len(images)):\n",
    "    image = images[i]\n",
    "    message = augment_api_request_body(user_query, image)\n",
    "    response = model.chat(messages=message)\n",
    "    result = response['choices'][0]['message']['content']\n",
    "    print(result)\n",
    "    image_descriptions.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Generate outfits with the reasoning model\n",
    "\n",
    "Now that we have each clothing and shoe item categorized, it will be much easier for the reasoning model to generate an outfit for the selected occasion. Let's instantiate and query the reasoning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_model = ModelInference(\n",
    "        model_id=\"ibm/granite-3-2-8b-instruct\",\n",
    "        credentials=credentials,\n",
    "        project_id=WATSONX_PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To align the filenames with the image descriptions, we can enumerate the list of image descriptions and create a list of dictionaries in which we store the description, category, occasion and filename of each item in the respective fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add filenames to the image descriptions\n",
    "closet = []\n",
    "for i, desc in enumerate(image_descriptions):\n",
    "    desc_dict = json.loads(desc)\n",
    "    desc_dict['filename'] = filenames[i]\n",
    "    image_descriptions[i] = json.dumps(desc_dict)\n",
    "\n",
    "closet = [json.loads(js) for js in image_descriptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's query the Granite 3.2 model with reasoning to produce an outfit for our specified criteria using the `closet` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is my thought process: \n",
      "- The outfit needs to be suitable for a casual morning at the park during fall. \n",
      "- I will select one shirt, one pair of pants, and one pair of shoes that fit the 'casual' occasion category. \n",
      "- I will avoid formal or overly dressy items and choose items that are comfortable for park activities.\n",
      "\n",
      "Here is my response:\n",
      "\n",
      "For a casual morning at the park in fall, I suggest the following outfit:\n",
      "\n",
      "1. **Shirt**: A blue plaid shirt with a collar and long sleeves (file: 'image13.jpeg')\n",
      "   - The plaid pattern is classic for fall and goes well with casual park settings. The long sleeves offer some protection against cooler morning temperatures.\n",
      "\n",
      "2. **Pants**: Khaki pants with a buttoned waistband and a button closure at the front (file: 'image7.jpeg')\n",
      "   - Khaki is a versatile choice that can match the casual vibe and also provide a nice balance with the plaid shirt. It's practical and comfortable for walking around.\n",
      "\n",
      "3. **Shoes**: A pair of tan leather boots with a chunky sole and high-top design (file: 'image3.jpeg')\n",
      "   - Tan leather boots offer a stylish yet comfortable option. The chunky sole provides good grip and support, ideal for navigating park trails or uneven ground. \n",
      "\n",
      "This combination provides a relaxed, put-together look suitable for a casual morning outing, while also considering comfort and practicality.\n"
     ]
    }
   ],
   "source": [
    "occasion = input(\"Enter the occasion\")                 #casual or formal (e.g. \"casual\")\n",
    "time_of_day = input(\"Enter the time of day\")           #morning, afternoon or evening (e.g. \"morning\")\n",
    "location = input(\"Enter the location\")                 #any location (e.g. \"park\")\n",
    "season = input(\"Enter the season\")                     #spring, summer, fall or winter (e.g. \"fall\")\n",
    "\n",
    "prompt = f\"\"\"Use the description, category, and occasion of the clothes in my closet to put together an outfit for a {occasion} {time_of_day} at the {location}.\n",
    "                The event takes place in the {season} season. Make sure to return only one shirt, bottoms, and shoes.\n",
    "                Use the description, category, and occasion provided. Do not classify the items yourself. \n",
    "                Include the file name of each image in your output along with the file extension. Here are the items in my closet: {closet}\"\"\"\n",
    "\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"control\",\n",
    "            \"content\": \"thinking\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"{prompt}\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "outfit = reasoning_model.chat(messages=messages)['choices'][0]['message']['content']\n",
    "print(outfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this generated outfit description, we can also display the clothing items that the model recommends! To do so, we can simply extract the filenames. In case the model mentions the same filename twice, it is important to check whether the image has not already been displayed as we iterate the list of images. We can do so by storing displayed images in the `selected_items` list. Finally, we can display the selected items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_items = []\n",
    "\n",
    "#extract the images of clothing that the model recommends\n",
    "for item, uploaded_file in zip(closet, images):\n",
    "    if item['filename'].lower() in outfit.lower() and not any(key['filename'] == item['filename'] for key in selected_items):\n",
    "        selected_items.append({\n",
    "            'image': uploaded_file,\n",
    "            'category': item['category'],\n",
    "            'filename': item['filename']\n",
    "        })\n",
    "\n",
    "#display the selected clothing items\n",
    "if len(selected_items) > 0:\n",
    "    for item in selected_items:\n",
    "        display(Image.open(directory + '/' + item['filename']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you built a system that uses AI to provide style advice to a user's specific event. Using photos or screenshots of the user's clothing, outfits are customized to meet the specified criteria. The Granite-Vision-3-2-2b model was critical for labeling and categorizing each item. Additionally, the Granite-3-2-8B-instruct model leveraged its reasoning capabilities to generate personalized outfit ideas.\n",
    "Some next steps for building off this application can include:\n",
    "- Customizing outfits to a user's personal style, body type, preferred color palette and more.\n",
    "- Broadening the criteria to include jackets and accessories.\n",
    "    - For example, the system might propose a blazer for a user attending a formal conference in addition to the selected shirt, pants and shoes.\n",
    "- Serving as a personal shopper by providing e-commerce product recommendations and pricing that align with the user's unique style and budget.\n",
    "- Adding chatbot functionality to ask the LLM questions about each outfit.\n",
    "- Providing a virtual try-on experience that uses a user selfie to simulate the final look.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
