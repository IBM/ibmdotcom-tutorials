{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gradient boosting classifiers in Scikit-Learn and Caret\n",
        "\n",
        "Gradient boosting is a powerful and widely used machine learning algorithm in data science used for classification tasks. It's part of a family of ensemble learning methods, along with bagging, which combine the predictions of multiple simpler models to improve overall performance. Gradient boosting regression uses gradient boosting to better generate output data based on a linear regression. A gradient boosting classifier, which you’ll explore in this tutorial, uses gradient boosting to better classify input data as belonging to two or more different classes. \n",
        "Gradient boosting is an update of the adaboost algorithm that uses decision stumps rather than trees. These decision stumps are similar to trees in a random forest but they have only one node and two leaves. The gradient boosting algorithm builds models sequentially, each step tries to correct the mistakes of the previous iteration. The training process often begins with creating a weak learner like a shallow decision tree for the training data. After that initial training, gradient boosting computes the error between the actual and predicted values (often called residuals) and then trains a new estimator to predict this error. That new tree is added to the ensemble to update the predictions to create a strong learner. Gradient boosting repeats this process until improvement stops or until a fixed number of iterations has been reached. Boosting itself is similar to gradient descent but “descends” the gradient by introducing new models.\n",
        "Boosting has several advantages: it has good performance on tabular data and it can handle both numerical and categorical data. It works well even with default parameters and is robust to outliers in the dataset. However, it can be slow to train and often highly sensitive to the hyperparameters set for the training process. Keeping the number of trees created smaller can speed up the training process when working with a large dataset. This step is usually done through the max depth parameter. Gradient boosting can also be prone to overfitting if not tuned properly. To prevent overfitting, you can configure the learning rate for the training process. This process is roughly the same for a classifier or a gradient boosting regressor and is used in the popular xgboost, which builds on gradient boosting by adding regularization.\n",
        "\n",
        "In this tutorial, you'll learn how to use two different programming languages and gradient boosting libraries to classify penguins by using the popular [Palmer Penguins](https://www.kaggle.com/datasets/parulpandey/palmer-archipelago-antarctica-penguin-data) dataset.\n",
        "\n",
        "# Step 1 Create a Notebook using R\n",
        "\n",
        "While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.\n",
        "\n",
        "Log in to [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?context=wx&apps=all) by using your IBM Cloud® account.\n",
        "\n",
        "Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project).\n",
        "\n",
        "You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.\n",
        "\n",
        "Create a [Jupyter Notebook](https://www.ibm.com/docs/en/watsonx/saas?topic=editor-creating-managing-notebooks).\n",
        "\n",
        "Make sure to select \"Runtime 24.1 on R 4.3 S (4 vCPU 16 GB RAM)\" when you create the notebook. This step opens a Jupyter Notebook environment where you can copy the code from this tutorial. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the IBM Granite Community. This Jupyter Notebook can be found on GitHub.\n",
        "\n",
        "# Step 3 Configure Libraries and Data\n",
        "\n",
        "In R the caret library is a powerful tool for general data preparation and for model fitting. You'll use it to prepare data and to train the model itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "install.packages('gbm')\n",
        "install.packages('caret')\n",
        "install.packages('palmerpenguins')\n",
        "\n",
        "library(gbm)\n",
        "library(caret)  \n",
        "library(palmerpenguins)\n",
        "\n",
        "head(penguins) # head() returns the top 6 rows of the dataframe\n",
        "summary(penguins) # prints a statistical summary of the data columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The createDataPartition function from the caret package to split the original dataset into a training and testing set and split data into training (70%) and testing set (30%).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "dim(penguins)\n",
        "\n",
        "# get rid of any NA\n",
        "\n",
        "penguins <- na.omit(penguins)\n",
        "parts = caret::createDataPartition(penguins$species, p = 0.7, list = F)\n",
        "\n",
        "train = penguins[parts, ]\n",
        "test = penguins[-parts, ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you're ready to train and test.\n",
        "\n",
        "# Step 4 Train and Test\n",
        "\n",
        "The train method from the caret library uses R formulas, where the dependent variable (often also called a target) is on the left hand side of a tilde '~' and the independent variables (often also called a features) are on the right hand side of the '~'. For instance:\n",
        "\n",
        "```\n",
        "height ~ age\n",
        "```\n",
        "\n",
        "This would predict height based on age.\n",
        "\n",
        "To caret train, you pass the formula, the training data, and the method to be used. The caret library provides methods for many different types of training, so setting the method as \"gbm\"  is where you'll specify to use gradient boosting. The next parameter configures the training process. The \"repeatedcv\" method performs X-fold cross-validation on subsamples of the training set data points. Here, you specify specify 3 repeats of 5-fold cross-validation, using a different set of folds for each cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "model_gbm <- caret::train(\"species ~ .\",\n",
        "                          data = train,\n",
        "                          method = \"gbm\", # gbm for gradient boosting machine\n",
        "                          trControl = trainControl(method = \"repeatedcv\", \n",
        "                                                   number = 5, \n",
        "                                                   repeats = 3, \n",
        "                                                   verboseIter = FALSE),\n",
        "                          verbose = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can use the predictive model to make predictions on test data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "pred_test = caret::confusionMatrix(\n",
        "  data = predict(model_gbm, test),\n",
        "  reference = test$species\n",
        ")\n",
        "\n",
        "print(pred_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will print:\n",
        "\n",
        "```\n",
        "Confusion Matrix and Statistics\n",
        "           Reference\n",
        "Prediction  Adelie Chinstrap Gentoo\n",
        "  Adelie        42         0      0\n",
        "  Chinstrap      0        20      0\n",
        "  Gentoo         1         0     35\n",
        "\n",
        "Overall Statistics\n",
        "                                          \n",
        "               Accuracy : 0.9898          \n",
        "                 95% CI : (0.9445, 0.9997)\n",
        "    No Information Rate : 0.4388          \n",
        "    P-Value [Acc > NIR] : < 2.2e-16       \n",
        "                  Kappa : 0.984           \n",
        "\n",
        " Mcnemar's Test P-Value : NA              \n",
        "\n",
        "Statistics by Class:\n",
        "                     Class: Adelie Class: Chinstrap Class: Gentoo\n",
        "Sensitivity                 0.9767           1.0000        1.0000\n",
        "Specificity                 1.0000           1.0000        0.9841\n",
        "Pos Pred Value              1.0000           1.0000        0.9722\n",
        "Neg Pred Value              0.9821           1.0000        1.0000\n",
        "Prevalence                  0.4388           0.2041        0.3571\n",
        "Detection Rate              0.4286           0.2041        0.3571\n",
        "Detection Prevalence        0.4286           0.2041        0.3673\n",
        "Balanced Accuracy           0.9884           1.0000        0.9921\n",
        "\n",
        "```\n",
        "\n",
        "Due to the nature of cross validation with folds the sensitivity and specificity for each class may be slightly different than what is observed here, although the accuracy will be the same. The accuracy is quite good, even with the Chinstrap penguin, which makes up on 20% of the training dataset.\n",
        "\n",
        "# Step 5 Create a Notebook in Python\n",
        "\n",
        "Now you'll learn how to create a gradient boosting model in Python. In the same project that you created previously, Create a [Jupyter Notebook](https://www.ibm.com/docs/en/watsonx/saas?topic=editor-creating-managing-notebooks). Make sure to create a Jupyter Notebook using Python 3.11 in Watson Studio. Make sure to select \"Runtime 24.1 on Python 3.11 XXS (1 vCPU 4 GB RAM)\" when you create the notebook. You're now ready to create a Gradient Boosting Classifier using Python.\n",
        "\n",
        "# Step 6 Configure Libraries and Data\n",
        "\n",
        "This step install the libraries that you'll use to train and test your Gradient Boosting Classifier. The training itself is done with scikit-learn and the data comes from the palmerpenguins library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install seaborn pandas scikit-learn palmerpenguins\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now install the libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from palmerpenguins import load_penguins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in the R code, there are some NAs in the penguins dataset that need to be removed. This code snippet loads the dataset, removes any NA rows, and then  splits the data into features and target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Load the penguins\n",
        "penguins = load_penguins() #initialize the dataset\n",
        "penguins = penguins.dropna()\n",
        "X = penguins.drop(\"species\", axis=1)\n",
        "y = penguins[\"species\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create a training and testing split of the dataset, with 70% of the data pulled for training and 30% reserved for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   X, y, test_size=0.3, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, you'll gather two lists of the column names, one for the categorical features of X and another for the numerical features, e.g. float64 or int64. Then, use ColumnTransformer from scikit-learn to apply different preprocessing to different column types. A OneHotEncoder will be applied to categorical features to convert them into binary vectors. A StandardScaler will be applied to numerical features to standardize them around a mean f 0 and a variance of 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define categorical and numerical features\n",
        "categorical_features = X.select_dtypes(\n",
        "   include=[\"object\"]\n",
        ").columns.tolist()\n",
        "\n",
        "numerical_features = X.select_dtypes(\n",
        "   include=[\"float64\", \"int64\"]\n",
        ").columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "   transformers=[\n",
        "       (\"cat\", OneHotEncoder(), categorical_features),\n",
        "       (\"num\", StandardScaler(), numerical_features),\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 7 Train and Test\n",
        "\n",
        "Now that you've created the feature sets and the prepocessor, you can create a pipeline to train the model. Other parameters you can configure are max_features, which sets the number of features to consider when looking for the best split. Also the criterion parameter, which measures the quality of a split for training. In this case we’re using the mean squared error with improvement score by [Friedman](https://jerryfriedman.su.domains/ftp/trebst.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(\n",
        "   [\n",
        "       (\"preprocessor\", preprocessor),\n",
        "       (\"classifier\", GradientBoostingClassifier(random_state=42, criterion='friedman_mse', max_features=2)),\n",
        "   ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Next, perform cross-validation to evaluate how well your machine learning pipeline performs on the training data. Calling the fit method of the pipeline you created trains the model. The loss function uses Mean Squared Error or mse by default.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
        "\n",
        "# Fit the model on the training data\n",
        "pipeline.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the model has been trained, predict the test set and check the performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This will print out the following:\n",
        "\n",
        "```\n",
        "Mean Cross-Validation Accuracy: 0.9775\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "      Adelie       1.00      1.00      1.00        31\n",
        "   Chinstrap       1.00      1.00      1.00        18\n",
        "      Gentoo       1.00      1.00      1.00        18\n",
        "    accuracy                           1.00        67\n",
        "   macro avg       1.00      1.00      1.00        67\n",
        "weighted avg       1.00      1.00      1.00        67\n",
        "```\n",
        "\n",
        "This is very close the accuracy reported by the R methods in the first part of this tutorial.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
