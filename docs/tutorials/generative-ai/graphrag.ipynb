{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c429ea3d-5848-451e-b813-c1680b736375",
   "metadata": {},
   "source": [
    "Graph retrieval augmented generation (Graph RAG) is emerging as a powerful technique for generative AI applications to use domain-specific knowledge and relevant information. Graph RAG is an alternative to vector search methods that use a vector database. Knowledge graphs are knowledge systems where graph databases such as Neo4j or Amazon Neptune can represent structured data. In a knowledge graph, the relationships between data points, called edges, are as meaningful as the connections between data points, called vertices or sometimes nodes. A knowledge graph makes it easy to traverse a network and process complex queries about connected data. Knowledge graphs are especially well suited for use cases involving chatbots, identity resolution, network analysis, recommendation engines, customer 360 and fraud detection.\n",
    "\n",
    "A Graph RAG approach leverages the structured nature of graph databases to give greater depth and context of retrieved information about networks or complex relationships. When a graph database is paired with a large language model (LLM), a developer can automate significant parts of the graph creation process from unstructured data like text. An LLM can process text data and identify entities, understand their relationships and represent them in a graph structure.\n",
    "\n",
    "There are many ways to create a Graph RAG application, for instance Microsoft’s GraphRAG, or pairing GPT4 with LlamaIndex. For this tutorial you’ll use Memgraph, an open source graph database solution to create a rag system by using Meta’s Llama-3 on watsonx. Memgraph uses Cypher, a declarative query language. It shares some similarities with SQL but focuses on nodes and relationships rather than tables and rows. You’ll have Llama 3 both create and populate your graph database from unstructured text and query information in the database.\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tTwBKXHIiMg?si=5QmwYF7Aq4ZoL-ny\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742b0da-a33d-481f-afdf-68f4c4467813",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "While you can choose from several tools, this tutorial walks you through\n",
    "how to set up an IBM account to use a Jupyter Notebook.\n",
    "\n",
    "Log in to [watsonx.ai™](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project) using your IBM Cloud® account.\n",
    "\n",
    "Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project).\n",
    "\n",
    "You get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this Project ID for this tutorial.\n",
    "\n",
    "Next, associate your project with the watsonx.ai Runtime\n",
    "\n",
    "a.  Create a [watsonx.ai Runtime](https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-trial) service instance (choose the Lite plan,\n",
    "    which is a free instance).\n",
    "\n",
    "b.  Generate an API Key in watsonx.ai Runtime. Save this API key for use in this tutorial.\n",
    "\n",
    "c.  Go to your project and select the Manage tab\n",
    "\n",
    "d.  In the left tab, select Services and Integrations\n",
    "\n",
    "e.  Select IBM services\n",
    "\n",
    "f.  Select Associate service and pick watsonx.ai Runtime.\n",
    "\n",
    "g.  Associate the watsonx.ai Runtime to the project that you created in watsonx.ai\n",
    "\n",
    "## Step 2\n",
    "\n",
    "Now, you'll need to install Docker from [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)\n",
    "\n",
    "Once you've installed Docker, install Memgraph using their Docker container. On OSX or Linux, you can use this command in a terminal:\n",
    "\n",
    "    curl https://install.memgraph.com | sh\n",
    "\n",
    "On a Windows computer use:\n",
    "\n",
    "    iwr https://windows.memgraph.com | iex\n",
    "\n",
    "Follow the installation steps to get the Memgraph engine and Memgraph lab up and running.\n",
    "\n",
    "## Step 3\n",
    "\n",
    "On your computer, create a fresh virtualenv for this project:\n",
    "\n",
    "    virtualenv kg_rag --python=python3.12\n",
    "\n",
    "In the Python environment for your notebook, install the following Python libraries:\n",
    "\n",
    "    ./kg_rag/bin/pip install langchain langchain-openai langchain_experimental langchain-community==0.3.15 neo4j langchain_ibm jupyterlab json-repair getpass4\n",
    "\n",
    "Now you're ready to connect to Memgraph.\n",
    "\n",
    "## Step 4\n",
    "\n",
    "If you've configured Memgraph to use a username and password, set them here, otherwise you can use the defaults of having neither. It's not good practice for a production database but for a local development environment that doesn't store sensitive data, it's not an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f0c685d-744e-46c3-a605-25796de5c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "from langchain_community.chains.graph_qa.memgraph import MemgraphQAChain\n",
    "from langchain_community.graphs import MemgraphGraph\n",
    " \n",
    "url = os.environ.get(\"MEMGRAPH_URI\", \"bolt://localhost:7687\")\n",
    "username = os.environ.get(\"MEMGRAPH_USERNAME\", \"\")\n",
    "password = os.environ.get(\"MEMGRAPH_PASSWORD\", \"\")\n",
    "\n",
    "#initialize memgraph connection\n",
    "graph = MemgraphGraph(\n",
    "    url=url, username=username, password=password, refresh_schema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91b9b0-dbb9-421f-bcdb-70f5f1ba16dd",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Now create a sample string that describes a dataset of relationships that you can use to test the graph generating capabilities of your LLM system. You can use more complex data sources but this simple example helps us demonstrate the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d6fb1f6-3711-4dcd-9a0b-8a2b80f0fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_text = \"\"\"\n",
    "John's title is Director of the Digital Marketing Group. John works with Jane whose title is Chief Marketing Officer. Jane works in the Executive Group. Jane works with Sharon whose title is the Director of Client Outreach. Sharon works in the Sales Group.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed913bd7-d395-4e30-bb44-682183230689",
   "metadata": {},
   "source": [
    "Enter the watsonx API key that you created in the first step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96305e1-71a8-4c47-990e-8973dda6ad29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "watsonx_api_key = getpass()\n",
    "os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b558ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "watsonx_project_id = getpass()\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = watsonx_project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f43b76-fa8b-4071-98c7-be3f25468a02",
   "metadata": {},
   "source": [
    "Now configure a WatsonxLLM instance to generate text. The temperature should be fairly low and the number of tokens high to encourage the model to generate as much detail as possible without hallucinating entities or relationships that aren't present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1a4ef78-1aeb-4500-abe2-0bb5b7ce2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n",
    "\n",
    "graph_gen_parameters = {\n",
    "    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n",
    "    GenTextParamsMetaNames.MAX_NEW_TOKENS: 1000,\n",
    "    GenTextParamsMetaNames.MIN_NEW_TOKENS: 1,\n",
    "    GenTextParamsMetaNames.TEMPERATURE: 0.3,\n",
    "    GenTextParamsMetaNames.TOP_K: 10,\n",
    "    GenTextParamsMetaNames.TOP_P: 0.8\n",
    "}\n",
    "\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    params=graph_gen_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e420f31-883c-4534-b9c7-b4b32b0405f9",
   "metadata": {},
   "source": [
    "The `LLMGraphTransformer` allows you to set what kinds of nodes and relationships you'd like the LLM to generate. In your case, the text describes employees at a company, the groups they work in and their job titles. Restricting the LLM to just those entities makes it more likely that you'll get a good representation of the knowledge in a graph.\n",
    "\n",
    "The call to `convert_to_graph_documents` has the LLMGraphTransformer create a knowledge graph from the text. This step generates the correct Neo4j syntax to insert the information into the graph database to represent the relevant context and relevant entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e669c775-8c23-4fab-87a3-eca5a64992ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.graph_transformers.llm import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "llm_transformer = LLMGraphTransformer(\n",
    "    llm=watsonx_llm, \n",
    "    allowed_nodes=[\"Person\", \"Title\", \"Group\"],\n",
    "    allowed_relationships=[\"TITLE\", \"COLLABORATES\", \"GROUP\"]\n",
    ")\n",
    "documents = [Document(page_content=graph_text)]\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff09e1-8b45-4e43-a5f2-d235e1f8f6a1",
   "metadata": {},
   "source": [
    "Now clear any old data out of the Memgraph database and insert the new nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17cc5a17-96df-4a5e-bacc-72b642bc55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the database is empty\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_ANALYTICAL\")\n",
    "graph.query(\"DROP GRAPH\")\n",
    "graph.query(\"STORAGE MODE IN_MEMORY_TRANSACTIONAL\")\n",
    " \n",
    "# create knowledge graph\n",
    "graph.add_graph_documents(graph_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff363a-e3b9-4c9b-a96e-f410b2d2c704",
   "metadata": {},
   "source": [
    "The generated Cypher syntax is stored in the `graph_documents` objects. You can inspect it simply by printing it as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eac9334-9630-4de2-94d0-c604aaee5ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GraphDocument(nodes=[Node(id='Director of Client Outreach', type='Title', properties={}), Node(id='Director of the Digital Marketing Group', type='Title', properties={}), Node(id='Sales Group', type='Group', properties={}), Node(id='Sharon', type='Person', properties={}), Node(id='Jane', type='Person', properties={}), Node(id='Chief Marketing Officer', type='Title', properties={}), Node(id='John', type='Person', properties={}), Node(id='Executive Group', type='Group', properties={})], relationships=[Relationship(source=Node(id='John', type='Person', properties={}), target=Node(id='Director of the Digital Marketing Group', type='Title', properties={}), type='TITLE', properties={}), Relationship(source=Node(id='John', type='Person', properties={}), target=Node(id='Jane', type='Person', properties={}), type='COLLABORATES', properties={}), Relationship(source=Node(id='Jane', type='Person', properties={}), target=Node(id='Chief Marketing Officer', type='Title', properties={}), type='TITLE', properties={}), Relationship(source=Node(id='Jane', type='Person', properties={}), target=Node(id='Executive Group', type='Group', properties={}), type='GROUP', properties={}), Relationship(source=Node(id='Jane', type='Person', properties={}), target=Node(id='Sharon', type='Person', properties={}), type='COLLABORATES', properties={}), Relationship(source=Node(id='Sharon', type='Person', properties={}), target=Node(id='Director of Client Outreach', type='Title', properties={}), type='TITLE', properties={}), Relationship(source=Node(id='Sharon', type='Person', properties={}), target=Node(id='Sales Group', type='Group', properties={}), type='GROUP', properties={})], source=Document(metadata={}, page_content=\"\\nJohn's title is Director of the Digital Marketing Group. John works with Jane whose title is Chief Marketing Officer. Jane works in the Executive Group. Jane works with Sharon whose title is the Director of Client Outreach. Sharon works in the Sales Group.\\n\"))]\n"
     ]
    }
   ],
   "source": [
    "print(f\"{graph_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576896bb-faae-4b40-b341-7a4884c20720",
   "metadata": {},
   "source": [
    "The schema and data types created by the Cypher can be seen in the graphs `get_schema` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d69fac0-06ed-4410-81bc-5e27b83f1865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Node labels and properties (name and type) are:\n",
      "- labels: (:Title)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Group)\n",
      "  properties:\n",
      "    - id: string\n",
      "- labels: (:Person)\n",
      "  properties:\n",
      "    - id: string\n",
      "\n",
      "Nodes are connected with the following relationships:\n",
      "(:Person)-[:COLLABORATES]->(:Person)\n",
      "(:Person)-[:GROUP]->(:Group)\n",
      "(:Person)-[:TITLE]->(:Title)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph.refresh_schema()\n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13025776-a70f-4f51-9265-7c5e99886021",
   "metadata": {},
   "source": [
    "You can also see the graph structure in the Memgraph labs viewer:\n",
    "\n",
    "![](graph_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae273e61-a940-486e-a4a2-538e98e0018b",
   "metadata": {},
   "source": [
    "The LLM has done a reasonable job of creating the correct nodes and relationships. Now it's time to query the knowledge graph.\n",
    "\n",
    "## Step 6\n",
    "\n",
    "Prompting the LLM correctly requires some prompt engineering. LangChain provides a FewShotPromptTemplate that can be used to give examples to the LLM in the prompt to ensure that it writes correct and succinct Cypher syntax. The following code gives several examples of questions and queries that the LLM should use. It also shows constraining the output of the model to only the query. An overly chatty LLM might add in extra information that would lead to invalid Cypher queries, so the prompt template asks the model to output only the query itself.\n",
    "\n",
    "Adding an instructive prefix also helps to constrain the model behavior and makes it more likely that the LLM will output correct Cypher syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceac5486-436a-47e7-951a-af7f7a7e00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"<|begin_of_text|>What group is Charles in?<|eot_id|>\",\n",
    "        \"query\": \"<|begin_of_text|>MATCH (p:Person {{id: 'Charles'}})-[:GROUP]->(g:Group) RETURN g.id<|eot_id|>\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"<|begin_of_text|>Who does Paul work with?<|eot_id|>\",\n",
    "        \"query\": \"<|begin_of_text|>MATCH (a:Person {{id: 'Paul'}})-[:COLLABORATES]->(p:Person) RETURN p.id<|eot_id|>\",\n",
    "    },\n",
    "        {\n",
    "        \"question\": \"What title does Rico have?<|eot_id|>\",\n",
    "        \"query\": \"<|begin_of_text|>MATCH (p:Person {{id: 'Rico'}})-[:TITLE]->(t:Title) RETURN t.id<|eot_id|>\",\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"<|begin_of_text|>{query}<|eot_id|>\"\n",
    ")\n",
    "\n",
    "prefix = \"\"\"\n",
    "Instructions: \n",
    "- Respond with ONE and ONLY ONE query.\n",
    "- Use provided node and relationship labels and property names from the\n",
    "schema which describes the database's structure. Upon receiving a user\n",
    "question, synthesize the schema to craft a precise Cypher query that\n",
    "directly corresponds to the user's intent. \n",
    "- Generate valid executable Cypher queries on top of Memgraph database. \n",
    "Any explanation, context, or additional information that is not a part \n",
    "of the Cypher query syntax should be omitted entirely. \n",
    "- Use Memgraph MAGE procedures instead of Neo4j APOC procedures. \n",
    "- Do not include any explanations or apologies in your responses. Only answer the question asked.\n",
    "- Do not include additional questions. Only the original user question.\n",
    "- Do not include any text except the generated Cypher statement.\n",
    "- For queries that ask for information or functionalities outside the direct\n",
    "generation of Cypher queries, use the Cypher query format to communicate\n",
    "limitations or capabilities. For example: RETURN \"I am designed to generate Cypher queries based on the provided schema only.\"\n",
    "\n",
    "Here is the schema information\n",
    "\n",
    "{schema}\n",
    "\n",
    "With all the above information and instructions, generate Cypher query for the\n",
    "user question. \n",
    "\n",
    "The question is:\n",
    "\n",
    "{question}\n",
    "\n",
    "Below are a number of examples of questions and their corresponding Cypher queries.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cypher_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=\"User input: {question}\\nCypher query: \",\n",
    "    input_variables=[\"question\", \"schema\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bbdea-6506-4f35-a9f5-3610e15767ca",
   "metadata": {},
   "source": [
    "Next, you'll create a prompt to control how the LLM answers the question with the information returned from Memgraph. We'll give the LLM several examples and instructions on how to respond once it has context information back from the graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "830f6cae-82cb-45e0-a3c2-ef4e496949a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_examples = [\n",
    "    {\n",
    "        \"question\": \"<|begin_of_text|>What group is Charles in?<|eot_id|>\",\n",
    "        \"context\": \"[{{'g.id': 'Executive Group'}}]\",\n",
    "        \"response\": \"Charles is in the Executive Group<|eot_id|>\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"<|begin_of_text|>Who does Paul work with?<|eot_id|>\",\n",
    "        \"context\": \"[{{'p.id': 'Greg'}}, {{'p2.id': 'Norma'}}]\",\n",
    "        \"response\": \"Paul works with Greg and Norma<|eot_id|>\"\n",
    "    },\n",
    "        {\n",
    "        \"question\": \"What title does Rico have?<|eot_id|>\",\n",
    "        \"context\": \"[{{'t.id': 'Vice President of Sales'}}]\",\n",
    "        \"response\": \"Vice President of Sales<|eot_id|>\"\n",
    "    }\n",
    "]\n",
    "\n",
    "qa_template = \"\"\"\n",
    "Use the provided question and context to create an answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Use only names departments or titles contained within {question} and {context}.\n",
    "\"\"\"\n",
    "\n",
    "qa_example_prompt = PromptTemplate.from_template(\n",
    "    \"\"\n",
    ")\n",
    "\n",
    "qa_prompt = FewShotPromptTemplate(\n",
    "    examples=qa_examples,\n",
    "    prefix=qa_template,\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    example_prompt=qa_example_prompt,\n",
    "    suffix=\" \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074b553-93b9-465a-a010-045b7235adc7",
   "metadata": {},
   "source": [
    "Now it's time to create the question answering chain. The `MemgraphQAChain` allows you to set which LLM you'd like to use, the graph schema to be used and information about debugging. Using a temperature of 0 and a length penalty encourages the LLM to keep the Cypher prompt short and straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4bb6e156-98ef-4425-9109-71b20f0e580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_parameters = {\n",
    "    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n",
    "    GenTextParamsMetaNames.MAX_NEW_TOKENS: 100,\n",
    "    GenTextParamsMetaNames.MIN_NEW_TOKENS: 1,\n",
    "    GenTextParamsMetaNames.TEMPERATURE: 0.0,\n",
    "    GenTextParamsMetaNames.TOP_K: 1,\n",
    "    GenTextParamsMetaNames.TOP_P: 0.9,\n",
    "    GenTextParamsMetaNames.LENGTH_PENALTY: {'decay_factor': 1.2, 'start_index': 20}\n",
    "}\n",
    "\n",
    "chain = MemgraphQAChain.from_llm(\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=\"meta-llama/llama-3-3-70b-instruct\",\n",
    "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        project_id=\"dfe8787b-1f6f-4e18-b36a-e22c00f141d1\",\n",
    "        params=query_gen_parameters\n",
    "    ),\n",
    "    graph = graph,\n",
    "    allow_dangerous_requests = True,\n",
    "    verbose = True,\n",
    "    return_intermediate_steps = True, # for debugging\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    qa_prompt=qa_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60045bd2-dfd3-44a7-8b4b-e61a46b1b7af",
   "metadata": {},
   "source": [
    "Now you can invoke the chain with a natural language question (note that your responses might be slightly different because LLMs are not purely deterministic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7b391e1-286d-4ce1-a4e3-1eaa43dc68ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MemgraphQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m MATCH (p:Person {id: 'John'})-[:TITLE]->(t:Title) RETURN t.id\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'t.id': 'Director of the Digital Marketing Group'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is Johns title?',\n",
       " 'result': ' \\nAnswer: Director of the Digital Marketing Group.',\n",
       " 'intermediate_steps': [{'query': \" MATCH (p:Person {id: 'John'})-[:TITLE]->(t:Title) RETURN t.id\"},\n",
       "  {'context': [{'t.id': 'Director of the Digital Marketing Group'}]}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is Johns title?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08aa4d-50aa-45cb-943b-56a4579a884e",
   "metadata": {},
   "source": [
    "In the next question, ask the chain a slightly more complex question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aad14e76-6409-448b-8910-1f8bd3ce6248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MemgraphQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m MATCH (p:Person {id: 'John'})-[:COLLABORATES]->(c:Person) RETURN c\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'c': {'id': 'Jane'}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Who does John collaborate with?',\n",
       " 'result': ' \\nAnswer: John collaborates with Jane.',\n",
       " 'intermediate_steps': [{'query': \" MATCH (p:Person {id: 'John'})-[:COLLABORATES]->(c:Person) RETURN c\"},\n",
       "  {'context': [{'c': {'id': 'Jane'}}]}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who does John collaborate with?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd2e2a-773a-4106-9c2c-a9dd843bb007",
   "metadata": {},
   "source": [
    "The correct answer is contained in the response. In some cases there may be extra text that you would want to remove before returning the answer to an end user.\n",
    "\n",
    "You can ask the Memgraph chain about Group relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "746790ba-0447-4c70-9b21-3767db88dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MemgraphQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m MATCH (p:Person {id: 'Jane'})-[:GROUP]->(g:Group) RETURN g.id\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'g.id': 'Executive Group'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What group is Jane in?',\n",
       " 'result': 'Jane is in Executive Group.',\n",
       " 'intermediate_steps': [{'query': \" MATCH (p:Person {id: 'Jane'})-[:GROUP]->(g:Group) RETURN g.id\"},\n",
       "  {'context': [{'g.id': 'Executive Group'}]}]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What group is Jane in?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1087c-2d15-45ae-8867-80dbeb202c45",
   "metadata": {},
   "source": [
    "This is the correct answer. \n",
    "\n",
    "Finally, ask the chain a question with two outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1f9adc4-7440-48a3-9370-322ae2a41fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MemgraphQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m MATCH (p:Person {id: 'Jane'})-[:COLLABORATES]->(c:Person) RETURN c\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'c': {'id': 'Sharon'}}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Who does Jane collaborate with?',\n",
       " 'result': ' Jane collaborates with Sharon.',\n",
       " 'intermediate_steps': [{'query': \" MATCH (p:Person {id: 'Jane'})-[:COLLABORATES]->(c:Person) RETURN c\"},\n",
       "  {'context': [{'c': {'id': 'Sharon'}}]}]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Who does Jane collaborate with?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0cd5d-4846-4527-bd44-2d14a1b34a61",
   "metadata": {},
   "source": [
    "The chain correctly identifies both of the collaborators.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this tutorial, you built a Graph RAG application using Memgraph and watsonx to generate the graph data structures and query them. Using an LLM through watsonx you extracted node and edge information from natural language source text and generated Cypher query syntax to populate a graph database. You then used watsonx to turn natural language questions about that source text into Cypher queries that extracted information from the graph database. Using prompt engineering the LLM turned the results from the Memgraph database into natural language responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
