{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tool calling in Large Language Models (LLMs) is the ability of the LLM to interact with external tools, services, or APIs to perform tasks. This  allows LLMs to extend their functionality, enhancing their ability to handle real-world tasks that may require access to external data, real-time information, or specific applications. When an LLM uses a web search tool, it can call the web to fetch real-time data that aren't available in the model's training data. Other types of tools might include Python for calculations, data analysis, or visualization, or calling a service endpoint for data. Tool calling can make a chatbot more dynamic and adaptable, allowing it to provide more accurate, relevant, and detailed responses based on live data or specialized tasks outside its immediate knowledge base. Popular frameworks for tool-calling include Langchain and now ollama.\n",
    "\n",
    "Ollama is a platform that offers open-source, local AI models for use on personal devices so that users can run LLMs directly on their computers. Unlike a service like the OpenAI api, there’s no need for an account since the model is on your local machine. Ollama focuses on privacy, performance, and ease of use, enabling users to access and interact with AI models without sending data to external servers. This can be particularly appealing for those concerned about data privacy or who want to avoid the reliance on external APIs. Ollama’s platform is designed to be easy to set up and use, and it supports various models, giving users a range of tools for natural language processing, code generation, and other AI tasks directly on their own hardware. It is well suited to a tool calling architecture because it can access all the capabilities of a local environment including data, programs, and custom software.\n",
    "\n",
    "In this tutorial you'll learn how to set up tool calling by using ollama to look through a local filesystem, a task which would be difficult to do with a remote LLM. Many ollama models are available for tool calling and building AI agents like Mistral and Llama 3.2, a full list is available at https://ollama.com/library. In this case we'll use IBM Granite 3.3 Dense which has tool support. The 2B and 8B models are text-only dense LLMs trained on designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n",
    "\n",
    "# Step 1\n",
    "\n",
    "First you'll download ollama from https://ollama.com/download and install it for your operating system. On OSX this is done via a .dmg file, on Linux via a single shell command, and on Windows with an installer. You may need admin access on your machine in order to run the installer.\n",
    "\n",
    "You can test that ollama is correctly installed by opening a terminal or command prompt and entering:\n",
    "\n",
    "```\n",
    "ollama -v \n",
    "```\n",
    "\n",
    "# Step 2\n",
    "\n",
    "Next, you'll add the initial imports. This demo will use the ollama python library to communicate with ollama and the pymupdf library to read PDF files in the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the tools\n",
    "\n",
    "Now you'll define the tools that the ollama tools instance will have access. Since the intent of the tools is to read files and look through images in the local file system, you'll create two python functions for each of those tools. The first is called `search_text_files` and it takes a keyword to search for in the local files. For the purposes of this demo, the code only searches for files in a specific folder but it could be extended to include a second parameter that sets which folder the tool will search in.\n",
    "\n",
    "You could use simple string matching to see whether the keyword is in the document but because ollama makes calling local llms easily, `search_text_files` will use Granite 3.3 to determine whether the keyword describes the document text. This is done by reading the document into a string called `document_text`. The function then calls ollama.chat and prompts the model with the following:\n",
    "\n",
    "```\n",
    "\"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + document_text \n",
    "```\n",
    "\n",
    "If the model responds 'yes', then the function returns the name of the file that contains the keyword that the user indicated in the prompt. If none of the files seem to contain the information, then the function returns 'None' as a string.\n",
    "\n",
    "This function may run slowly the first time because ollama will download Granite 3.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_text_files(keyword: str) -> str:\n",
    "  \n",
    "  directory = os.listdir(\"./files/\")\n",
    "  for fname in directory:\n",
    "    \n",
    "    # look through all the files in our directory that aren't hidden files\n",
    "    if os.path.isfile(\"./files/\" + fname) and not fname.startswith('.'):\n",
    "\n",
    "        if(fname.endswith(\".pdf\")):\n",
    "           \n",
    "           document_text = \"\"\n",
    "           doc = pymupdf.open(\"./files/\" + fname)\n",
    "\n",
    "           for page in doc: # iterate the document pages\n",
    "               document_text += page.get_text() # get plain text (is in UTF-8)\n",
    "               \n",
    "           doc.close()\n",
    "\n",
    "           prompt = \"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + document_text \n",
    "\n",
    "           res = ollama.chat(\n",
    "                model=\"granite3.3:2b\",\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "\n",
    "           if 'Yes' in res['message']['content']:\n",
    "                return \"./files/\" + fname\n",
    "\n",
    "        elif(fname.endswith(\".txt\")):\n",
    "\n",
    "            f = open(\"./files/\" + fname, 'r')\n",
    "            file_content = f.read()\n",
    "            \n",
    "            prompt = \"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + file_content \n",
    "\n",
    "            res = ollama.chat(\n",
    "                model=\"granite3.3:2b\",\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "           \n",
    "            if 'Yes' in res['message']['content']:\n",
    "                f.close()\n",
    "                return \"./files/\" + fname\n",
    "\n",
    "  return \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The second tool is called `search_image_files` and it takes a keyword to search for in the local photos. The search is done by using the llava image description model via ollama. This model will return a text description of each image file in the folder and search for the keyword in the description. One of the strengths of using ollama is that multi-agent systems can easily be built to call one model with another. \n",
    "\n",
    "The function returns a string, which is the name of the file whose description contains the keyword that the user indicated in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_image_files(keyword:str) -> str:\n",
    "\n",
    "    directory = os.listdir(\"./files/\")\n",
    "    image_file_types = (\"jpg\", \"png\", \"jpeg\")\n",
    "\n",
    "    for fname in directory:\n",
    "\n",
    "        if os.path.isfile(\"./files/\" + fname) and not fname.startswith('.') and fname.endswith(image_file_types):\n",
    "            res = ollama.chat(\n",
    "                model=\"llava\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': 'Describe this image in short sentences. Use simple phrases first and then describe it more fully.',\n",
    "                        'images': [\"./files/\" + fname]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if keyword in res['message']['content']:\n",
    "                return \"./files/\" + fname\n",
    "    \n",
    "    return \"None\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 4: Define the tools for ollama\n",
    "\n",
    " Now that the functions for ollama to call have been defined, you'll configure the tool information for ollama itself. The first step is to create an object that maps the name of the tool to the functions for ollama function calling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions = {\n",
    "  'Search inside text files':search_text_files,\n",
    "  'Search inside image files':search_image_files\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure a tools array to tell ollama what tools it will have access to and what those tools require. This is an array with one object schema per tool that tells the ollama tool calling framework how to call the tool and what it returns.\n",
    "\n",
    "In the case of both of the tools that you created earlier, they are functions that require a `keyword` parameter. Currently only functions are supported although this may change in the future. The description of the function and of the parameter help the model call the tool correctly. The `description` field for the function of each tool is passed to the LLM when it selects which tool to use. The `description` of the keyword is passed to the model when it generates the parameters that will be passed to the tool. Both of these are places you may look to fine tune prompts when you create your own tool calling applications with ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools don't need to be defined as an object but this helps pass the correct parameters\n",
    "# to the tool call itself by giving the model a prompt of how the tool is to be used\n",
    "ollama_tools=[\n",
    "     {\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'Search inside text files',\n",
    "        'description': 'This tool searches in PDF or plaintext or text files in the local file system for descriptions or mentions of the keyword.',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'keyword': {\n",
    "              'type': 'string',\n",
    "              'description': 'Generate one keyword from the user request to search for in text files',\n",
    "            },\n",
    "          },\n",
    "          'required': ['keyword'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'Search inside image files',\n",
    "        'description': 'This tool searches for photos or image files in the local file system for the keyword.',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'keyword': {\n",
    "              'type': 'string',\n",
    "              'description': 'Generate one keyword from the user request to search for in image files',\n",
    "            },\n",
    "          },\n",
    "          'required': ['keyword'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll use this tools definition when you call ollama with user input.\n",
    "\n",
    "# Step 5: Pass user input to ollama\n",
    "\n",
    "Now its time to pass user input to ollama and have it return the results of the tool calls. First, make sure that ollama is running on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['ollama', 'serve']>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if ollama is not currently running, start it\n",
    "import subprocess\n",
    "subprocess.Popen([\"ollama\",\"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ask the user for input. You can also hardcode the input or retrieve from a chat interface depdending on you configure your application. The `input` function will wait for user input before continuing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about dogs\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "user_input = input(\"What would you like to search for?\")\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the user query is passed to ollama itself. The messages need a role for the user and the content that the user input. This is passed to ollama using the `chat` function. The first parameter is the model you want to use, in this case Granite 3.3, then the message with the user input, and finally the tools array that you configured earlier.\n",
    "\n",
    "The `chat` function will generate an output selecting which tool to use and what parameters should be passed to it in the subsequent tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role': 'user', 'content':user_input}]\n",
    "\n",
    "response: ollama.ChatResponse = ollama.chat(\n",
    "   \n",
    "  # set which model we're using\n",
    "  'granite3.3:2b',\n",
    "\n",
    "  # use the message from the user\n",
    "  messages=messages,\n",
    "\n",
    "  tools=ollama_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has generated tool calls in the output, run all of the tool calls with the parameters that the model generated and check the output. In this application Granite 3.3 is used to generate the final output as well, so the results of the tool calls are added to the initial user input and then passed to the model.\n",
    "\n",
    "Multiple tool calls may return file matches, so the responses are collected in an array which is then passed to Granite 3.3 to generate a response. The prompt that precedes the data instructs the model how to respond:\n",
    "\n",
    "'''\n",
    "If the tool output contains one or more file names, then give the user only the filename found. Do not add additional details. \n",
    "If the tool output is empty ask the user to try again. Here is the tool output: \n",
    "'''\n",
    "\n",
    "The final output is then generated using either the returned file names or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool:  Search inside text files  \n",
      " with arguments:  {'keyword': 'dogs'}\n",
      " Tool response is ./files/File4.pdf\n",
      "Search inside text files  has output:  ['./files/File4.pdf']\n",
      "Calling tool:  Search inside image files  \n",
      " with arguments:  {'keyword': 'dogs'}\n",
      " Tool response is None\n",
      "Final response: The keyword \"dogs\" was found in File4.pdf.\n"
     ]
    }
   ],
   "source": [
    "# this is a place holder that to use to see whether the tools return anything \n",
    "output = []\n",
    "\n",
    "if response.message.tool_calls:\n",
    "  \n",
    "  # There may be multiple tool calls in the response\n",
    "  for tool_call in response.message.tool_calls:\n",
    "\n",
    "    # Ensure the function is available, and then call it\n",
    "    if function_to_call := available_functions.get(tool_call.function.name):\n",
    "      print('Calling tool: ', tool_call.function.name, ' \\n with arguments: ', tool_call.function.arguments)\n",
    "      tool_res = function_to_call(**tool_call.function.arguments)\n",
    "\n",
    "      print(\" Tool response is \" + str(tool_res))\n",
    "\n",
    "      if(str(tool_res) != \"None\"):\n",
    "        output.append(str(tool_res))\n",
    "        print(tool_call.function.name, ' has output: ', output)\n",
    "    else:\n",
    "      print('Could not find ', tool_call.function.name)\n",
    "\n",
    "  # Now chat with the model using the tool call results\n",
    "  # Add the function response to messages for the model to use\n",
    "  messages.append(response.message)\n",
    "\n",
    "  prompt = '''\n",
    "    Give the user the file names found and their search query.\n",
    "    *Important*: Do not add additional details. Do not change the file name given in tool output. \n",
    "    If the tool output is 'None' then apologize and ask the user to try again. \n",
    "    If the tool output contains a file name, output the file name.\n",
    "    Here is the tool output:\n",
    "  '''\n",
    "\n",
    "  messages.append({'role': 'tool', 'content': prompt + \" \" + \", \".join(str(x) for x in output)})\n",
    "  \n",
    "  # Get a response from model with function outputs\n",
    "  final_response = ollama.chat('granite3.3:2b', messages=messages)\n",
    "  print('Final response:', final_response.message.content)\n",
    "\n",
    "else:\n",
    "\n",
    "  # the model wasn't able to pick the correct tool from the prompt\n",
    "  print('No tool calls returned from model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Granite 3.3 picked the correct keyword from the input, 'dogs', and searched through the files in the folder, finding the keyword in a PDF file. Since LLM results are not purely deterministic, you may get slightly different results with the same prompt or very similar prompts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
