{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement function calling with the Granite-3.0-8B-Instruct model in Python with watsonx\n",
    "\n",
    "**Authors:** Erika Russi, Anna Gutowska, Jess Bozorg\n",
    "\n",
    "In this tutorial, you will use the IBM® [Granite-3.0-8B-Instruct model](https://www.ibm.com/granite) now available on watsonx.ai™ to perform custom function calling.  \n",
    "\n",
    "Traditional [large language models (LLMs)](https://www.ibm.com/topics/large-language-models), like the OpenAI GPT-4 (generative pre-trained transformer) model available through ChatGPT, and the IBM Granite™ models that we'll use in this tutorial, are limited in their knowledge and reasoning. They produce their responses based on the data used to train them and are difficult to adapt to personalized user queries. To obtain the missing information, these [generative AI](https://www.ibm.com/topics/generative-ai) models can integrate external tools within the function calling. This method is one way to avoid fine-tuning a foundation model for each specific use-case. The function calling examples in this tutorial will implement external [API](https://www.ibm.com/topics/api) calls. \n",
    "\n",
    "The Granite-3.0-8B-Instruct model and tokenizer use [natural language processing (NLP)](https://www.ibm.com/topics/natural-language-processing) to parse query syntax. In addition, the models use function descriptions and function parameters to determine the appropriate tool calls. Key information is then extracted from user queries to be passed as function arguments. \n",
    "\n",
    "# Steps\n",
    "\n",
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this tutorial is best suited for a Jupyter Notebook. Jupyter Notebooks are widely used within data science to combine code with various data sources such as text, images and data visualizations. \n",
    "\n",
    "This tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.\n",
    "\n",
    "1. Log in to [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?context=wx&apps=all) using your IBM Cloud account.\n",
    "\n",
    "2. Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas).\n",
    "\n",
    "\tYou can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this ID for this tutorial.\n",
    "\n",
    "3. Create a [Jupyter Notebook](https://www.ibm.com/docs/en/watsonx/saas?topic=editor-creating-managing-notebooks).\n",
    "\n",
    "This step opens a notebook environment where you can copy the code from this tutorial. Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the [IBM Granite Community](https://github.com/ibm-granite-community). This tutorial is also available on [Github](https://github.com/IBM/ibmdotcom-tutorials/tree/main/generative-ai/function-calling.ipynb).\n",
    "\n",
    "To avoid Python package dependency conflicts, we recommend setting up a [virtual environment](https://docs.python.org/3/library/venv.html). \n",
    "\n",
    "## Step 2. Set up a Watson Machine Learning service instance and API key\n",
    "\n",
    "1. Create a [Watson Machine Learning](https://cloud.ibm.com/catalog/services/watson-machine-learning) service instance (choose the Lite plan, which is a free instance).\n",
    "\n",
    "2. Generate an [API Key in WML](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-authentication.html). \n",
    "\n",
    "3. Associate the WML service to the project you created in [watsonx.ai](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html). \n",
    "\n",
    "## Step 3. Install and import relevant libraries and set up your credentials\n",
    "\n",
    "We'll need a few libraries and modules for this tutorial. Make sure to import the following ones; if they're not installed, you can resolve this with a quick pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from lomond->ibm-watsonx-ai<2.0.0,>=1.1.14->langchain-ibm) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installations\n",
    "%pip install transformers | tail -n 1\n",
    "%pip install torch torchvision | tail -n 1\n",
    "%pip install langchain-ibm | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import json\n",
    "import requests\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.getcwd()+\"/.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can prepare our environment by setting the model ID for the `granite-3-8b-instruct` model, and the tokenizer for the same Granite model. \n",
    "\n",
    "Replace the placeholder text with the `WATSONX_APIKEY` and `WATSONX_PROJECT_ID` you created in steps 1 and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"ibm/granite-3-8b-instruct\"\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.0-8b-instruct\")\n",
    "\n",
    "WATSONX_URL = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "WATSONX_APIKEY = \"<YOUR_WATSONX_API_KEY_HERE>\"\n",
    "\n",
    "WATSONX_PROJECT_ID = \"<YOUR_WATSONX_PROJECT_ID_HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_stock_price` function in this tutorial requires an `AV_STOCK_API_KEY` key. To generate a free `AV_STOCK_API_KEY`, please visit the [Alpha Vantage website](https://www.alphavantage.co/support/#api-key). \n",
    "\n",
    "Secondly, the `get_current_weather` function requires a `WEATHER_API_KEY`. To generate one, please [create an account](https://home.openweathermap.org/users/sign_up). Upon creating an account, select the \"API Keys\" tab to display your free key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_STOCK_API_KEY = \"<AV_STOCK_API_KEY_HERE>\"\n",
    "\n",
    "WEATHER_API_KEY = \"<WEATHER_API_KEY_HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Define the functions\n",
    "\n",
    "We can now define our functions. In this tutorial, the `get_stock_price` function uses the Stock Market Data API available through Alpha Vantage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price(ticker: str, date: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Retrieves the lowest and highest stock prices for a given ticker and date.\n",
    "    Args:\n",
    "        ticker (str): The stock ticker symbol, e.g., \"IBM\".\n",
    "        date (str): The date in \"YYYY-MM-DD\" format for which you want to get stock prices.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the low and high stock prices on the given date, or (\"none\", \"none\") if not found.\n",
    "    \"\"\"\n",
    "    print(f\"Getting stock price for {ticker} on {date}\")\n",
    "    try:\n",
    "        stock_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={AV_STOCK_API_KEY}\"\n",
    "        stock_data = requests.get(stock_url)\n",
    "        stock_low = stock_data.json()[\"Time Series (Daily)\"][date][\"3. low\"]\n",
    "        stock_high = stock_data.json()[\"Time Series (Daily)\"][date][\"2. high\"]\n",
    "        return stock_low, stock_high\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock data: {e}\")\n",
    "        return \"none\", \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_current_weather` function retrieves the real-time weather in a given location using the Current Weather Data API via [OpenWeather](https://openweathermap.org/api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given location (default: San Francisco).\n",
    "    Args:\n",
    "        location (str): The name of the city for which to retrieve the weather information.\n",
    "    Returns:\n",
    "        dict: A dictionary containing weather information such as temperature, weather description, and humidity.\n",
    "    \"\"\"\n",
    "    print(f\"Getting current weather for {location}\")\n",
    "    \n",
    "    try:\n",
    "        # API request to fetch weather data\n",
    "        weather_url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={WEATHER_API_KEY}&units=metric\"\n",
    "        weather_data = requests.get(weather_url)\n",
    "        data = weather_data.json()\n",
    "        # Extracting relevant weather details\n",
    "        weather_description = data[\"weather\"][0][\"description\"]\n",
    "        temperature = data[\"main\"][\"temp\"]\n",
    "        humidity = data[\"main\"][\"humidity\"]\n",
    "        \n",
    "        # Returning weather details\n",
    "        return {\n",
    "            \"description\": weather_description,\n",
    "            \"temperature\": temperature,\n",
    "            \"humidity\": humidity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return {\"weather\": \"NA\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Set up the API request\n",
    "\n",
    "Now that our functions are defined, we can create a function that generates a watsonx API request for the provided instructions using the watsonx API endpoint. We will use this function each time we make a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(instructions: str) -> dict:\n",
    "    model_parameters = {\n",
    "        \"decoding_method\": \"greedy\",\n",
    "        \"max_new_tokens\": 200,\n",
    "        \"repetition_penalty\": 1,\n",
    "        \"stop_sequences\": [\"\\n\\n\"]\n",
    "    }\n",
    "    model = WatsonxLLM(\n",
    "        model_id=MODEL_ID, \n",
    "        url= WATSONX_URL,\n",
    "        apikey=WATSONX_APIKEY,\n",
    "        project_id=WATSONX_PROJECT_ID,\n",
    "        params=model_parameters\n",
    "    )\n",
    "    response = model.invoke(instructions)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a list of available functions. Here, we declare our function definitions that require the function names, descriptions, parameters and required properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and country code, e.g. San Francisco, US\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_stock_price\",\n",
    "        \"description\": \"Retrieves the lowest and highest stock price for a given ticker symbol and date. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD. It should be used when the user asks about the current or most recent price of a specific stock. It will not provide any other information about the stock or company.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"ticker\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\",\n",
    "                },\n",
    "                \"date\": {\"type\": \"string\", \"description\": \"Date in YYYY-MM-DD format\"},\n",
    "            },\n",
    "            \"required\": [\"ticker\", \"date\"],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Perform function calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6a. Calling the get_stock_price function\n",
    "\n",
    "To prepare for the API requests, we must set our `query` and a JSON list of the available functions for payload used in the tokenizer chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What were the IBM stock prices on October 7, 2024?\"\n",
    "\n",
    "payload = {\n",
    "    \"functions_str\": [json.dumps(x) for x in functions],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'functions_str': ['{\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country code, e.g. San Francisco, US\"}}, \"required\": [\"location\"]}}',\n",
       "  '{\"name\": \"get_stock_price\", \"description\": \"Retrieves the lowest and highest stock price for a given ticker symbol and date. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD. It should be used when the user asks about the current or most recent price of a specific stock. It will not provide any other information about the stock or company.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"ticker\": {\"type\": \"string\", \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"}, \"date\": {\"type\": \"string\", \"description\": \"Date in YYYY-MM-DD format\"}}, \"required\": [\"ticker\", \"date\"]}}']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a chat template is useful for breaking up long strings of texts into one or more messages with corresponding labels. This allows the LLM to process the input in a format that it expects. Because we want our output to be in a string format, we can set the `tokenize` parameter to false. The `add_generation_prompt` can be set to true in order to append the tokens indicating the beginning of an assistant message to the output. This will be useful when generating chat completions with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_of_role|>system<|end_of_role|>You are a helpful assistant with access to the following function calls. Your task is to produce a sequence of function calls necessary to generate response to the user utterance. Use the following function calls as required.{\\'functions_str\\': [\\'{\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country code, e.g. San Francisco, US\"}}, \"required\": [\"location\"]}}\\', \\'{\"name\": \"get_stock_price\", \"description\": \"Retrieves the lowest and highest stock price for a given ticker symbol and date. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD. It should be used when the user asks about the current or most recent price of a specific stock. It will not provide any other information about the stock or company.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"ticker\": {\"type\": \"string\", \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"}, \"date\": {\"type\": \"string\", \"description\": \"Date in YYYY-MM-DD format\"}}, \"required\": [\"ticker\", \"date\"]}}\\']}<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What were the IBM stock prices on October 7, 2024?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\":\"system\",\"content\": f\"You are a helpful assistant with access to the following function calls. Your task is to produce a sequence of function calls necessary to generate response to the user utterance. Use the following function calls as required.{payload}\"},\n",
    "    {\"role\": \"user\", \"content\": query }\n",
    "]\n",
    "\n",
    "instruction_1 = TOKENIZER.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "instruction_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call the `make_api_request` function and pass the instructions we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting IAM Token.\n",
      "Reason: <Response [400]>\n"
     ]
    },
    {
     "ename": "WMLClientError",
     "evalue": "Error getting IAM Token.\nReason: <Response [400]>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWMLClientError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_1 \u001b[38;5;241m=\u001b[39m make_api_request(instruction_1)\n\u001b[1;32m      2\u001b[0m data_1\n",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m, in \u001b[0;36mmake_api_request\u001b[0;34m(instructions)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_api_request\u001b[39m(instructions: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m      2\u001b[0m     model_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoding_method\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreedy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepetition_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m     }\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m WatsonxLLM(\n\u001b[1;32m      9\u001b[0m         model_id\u001b[38;5;241m=\u001b[39mMODEL_ID, \n\u001b[1;32m     10\u001b[0m         url\u001b[38;5;241m=\u001b[39m WATSONX_URL,\n\u001b[1;32m     11\u001b[0m         apikey\u001b[38;5;241m=\u001b[39mWATSONX_APIKEY,\n\u001b[1;32m     12\u001b[0m         project_id\u001b[38;5;241m=\u001b[39mWATSONX_PROJECT_ID,\n\u001b[1;32m     13\u001b[0m         params\u001b[38;5;241m=\u001b[39mmodel_parameters\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m     response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minvoke(instructions)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/langchain_core/load/serializable.py:111\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/langchain_ibm/llms.py:223\u001b[0m, in \u001b[0;36mWatsonxLLM.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m             check_for_attribute(\n\u001b[1;32m    207\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstance_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWATSONX_INSTANCE_ID\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m             )\n\u001b[1;32m    210\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m Credentials(\n\u001b[1;32m    211\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;241m.\u001b[39mget_secret_value() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    212\u001b[0m         api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapikey\u001b[38;5;241m.\u001b[39mget_secret_value() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapikey \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     watsonx_model \u001b[38;5;241m=\u001b[39m ModelInference(\n\u001b[1;32m    224\u001b[0m         model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[1;32m    225\u001b[0m         deployment_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment_id,\n\u001b[1;32m    226\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[1;32m    227\u001b[0m         params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams,\n\u001b[1;32m    228\u001b[0m         project_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_id,\n\u001b[1;32m    229\u001b[0m         space_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspace_id,\n\u001b[1;32m    230\u001b[0m     )\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwatsonx_model \u001b[38;5;241m=\u001b[39m watsonx_model\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/ibm_watsonx_ai/foundation_models/inference/model_inference.py:164\u001b[0m, in \u001b[0;36mModelInference.__init__\u001b[0;34m(self, model_id, deployment_id, params, credentials, project_id, space_id, verify, api_client, validate, persistent_connection)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m credentials:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibm_watsonx_ai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m APIClient\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m APIClient(credentials, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m api_client:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m api_client\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/ibm_watsonx_ai/client.py:373\u001b[0m, in \u001b[0;36mAPIClient.__init__\u001b[0;34m(self, credentials, project_id, space_id, verify, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# For cloud, service_instance.details will be set during space creation( if instance is associated ) or\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# while patching a space with an instance\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_instance: ServiceInstance \u001b[38;5;241m=\u001b[39m ServiceInstance(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvolumes \u001b[38;5;241m=\u001b[39m Volume(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_fm_ga_api:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/ibm_watsonx_ai/service_instance.py:56\u001b[0m, in \u001b[0;36mServiceInstance.__init__\u001b[0;34m(self, client)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# This is used in connections.py\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_href_definitions \u001b[38;5;241m=\u001b[39m HrefDefinitions(\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mCLOUD_PLATFORM_SPACES,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mPLATFORM_URL,\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mICP_PLATFORM_SPACES,\n\u001b[1;32m     54\u001b[0m )\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_token()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mproceed:  \u001b[38;5;66;03m# there is no 'token' in credentials\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_expiration_datetime() \u001b[38;5;241m-\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/ibm_watsonx_ai/service_instance.py:237\u001b[0m, in \u001b[0;36mServiceInstance._get_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_token\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_token()\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_token_refresh_possible():\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mICP_PLATFORM_SPACES:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/ibm_watsonx_ai/service_instance.py:273\u001b[0m, in \u001b[0;36mServiceInstance._create_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mCLOUD_PLATFORM_SPACES:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_is_IAM():\n\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_IAM_token()\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m WMLClientError(\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key for IAM token is not provided in credentials for the client.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/ibm_watsonx_ai/service_instance.py:345\u001b[0m, in \u001b[0;36mServiceInstance._get_IAM_token\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expiration_datetime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WMLClientError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError getting IAM Token.\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "\u001b[0;31mWMLClientError\u001b[0m: Error getting IAM Token.\nReason: <Response [400]>"
     ]
    }
   ],
   "source": [
    "data_1 = make_api_request(instruction_1)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appropriate `get_stock_price` tool use was selected from the set of functions. To run the function, let's extract relevant arguments from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m function_call \u001b[38;5;241m=\u001b[39m ast\u001b[38;5;241m.\u001b[39mliteral_eval(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m.+})\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_1)\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      2\u001b[0m function_call\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_1' is not defined"
     ]
    }
   ],
   "source": [
    "function_call = ast.literal_eval(re.search(\"({.+})\", data_1).group(0))\n",
    "function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function name, ticker and date extracted, we can set these variables and call the function. To call the function using its name as a string, we can use the `globals()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting stock price for IBM on 2024-10-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('225.0200', '227.6700')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_name = function_call[\"name\"]\n",
    "ticker = function_call[\"arguments\"][\"ticker\"]\n",
    "date = function_call[\"arguments\"][\"date\"]\n",
    "stock_info = globals()[function_name](ticker, date)\n",
    "stock_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function successfully retrieved the requested stock price. To generate a synthesized final response, we can pass another prompt to the Granite model along with the information collected from function calling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The stock price for IBM on 2024-10-07 was between 225.02 USD and 227.67 USD.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_2 = f\"SYSTEM: You are a helpful assistant. Answer this stock information in USD.\\n\\nUSER: Here is the stock price for {ticker} on {date}:{stock_info}\\nASSISTANT:\"\n",
    "data_2 = make_api_request(instruction_2)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6b. Calling the get_current_weather function\n",
    "\n",
    "As our next query, let’s inquire about the current weather in San Francisco. We can follow the same steps as in Step 5a by adjusting the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_of_role|>system<|end_of_role|>You are a helpful assistant with access to the following function calls. Your task is to produce a sequence of function calls necessary to generate response to the user utterance. Use the following function calls as required.{\\'functions_str\\': [\\'{\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and country code, e.g. San Francisco, US\"}}, \"required\": [\"location\"]}}\\', \\'{\"name\": \"get_stock_price\", \"description\": \"Retrieves the lowest and highest stock price for a given ticker symbol and date. The ticker symbol must be a valid symbol for a publicly traded company on a major US stock exchange like NYSE or NASDAQ. The tool will return the latest trade price in USD. It should be used when the user asks about the current or most recent price of a specific stock. It will not provide any other information about the stock or company.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"ticker\": {\"type\": \"string\", \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"}, \"date\": {\"type\": \"string\", \"description\": \"Date in YYYY-MM-DD format\"}}, \"required\": [\"ticker\", \"date\"]}}\\']}<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What is the current weather in San Francisco?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the current weather in San Francisco?\"\n",
    "\n",
    "chat = [\n",
    "    {\"role\":\"system\",\"content\": f\"You are a helpful assistant with access to the following function calls. Your task is to produce a sequence of function calls necessary to generate response to the user utterance. Use the following function calls as required.{payload}\"},\n",
    "    {\"role\": \"user\", \"content\": query }\n",
    "]\n",
    "\n",
    "instruction_1 = TOKENIZER.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "instruction_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'function_calls': [{'name': 'get_current_weather', 'arguments': {'location': 'San Francisco, US'}}]}\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = make_api_request(instruction_1)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function_calls': [{'name': 'get_current_weather',\n",
       "   'arguments': {'location': 'San Francisco, US'}}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_call = ast.literal_eval(re.search(\"({.+})\", data_1).group(0))\n",
    "function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the model decides the appropriate function, in this case `get_current_weather`, and extracts the location correctly. Now, let's call the function with the argument generated by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current weather for San Francisco, US\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'few clouds', 'temperature': 16.97, 'humidity': 43}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_name = function_call['function_calls'][0]['name']\n",
    "location = function_call['function_calls'][0][\"arguments\"][\"location\"]\n",
    "weather_info = globals()[function_name](location)\n",
    "weather_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function response correctly describes the current weather in San Francisco. Lastly, let's generate a synthesized final response with the results of this function call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The current weather in San Francisco, US is few clouds, with a temperature of 16.97 and humidity of 43.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction_2 = f\"SYSTEM: You are a helpful assistant. Synthesize this weather information.\\n\\nUSER: Here is the current weather in {location}: {weather_info}\\nASSISTANT:\"\n",
    "data_2 = make_api_request(instruction_2)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you built custom functions and used the Granite-3.0-8B-Instruct model to determine which function to call based on  key information from user queries. With this information, you called the function with the arguments as stated in the model response. These function calls produce the expected output. Finally, you called the Granite-3.0-8B-Instruct model again to synthesize the information returned by the functions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
