{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fd20dcc2-b7e4-4634-9900-16246039d724"
   },
   "source": [
    "# Use automatic speech recognition (ASR) to generate a podcast transcript using Granite 3.3 and watsonx.ai\n",
    "\n",
    "In this tutorial, you will use the [open source IBM® Granite® 3.3 speech model](https://huggingface.co/ibm-granite/granite-speech-3.3-8b) to generate an IBM [\"Mixture of Experts\"](https://www.ibm.com/think/podcasts/mixture-of-experts) podcast transcript from YouTube. Then, using the [open source IBM Granite-3.3-8B-Instruct large language model (LLM)](https://huggingface.co/ibm-granite/granite-3.3-8b-instruct), you will output a summary of the generated transcript. You will run this code on a watsonx.ai® notebook.\n",
    "\n",
    "## Automatic speech recognition\n",
    "\n",
    "Automatic speech recognition (ASR) also known as [speech recognition](https://www.ibm.com/think/topics/speech-recognition) or [speech-to-text](https://www.ibm.com/think/topics/speech-to-text), is the technology that converts spoken language into written text. Various [machine learning algorithms](https://www.ibm.com/think/topics/machine-learning-algorithms) and [artificial intelligence](https://www.ibm.com/think/topics/artificial-intelligence) computation techniques are used to convert speech into text. Speech recognition technology has evolved significantly from its beginnings in the mid-twentieth century to today.\n",
    "\n",
    "In the 1960s, spectrograms were initially used to analyze speech. In the subsequent decades, a shift to statistical models occurred. Hidden Markov Models (HMMs) appeared and became dominant for modeling sequences of small sound units known as phonemes in linguistics. ASR systems architecture was made up of three separate components: an acoustic model, a language model and a decoder.\n",
    "\n",
    "By the 2010s, advancements in [deep learning](https://www.ibm.com/think/topics/deep-learning) began impacting the traditional speech recognition systems architecture. [Encoder-decoder models](https://www.ibm.com/think/topics/encoder-decoder-model) might use a [recurrent neural network (RNN)](https://www.ibm.com/think/topics/recurrent-neural-networks) or a [convolutional neural network (CNN)](https://www.ibm.com/think/topics/convolutional-neural-networks) architecture where an encoder processes input data and a decoder generates output based on the encoder's representation. Models can be trained on large unlabeled datasets of audio-text pairs to learn how to correspond audio signals with transcriptions. Popular ASR models include DeepSpeech and Wav2Vec.\n",
    "\n",
    "Today, virtual assistants such as Apple’s Siri, Amazon’s Alexa or Microsoft’s Cortana use ASR technology to process real-time human speech. They are also able to integrate speech-to-text with [large language models (LLMs)](https://www.ibm.com/think/topics/large-language-models) and [natural language processing (NLP)](https://www.ibm.com/think/topics/natural-language-processing). LLMs  can be used to add context, which can help when word choices are more ambiguous or if there is variability in human speech patterns.\n",
    "\n",
    "# Prerequisites\n",
    "You need an [IBM Cloud® account](https://cloud.ibm.com/registration) to create a [watsonx.ai](https://www.ibm.com/products/watsonx-ai) project.\n",
    "\n",
    "# Steps\n",
    "\n",
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook. \n",
    "\n",
    "1. Log in to [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?context=wx&apps=all) by using your IBM Cloud account.\n",
    "\n",
    "2. Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project).\n",
    "\n",
    "3. Create a [Jupyter Notebook](https://www.ibm.com/docs/en/watsonx/saas?topic=editor-creating-managing-notebooks).\n",
    "\n",
    "Make sure that you choose `GPU 2xV100 Runtime 24.1` to define the configuration. This step opens a Jupyter Notebook environment where you can copy the code from this tutorial. \n",
    "\n",
    "Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. This tutorial is available on [GitHub](https://github.com/IBM/ibmdotcom-tutorials). To view more Granite tutorials, check out the [IBM Granite Community](https://github.com/ibm-granite-community).\n",
    "\n",
    "## Step 2. Install and import relevant libraries\n",
    "\n",
    "We have a few dependencies for this tutorial. Make sure to import the following packages; if they're not installed, you can resolve this issue with a quick pip installation.\n",
    "\n",
    "If you receive a \"pip dependency resolver\" error related to the `caikit-nlp` package, you can ignore it for now as the rest of the notebook should still be able to run normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1164471e-5132-4dce-b234-af97a9d4d9b6"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "! pip install -q peft torchaudio soundfile pytubefix pytube moviepy tqdm https://github.com/huggingface/transformers/archive/main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8718adec-56b5-4427-915f-a252ff0d2625"
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import json\n",
    "import os\n",
    "\n",
    "from pytubefix import YouTube\n",
    "from tqdm import tqdm\n",
    "from moviepy.audio.io.AudioFileClip import AudioFileClip\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Download the podcast audio from YouTube\n",
    "\n",
    "In this tutorial, we will use the latest episode of the IBM \"Mixture of Experts\" podcast, \"AI on IBM z17, Meta's Llama 4 and Google Cloud Next 2025\". The podcast is hosted on [YouTube](https://www.youtube.com/watch?v=90fUR1PQgt4). We'll first create a `YouTube` object and use the `streams.filter(only_audio=True)` method to capture only the raw audio. From there, we'll extract the audio from the video and save it as an M4A audio file, `out_file`. `base` is the full file name, including the directory in which the file will be saved without the `m4a` extension. We'll use the `base` variable later when we convert the audio format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05b47edc-b6ac-4908-8374-5db73880093f"
   },
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=90fUR1PQgt4\" #latest episode 37 minutes\n",
    "\n",
    "# Create a YouTube object\n",
    "yt = YouTube(url)\n",
    "\n",
    "# Download only the audio stream from the video\n",
    "video = yt.streams.filter(only_audio=True).first()\n",
    "\n",
    "# Save the audio to a file\n",
    "out_file = video.download()\n",
    "\n",
    "# Get the base name and extension of the downloaded audio\n",
    "base = os.path.splitext(out_file)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f9f539d2-0dca-4eea-92be-436d34795a97"
   },
   "source": [
    "## Step 4: Prepare the podcast audio file for model inference\n",
    "\n",
    "We'll need to make a couple of modifications to the podcast audio file before we can use it for model inference. \n",
    "\n",
    "First, we need to convert the M4A file to a WAV file to use it with the Granite Speech model. We will use the moviepy library to do this conversion. We can use the `base` variable that we defined earlier to create the new file name with the `.wav` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b04565a-f9e5-44f5-96d9-204233a36a15"
   },
   "outputs": [],
   "source": [
    "# Load the M4A file\n",
    "audio_clip = AudioFileClip(out_file)\n",
    "\n",
    "# Write the audio to a WAV file\n",
    "audio_clip.write_audiofile(base+\".wav\")\n",
    "\n",
    "# Close the audio clip\n",
    "audio_clip.close()\n",
    "\n",
    "audio_path = base+\".wav\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use `torchaudiodio.load()` to load the audio file as a tensor and extract the sample rate. \n",
    "\n",
    "We'll also need to convert the returned waveform from stereo sound to mono sound. We can do this by taking the average of the stereo sound channels by using `torch.mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2241b6ac-f0dd-4f48-bdc3-6c78d10f0db4"
   },
   "outputs": [],
   "source": [
    "#Resulting waveform and sample rate\n",
    "waveform, sample_rate = torchaudio.load(audio_path, normalize=True)\n",
    "\n",
    "# convert from stereo to mono\n",
    "mono_waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "# confirm the waveform is mono\n",
    "assert mono_waveform.shape[0] == 1 # mono"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to resample the mono waveform to the model's sample rate: 16 khz. We can use torchaudio’s resampling API to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6c99b2a-1817-4521-a851-1ae411ed2d97"
   },
   "outputs": [],
   "source": [
    "# Resample the mono waveform to the model's sample rate\n",
    "resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "resampled_waveform = resample_transform(mono_waveform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can split the resampled waveform into chunks of equal size to feed into the model for easier inference. \n",
    "\n",
    "We'll use `torch.split()` to split the full resampled waveform into chunks of 30 seconds and a chunk size sample equal to 30 seconds * 16 khz. This step will give us a list of waveforms, `chunks`, each with 30 seconds of audio data. We will feed each chunk into the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a812107e-e1bd-4546-81a8-f04e620ac147"
   },
   "outputs": [],
   "source": [
    "# Define the desired chunk size\n",
    "chunk_size_seconds = 30 \n",
    "chunk_size_samples = chunk_size_seconds * 16000\n",
    "\n",
    "# Split the waveform into chunks of equal size\n",
    "chunks = torch.split(resampled_waveform, chunk_size_samples, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load and instantiate the Granite speech model\n",
    "\n",
    "Now we can start instantiating our speech model. \n",
    "\n",
    "We will first set our torch device to CPU. If device is set to GPU, you might encounter out of memory errors when running this notebook, but CPU should work just fine on your watsonx.ai notebook. We can then set up our processor and tokenizer for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b55f1743-55b0-48b2-a426-d8cbeac09080"
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model_name = \"ibm-granite/granite-speech-3.3-8b\"\n",
    "speech_granite_processor = AutoProcessor.from_pretrained(\n",
    "    model_name, trust_remote_code=True)\n",
    "tokenizer = speech_granite_processor.tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running your notebook on the watsonx.ai platform, you may also need to run the following code to manually edit the `adapter_config.json` file. This will avoid an error when loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fdf974d-892c-4961-abe0-e76591127506"
   },
   "outputs": [],
   "source": [
    "adapter_config_file = hf_hub_download(model_name, 'adapter_config.json')\n",
    "\n",
    "#load the existing config file and print it\n",
    "with open(adapter_config_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(\"Existing config file:\", data)\n",
    "\n",
    "\n",
    "#remove key, value pairs in config file throwing error\n",
    "keys_to_delete = ['layer_replication', 'loftq_config', 'megatron_config', 'megatron_core', 'use_dora', 'use_rslora']\n",
    "\n",
    "for key in keys_to_delete:\n",
    "    if key in data:\n",
    "        del data[key]\n",
    "\n",
    "# write the updated config file back to disk\n",
    "with open(adapter_config_file, 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "with open(adapter_config_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "#verify keys were removed\n",
    "print(\"Corrected config file:\", data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we can finally load the model! We'll use `AutoModelForSpeechSeq2Seq` from the `transformers` library and the `from_pretrained` method to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2f2306e-d6b2-4cd5-bcc5-47313cfe38e1"
   },
   "outputs": [],
   "source": [
    "speech_granite = AutoModelForSpeechSeq2Seq.from_pretrained(model_name, trust_remote_code=True).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create an ASR system with the Granite speech model\n",
    "\n",
    "Now that we have the model loaded and the audio data prepared, we can use it to generate text from speech. \n",
    "\n",
    "We'll start by creating a prompt for the model to transcribe the audio data. We'll use `tokenizer.apply_chat_template()` to convert the prompt into a format that can be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaf8351f-90cd-4fa6-b607-783c8b274310"
   },
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"Knowledge Cutoff Date: April 2025.\\nToday's Date: April 16, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<|audio|>can you transcribe the speech into a written format?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    chat, tokenize=False, add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can set up an empty list `generated_texts`, to gather the generated text from each chunk of audio input. \n",
    "\n",
    "We set up a `for` loop to iterate through each audio chunk and pass it to the model for generation. Here, we will also track the progress of the loop by using a `tqdm` progress bar.\n",
    "\n",
    "The model inputs are created through the `speech_granite_processor` that we established earlier. The processor takes the `text` and `chunk` as input and returns a processed version of the audio data for the model to use.\n",
    "\n",
    "The model outputs are produced by using the speech model's `generate` method. From there, we use the `tokenizer` to convert the model outputs into human-readable text and store each chunk's transcription into our `generated_texts` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "927b05d5-fdd8-4cf7-b8cb-648369168ca6"
   },
   "outputs": [],
   "source": [
    "generated_texts = []\n",
    "\n",
    "for chunk in tqdm(chunks, desc=\"Generating transcript...\"):\n",
    "\n",
    "    model_inputs = speech_granite_processor(\n",
    "        text,\n",
    "        chunk,\n",
    "        device=device, # Computation device; returned tensors are put on CPU\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    model_outputs = speech_granite.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1000,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        min_length=1,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=1.0,\n",
    "        length_penalty=1.0,\n",
    "        temperature=1.0,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,)\n",
    "\n",
    "    num_input_tokens = model_inputs[\"input_ids\"].shape[-1]\n",
    "    new_tokens = torch.unsqueeze(model_outputs[0, num_input_tokens:], dim=0)\n",
    "\n",
    "    output_text = tokenizer.batch_decode(\n",
    "        new_tokens, add_special_tokens=False, skip_special_tokens=True)[0]\n",
    "\n",
    "    generated_texts.append(output_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the chunk transcripts are currently individual strings in a list, we'll join the strings together with a space in between to make one cohesive full transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "806d7e61-0977-4a62-adc3-7087ca9bf989"
   },
   "outputs": [],
   "source": [
    "full_transcript = \" \".join(generated_texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Use the Granite instruct model for summarization\n",
    "\n",
    "Now that we have a full transcript, we'll use the same model to summarize it. We can access the Granite-3.3-8B-Instruct model directly from Granite-speech-3.3-8b by simply calling it with a text prompt that doesn't contain the `<|audio|>` token. \n",
    "\n",
    "We'll set up a new prompt to instruct this model to generate a summary of the full transcript. We can use `tokenizer.apply_chat_template()` again to convert the prompt for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf632b10-c259-48a8-b727-f41ccca4947e"
   },
   "outputs": [],
   "source": [
    "conv = [{\"role\": \"user\", \n",
    "         \"content\": f\"Compose a single, unified summary of the following transcript. Your response should only include the unified summary. Do not provide any further explanation. Transcript:{full_transcript}\"}]\n",
    "\n",
    "text = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `speech_granite_processor` again to create out model inputs, but we won't pass in any audio file this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = speech_granite_processor(\n",
    "    text,\n",
    "    device=device, # Computation device; returned tensors are put on CPU\n",
    "    return_tensors=\"pt\",\n",
    ").to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will receive output from `speech_granite.generate()` as a tensor. We can convert this output to text by using `tokenizer.decode()`. And print our final summary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06d50682-8f24-430b-9a51-777b6c7cace6"
   },
   "outputs": [],
   "source": [
    "output = speech_granite.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens= 2000, # concise summary\n",
    ")\n",
    "\n",
    "summary = tokenizer.decode(output[0, model_inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you downloaded an English audio file from YouTube. You transformed the audio file for consumption by the Granite speech model, generated a full transcript of the audio and used a Granite instruct model to generate a summary of the transcript."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
