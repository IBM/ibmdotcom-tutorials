{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a LangChain RAG system in Python with watsonx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contributors**: Nicholas Moss, Erika Russi\n",
    "\n",
    "In this tutorial, we’ll use LangChain to walk through a step-by-step simple Retrieval Augmented Generation ([RAG](https://research.ibm.com/blog/retrieval-augmented-generation-RAG)) example in Python. RAG is a technique in natural language processing (NLP) that combines information retrieval and generative models to produce more accurate, relevant and contextually aware responses. \n",
    "\n",
    "For our use case, we’ll set up a RAG system for various IBM webpages related to the company's technology, products and offerings. The fetched content from these ibm.com websites will make up our knowledge base. From this knowledge base, we will then provide context to an LLM so it can answer some questions about IBM products. \n",
    "\n",
    "# More about RAG and LangChain\n",
    "\n",
    "In traditional language generation tasks, [large language models](https://www.ibm.com/topics/large-language-models) (LLMs) like OpenAI’s GPT-3.5 (Generative Pre-trained Transformer) or [IBM’s Granite Models](https://www.ibm.com/granite) are used to construct responses based on an input prompt. However, these models may struggle to produce responses that are contextually relevant, factually accurate or up to date. The models may not know the latest information on IBM products. To tackle this knowledge gap, we can use methods such as [fine-tuning](https://www.ibm.com/topics/fine-tuning) or continued pre-training, but both can be expensive. Instead, we can use RAG to leverage a knowledge base of existing content. \n",
    "\n",
    "RAG applications address the knowledge gap limitation by incorporating a retrieval step before response generation. During retrieval, [vector search](https://www.ibm.com/topics/vector-search) can be used to identify contextually pertinent information, such as relevant information or documents from a large corpus of text, typically stored in a [vector database](https://www.ibm.com/topics/vector-database). Finally, an LLM is used to generate a response based on the retrieved context.\n",
    "\n",
    "LangChain is a powerful, open-source framework that facilitates the development of applications using LLMs for various NLP tasks. In the context of RAG, LangChain plays a critical role by combining the strengths of retrieval-based methods and generative models to enhance the capabilities of NLP systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "You need an [IBM Cloud account](https://cloud.ibm.com/registration?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-implement-xgboost-in-python&cm_sp=ibmdev-_-developer-_-trial) to create a [watsonx.ai](https://www.ibm.com/products/watsonx-ai?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-implement-xgboost-in-python&cm_sp=ibmdev-_-developer-_-product) project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "## Step 1. Set up your environment\n",
    "\n",
    "While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook. \n",
    "\n",
    "1. Log in to [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?context=wx&apps=all) using your IBM Cloud account.\n",
    "\n",
    "2. Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project).\n",
    "\n",
    "\tYou can get your project ID from within your project. Click the **Manage** tab. Then, copy the project ID from the **Details** section of the **General** page. You need this ID for this tutorial.\n",
    "\n",
    "3. Create a [Jupyter Notebook](https://www.ibm.com/docs/en/watsonx/saas?topic=editor-creating-managing-notebooks).\n",
    "\n",
    "This step will open a Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the [IBM Granite Community](https://github.com/ibm-granite-community). This tutorial is also available on [Github](https://github.com/IBM/ibmdotcom-tutorials/tree/main/generative-ai/langchain-rag.ipynb).\n",
    "\n",
    "## Step 2. Set up a Watson Machine Learning (WML) service instance and API key.\n",
    "\n",
    "1. Create a [Watson Machine Learning](https://cloud.ibm.com/catalog/services/watson-machine-learning) service instance (select your appropriate region and choose the Lite plan, which is a free instance).\n",
    "\n",
    "\n",
    "2. Generate an [API Key in WML](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-authentication.html). \n",
    "\n",
    "\n",
    "3. Associate the WML service to the project that you created in [watsonx.ai](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html). \n",
    "\n",
    "## Step 3. Install and import relevant libraries and set up credentials\n",
    "\n",
    "We have a few dependencies for this tutorial. Make sure to import the libraries below, and if they're not installed, you can resolve this with a quick pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#installations\n",
    "%pip install -q python-dotenv\n",
    "%pip install -q langchain\n",
    "%pip install -q langchain_chroma\n",
    "%pip install -q langchain-community\n",
    "%pip install -qU langchain_ibm\n",
    "%pip install -qU langchain_community beautifulsoup4\n",
    "%pip install -q \"ibm-watson-machine-learning>=1.0.327\"\n",
    "%pip install -q \"nltk>=3.9.1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import nltk\n",
    "\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "\n",
    "from langchain_ibm import WatsonxEmbeddings, WatsonxLLM\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your credentials. Please store your `WATSONX_PROJECT_ID` and `WATSONX_APIKEY` in a separate `.env` file in the same level of your directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(os.getcwd()+\"/.env\", override=True)\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": os.getenv(\"WATSONX_APIKEY\", \"\"),\n",
    "}\n",
    "\n",
    "project_id = os.getenv(\"WATSONX_PROJECT_ID\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4. Index the URLs to create the knowledge base\n",
    "\n",
    "We’ll index our ibm.com articles from URLs to create a knowledge base as a vectorstore. The content from these URLs will be our data sources and context for this exercise. The context will then be provided to an LLM to answer any questions we have about IBM products or technologies.\n",
    "\n",
    "The first step to building vector embeddings is to clean and process the raw dataset. This may involve the removal of noise and standardization of the text. For our example, we won’t do any cleaning since the text is already cleaned and standardized.\n",
    "\n",
    "First, let's establish `URLS_DICTIONARY`. `URLS_DICTIONARY` is a dict that helps us map the URLs from which we will be extracting the content. Let's also set up a name for our collection: `askibm_2024`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS_DICTIONARY = {\n",
    "    \"granite.html\": \"https://www.ibm.com/granite\",\n",
    "    \"products_watsonx_ai.html\": \"https://www.ibm.com/products/watsonx-ai\",\n",
    "    \"products_watsonx_ai_foundation_models.html\": \"https://www.ibm.com/products/watsonx-ai/foundation-models\",\n",
    "    \"watsonx_pricing.html\": \"https://www.ibm.com/watsonx/pricing\",\n",
    "    \"watsonx.html\": \"https://www.ibm.com/watsonx\",\n",
    "    \"products_watsonx_data.html\": \"https://www.ibm.com/products/watsonx-data\",\n",
    "    \"products_watsonx_assistant.html\": \"https://www.ibm.com/products/watsonx-assistant\",\n",
    "    \"products_watsonx_code_assistant.html\": \"https://www.ibm.com/products/watsonx-code-assistant\",\n",
    "    \"products_watsonx_orchestrate.html\": \"https://www.ibm.com/products/watsonx-orchestrate\",\n",
    "    \"products_watsonx_governance.html\": \"https://www.ibm.com/products/watsonx-governance\",\n",
    "    \"granite_code_models_open_source.html\": \"https://research.ibm.com/blog/granite-code-models-open-source\",\n",
    "    \"red_hat_enterprise_linux_ai.html\": \"https://www.redhat.com/en/about/press-releases/red-hat-delivers-accessible-open-source-generative-ai-innovation-red-hat-enterprise-linux-ai\",\n",
    "    \"model_choice.html\": \"https://www.ibm.com/blog/announcement/enterprise-grade-model-choices/\",\n",
    "    \"democratizing.html\": \"https://www.ibm.com/blog/announcement/democratizing-large-language-model-development-with-instructlab-support-in-watsonx-ai/\",\n",
    "    \"ibm_consulting_expands_ai.html\": \"https://newsroom.ibm.com/Blog-IBM-Consulting-Expands-Capabilities-to-Help-Enterprises-Scale-AI\",\n",
    "    \"ibm_data_product_hub.html\": \"https://www.ibm.com/products/data-product-hub\",\n",
    "    \"ibm_price_performance_data.html\": \"https://www.ibm.com/blog/announcement/delivering-superior-price-performance-and-enhanced-data-management-for-ai-with-ibm-watsonx-data/\",\n",
    "    \"ibm_bi_adoption.html\": \"https://www.ibm.com/blog/a-new-era-in-bi-overcoming-low-adoption-to-make-smart-decisions-accessible-for-all/\",\n",
    "    \"watsonx_code_assistant_for_z.html\": \"https://www.ibm.com/blog/announcement/ibm-watsonx-code-assistant-for-z-accelerate-the-application-lifecycle-with-generative-ai-and-automation/\",\n",
    "    \"code_assistant_for_java.html\": \"https://www.ibm.com/blog/announcement/watsonx-code-assistant-java/\",\n",
    "    \"code_assistant_for_orchestrate.html\": \"https://www.ibm.com/blog/announcement/watsonx-orchestrate-ai-z-assistant/\",\n",
    "    \"accelerating_gen_ai.html\": \"https://newsroom.ibm.com/Blog-How-IBM-Cloud-is-Accelerating-Business-Outcomes-with-Gen-AI\",\n",
    "    \"watsonx_open_source.html\": \"https://newsroom.ibm.com/2024-05-21-IBM-Unveils-Next-Chapter-of-watsonx-with-Open-Source,-Product-Ecosystem-Innovations-to-Drive-Enterprise-AI-at-Scale\",\n",
    "    \"ibm_concert.html\": \"https://www.ibm.com/products/concert\",\n",
    "    \"ibm_consulting_advantage_news.html\": \"https://newsroom.ibm.com/2024-01-17-IBM-Introduces-IBM-Consulting-Advantage,-an-AI-Services-Platform-and-Library-of-Assistants-to-Empower-Consultants\",\n",
    "    \"ibm_consulting_advantage_info.html\": \"https://www.ibm.com/consulting/info/ibm-consulting-advantage\"\n",
    "}\n",
    "COLLECTION_NAME = \"askibm_2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load our documents using the LangChain `WebBaseLoader` for the list of URLs we have. Loaders load in data from a source and return a list of Documents. A Document is an object with some page_content (str) and metadata (dict). We'll print the `page_content` of a sample document at the end to see how it's been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\n\\n\\nGranite | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\nGranite\\n\\n\\n\\nIBM Granite\\n\\n\\n                        \\n\\n\\n  \\n  \\n      IBM® Granite™ is our family of open, performant, and trusted AI models, tailored for business and optimized to scale your AI applications.\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                    \\n\\n\\nTry Granite\\n\\n\\nRead Granite documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Meet Granite 3.0\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\nOur third generation of AI language models are here. Fit for purpose and open sourced, these enterprise-ready models deliver exceptional performance against safety benchmarks and across a wide range of enterprise tasks from cybersecurity to RAG.\\n\\n\\n\\n\\n            IBM Granite 3.0: new open, enterprise-ready models\\n            \\n            \\n        \\n\\n            Granite 3.0 technical paper\\n            \\n            \\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\n\\n\\n  \\n\\n\\n\\n    Models\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Granite 3.0 language models\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nBase and instruction-tuned language models designed for agentic workflows, RAG, text summarization, text analytics and extraction, classification, and content generation.\\n\\n\\n\\nRead Granite 3.0 documentation\\n            \\n        \\nGet language models on Hugging Face\\n            \\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Granite for code\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nDecoder-only models designed for code generative tasks, including code generation, code explanation, and code editing, trained with code written in 116 programming languages.\\n\\n\\n\\nRead Granite for code documentation\\n            \\n        \\nGet code models on Hugging Face\\n            \\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Granite for time series\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nLightweight and pre-trained for time-series forecasting, optimized to run efficiently across a range of hardware configurations.\\n\\n\\n\\nRead Granite time series documentation\\n            \\n        \\nGet time series models on Hugging Face\\n            \\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Granite Guardian\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nSafeguard AI with Granite Guardian, ensuring enterprise data security and mitigating risks across a variety of user prompts and LLM responses, with top performance in 15+ safety benchmarks.\\n\\n\\n\\nRead Granite Guardian documentation\\n            \\n        \\nGet Granite Guardian on Hugging Face\\n            \\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Granite for geospatial data\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nNASA and IBM teamed up to create an AI Foundation Model for Earth Observations using large-scale satellite and remote sensing data.\\n\\n\\n\\nGet the geospatial model on Hugging Face\\n            \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBenchmarks\\n\\n\\n\\r\\n            \\n\\n\\n  \\n  \\n      Previous generations of Granite models prioritized specialized use cases. In addition to offering even greater efficacy in those arenas, IBM Granite 3.0 models match—and, in some cases, exceed—the general performance of leading open weight LLMs across both academic and enterprise benchmarks.\\n  \\n\\n\\n\\n\\n    \\n\\n\\r\\n        \\n\\n\\n\\n            Explore more benchmarks\\n            \\n            \\n        \\n\\n\\n\\n\\n\\n\\n            \\n            \\n\\n\\n  \\n\\n\\n\\n    Why Granite?\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Open\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nChoose the right model, from sub-billion to 34B parameters, open-sourced under Apache 2.0.\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Performant\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nDon’t sacrifice performance for cost. Granite outperforms comparable models1 across a variety of enterprise tasks.\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Trusted\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nBuild responsible AI with a comprehensive set of risk and harm detection capabilities, transparency, and IP protection.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Build with Granite\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\nDeploy open-source Granite models in production with Red Hat Enterprise Linux AI and watsonx, providing you the support and tooling needed to confidently deploy AI at scale. Build faster with capabilities such as tool-calling, 12 languages, multi-modal adaptors (coming soon), and more.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLeverage generative AI to accelerate code generation and increase developer productivity\\n\\n                    \\n                        Check out watsonx Code Assistant\\n                    \\n                    \\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n        \\n        \\n    \\n\\n        Explore the lightweight AI coding companion powered by IBM Granite\\n        \\n    \\n\\n\\n\\n    \\n\\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n        \\n        \\n    \\n\\n        Get started with the Granite family of foundation models on Hugging Face\\n        \\n    \\n\\n\\n\\n    \\n\\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n        \\n        \\n    \\n\\n        Build AI applications using Granite foundation models on the IBM® watsonx.ai™ studio\\n        \\n    \\n\\n\\n\\n    \\n\\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n        \\n        \\n    \\n\\n        Seamlessly develop, test and run Granite family LLMs for enterprise applications\\n        \\n    \\n\\n\\n\\n    \\n\\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nExplore the tutorials\\n\\n\\n\\n\\n                    \\n                    Build a LangChain agentic RAG system using Granite-3.0-8B-Instruct in watsonx.ai\\n\\n\\n\\nDiscover how to build an AI agent that can answer questions\\n\\n\\n\\n\\n\\n\\n\\n\\n                    \\n                    Function calling with IBM Granite 3.0 8B Instruct\\n\\n\\n\\nIn this tutorial, you will use the IBM® Granite-3.0-8B-Instruct model now available on watsonx.ai™ to perform custom function calling.\\n\\n\\n\\n\\n\\n\\n\\n\\n                    \\n                    Post training quantization of Granite-3.0-8B-Instruct in Python with watsonx\\n\\n\\n\\nQuantize a pre-trained model in a few different ways to show the size of the models and compare how they perform on a task\\n\\n\\n\\n\\n\\n\\n\\n\\n                    \\n                    Evaluate RAG pipeline using Ragas in Python with watsonx\\n\\n\\n\\nUse the Ragas framework for Retrieval-Augmented Generation (RAG) evaluation in Python using LangChain\\n\\n\\n\\n\\n\\n\\n\\n\\n                    \\n                    Using foundation models for time series forecasting\\n\\n\\n\\nForecast the future based on learning with the TinyTimeMixer (TTM) Granite Model\\n\\n\\n\\n\\n\\n\\n\\n\\n                    \\n                    Generating SQL from text with LLMs\\n\\n\\n\\nConvert text into a structured representation and generate a semantically correct SQL query\\n\\n\\n\\n\\n\\n\\n\\n\\n                    \\n                    Build a local AI co-pilot using IBM Granite Code, Ollama, and Continue\\n\\n\\n\\nPrompt tune a Granite model in Python using a synthetic dataset containing positive and negative customer reviews\\n\\n\\n\\n\\n\\n\\n\\n\\n                    \\n                    View the full granite cookbook\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                \\n                    \\n\\n\\n  \\n\\n\\n\\n    Granite in the news\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                \\n            \\n\\n\\n\\n            IBM Research\\n        \\n\\n            Granite 3.0 technical paper\\n        \\nThis report presents Granite 3.0 and discloses technical details of pre- and post-training to accelerate the development of open foundation models.\\n\\n\\n\\n\\n\\n            IBM blog\\n        \\n\\n            IBM Granite 3.0: new open, enterprise-ready models\\n        \\nTrained on 12 languages + 116 programming languages, the new Granite 3.0 8B and 2B models are here. Explore new benchmarks on performance, safety and security + the latest tutorials.\\n\\n\\n\\n\\n\\n            Technical blog\\n        \\n\\n            IBM Granite Now Available Through the Generative AI Hub in SAP AI Core\\n        \\nSAP users can now harness the power of IBM watsonx and IBM Granite, beginning with Granite.13b.chat large language model, available through the generative AI hub on SAP AI core on the SAPBusiness Technology Platform (SAP BTP).\\n\\n\\n\\n\\n\\n            Leaderboard\\n        \\n\\n            Ranked top 5 in Stanford Transparency Index\\n        \\nA report from Stanford University’s Center for Research on Foundation Models showed that IBM’s model scored a perfect 100% in several categories designed to measure how open models really are. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNext steps\\n\\n\\n\\n\\n\\n\\n\\nTry Granite\\n\\n\\nRead Granite documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the IBM statement on IP protection \\n\\n\\n\\nIBM believes in the creation, deployment and utilization of AI models that advance innovation across the enterprise responsibly. IBM watsonx AI and data platform have an end-to-end process for building and testing foundation models and generative AI. For IBM-developed models, we search for and remove duplication, and we employ URL blocklists, filters for objectionable content and document quality, sentence splitting and tokenization techniques, all before model training.\\nDuring the data training process, we work to prevent misalignments in the model outputs and use supervised fine-tuning to enable better instruction following so that the model can be used to complete enterprise tasks via prompt engineering. We are continuing to develop the Granite models in several directions, including other modalities, industry-specific content and more data annotations for training, while also deploying regular, ongoing data protection safeguards for IBM developed models.\\xa0\\nGiven the rapidly changing generative AI technology landscape, our end-to-end processes are expected to continuously evolve and improve.\\xa0As a testament to the rigor IBM puts into the development and testing of its foundation models, the company provides its standard contractual intellectual property indemnification for IBM-developed models, similar to those it provides for IBM hardware and software products.\\nMoreover, contrary to some other providers of large language models and consistent with the IBM standard approach on indemnification, IBM does not require its customers to indemnify IBM for a customer's use of IBM-developed models. Also, consistent with the IBM approach to its indemnification obligation, IBM does not cap its indemnification liability for the IBM-developed models.\\nThe current watsonx models now under these protections include:\\n(1) Slate family of encoder-only models.\\n(2) Granite family of a decoder-only model.\\nLearn more about licensing for Granite models\\n\\n\\n\\n\\n\\nFootnotes\\n\\n\\n\\n1Performance of Granite models conducted by IBM Research against leading open models across both academic and enterprise benchmarks - https://ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for url in list(URLS_DICTIONARY.values()):\n",
    "    loader = WebBaseLoader(url)\n",
    "    data = loader.load()\n",
    "    documents += data\n",
    "\n",
    "# #show sample document\n",
    "documents[0].page_content\n",
    "\n",
    "#Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sample document, it looks like there's a lot of white space and new line characters that we can get rid of. Let's clean that up and add some metadata to our documents, including an id number and the source of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.ibm.com/granite', 'title': 'Granite | IBM', 'description': 'Start building with Granite 3.0, our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications.', 'language': 'en', 'id': 0}\n",
      "{'source': 'https://www.ibm.com/products/watsonx-ai', 'title': 'IBM watsonx.ai', 'description': 'A next generation enterprise studio for AI builders to train, validate, tune and deploy AI models', 'language': 'en', 'id': 1}\n",
      "{'source': 'https://www.ibm.com/products/watsonx-ai/foundation-models', 'title': 'Foundation Models - IBM watsonx.ai', 'description': 'Explore the family of language and code foundation models within the watsonx platform. ', 'language': 'en', 'id': 2}\n",
      "{'source': 'https://www.ibm.com/watsonx/pricing', 'title': 'IBM watsonx | Pricing', 'description': 'IBM watsonx.ai pricing', 'language': 'en', 'id': 3}\n",
      "{'source': 'https://www.ibm.com/watsonx', 'title': 'IBM watsonx —\\xa0An AI and data platform built for business ', 'description': 'IBM® watsonx™ includes three core components and a set of AI assistants designed to help you scale and accelerate the impact of AI across your business.', 'language': 'en', 'id': 4}\n",
      "{'source': 'https://www.ibm.com/products/watsonx-data', 'title': 'IBM watsonx.data', 'description': 'The hybrid, open data lakehouse to power AI and analytics with all your data, anywhere.', 'language': 'en', 'id': 5}\n",
      "{'source': 'https://www.ibm.com/products/watsonx-assistant', 'title': 'IBM watsonx Assistant Virtual Agent', 'description': 'IBM watsonx Assistant provides customers with fast, consistent and accurate answers across any application, device or channel.', 'language': 'en', 'id': 6}\n",
      "{'source': 'https://www.ibm.com/products/watsonx-code-assistant', 'title': 'Code smarter, not harder', 'description': ' IBM watsonx Code Assistant harnesses generative AI to augment developer skill sets, simplifying and automating your coding and modernization efforts.', 'language': 'en', 'id': 7}\n",
      "{'source': 'https://www.ibm.com/products/watsonx-orchestrate', 'title': 'IBM watsonx Orchestrate', 'description': 'IBM watsonx Orchestrate is a generative AI and automation solution that empowers your business by automating tasks, simplifying complex processes.', 'language': 'en', 'id': 8}\n",
      "{'source': 'https://www.ibm.com/products/watsonx-governance', 'title': 'IBM watsonx.governance', 'description': 'Learn how you can direct, manage and monitor your AI with\\r\\nwatsonx.governance, a single platform to speed responsible, transparent, explainable AI. Find out how you can address risks that AI presents, adhere and adapt to changing regulations, and help manage the complete AI lifecycle governance.', 'language': 'en', 'id': 9}\n",
      "{'source': 'https://research.ibm.com/blog/granite-code-models-open-source', 'title': 'IBM’s Granite code model family is going open source - IBM Research', 'description': 'IBM is open sourcing family of Granite code models to make coding as easy as possible — for as many developers as possible.', 'language': 'en-US', 'id': 10}\n",
      "{'source': 'https://www.redhat.com/en/about/press-releases/red-hat-delivers-accessible-open-source-generative-ai-innovation-red-hat-enterprise-linux-ai', 'title': 'Red Hat Delivers Accessible, Open Source Generative AI Innovation with Red Hat Enterprise Linux AI', 'description': 'Red Hat announces the launch of Red Hat Enterprise Linux AI (RHEL AI), a foundation model platform that enables users to more seamlessly develop, test and deploy generative AI (GenAI) models.', 'language': 'en', 'id': 11}\n",
      "{'source': 'https://www.ibm.com/blog/announcement/enterprise-grade-model-choices/', 'title': 'New strategic partnerships from IBM offer clients a wide range of enterprise-grade model choices | IBM', 'description': 'At IBM, we take a differentiated approach to delivering enterprise-grade models that help clients scale quality gen AI with confidence.', 'language': 'en', 'id': 12}\n",
      "{'source': 'https://www.ibm.com/blog/announcement/democratizing-large-language-model-development-with-instructlab-support-in-watsonx-ai/', 'title': 'Democratizing Large Language Model development with InstructLab support in watsonx.ai | IBM', 'description': 'We believe in a better path forward to help enable real collaborative model development in the open—to democratize AI building for everyone.', 'language': 'en', 'id': 13}\n",
      "{'source': 'https://newsroom.ibm.com/Blog-IBM-Consulting-Expands-Capabilities-to-Help-Enterprises-Scale-AI', 'title': 'IBM Consulting Expands Capabilities to Help Enterprises Scale AI', 'description': 'IBM Consulting is announcing that we are planning to launch a practice to help clients create their own AI foundation models and LLMs using this open-source approach. IBM consultants will help enterprise clients leverage their proprietary data to train purpose-specific AI models that can be scaled to better fit the cost and performance requirements of an enterprise’s business needs. ', 'language': 'en-us', 'id': 14}\n",
      "{'source': 'https://www.ibm.com/products/data-product-hub', 'title': 'IBM Data Product Hub', 'description': 'IBM Data Product Hub enables creation and sharing of data products with data consumers who can discover and use the right data in a compliant manner from a reliable data marketplace.', 'language': 'en', 'id': 15}\n",
      "{'source': 'https://www.ibm.com/blog/announcement/delivering-superior-price-performance-and-enhanced-data-management-for-ai-with-ibm-watsonx-data/', 'title': 'Delivering superior price-performance and enhanced data management for AI with IBM watsonx.data', 'description': 'Watsonx.data enables enterprises to unlock value in their existing data by connecting to existing storage and analytical environments.', 'language': 'en', 'id': 16}\n",
      "{'source': 'https://www.ibm.com/blog/a-new-era-in-bi-overcoming-low-adoption-to-make-smart-decisions-accessible-for-all/', 'title': 'A new era in BI: Overcoming low adoption to make smart decisions accessible for all | IBM', 'description': 'A deeper look into why business intelligence challenges might persist and what it means for users across an organization.', 'language': 'en', 'id': 17}\n",
      "{'source': 'https://www.ibm.com/blog/announcement/ibm-watsonx-code-assistant-for-z-accelerate-the-application-lifecycle-with-generative-ai-and-automation/', 'title': 'IBM watsonx Code Assistant for Z: accelerate the application lifecycle with generative AI and automation | IBM', 'description': 'We are excited to announce a new product capability, code explanation, available as an add-on to watsonx Code Assistant for Z.', 'language': 'en', 'id': 18}\n",
      "{'source': 'https://www.ibm.com/blog/announcement/watsonx-code-assistant-java/', 'title': 'Accelerating the Java application lifecycle with generative AI and automation | IBM', 'description': 'At Think 2024 conference, we are excited to announce an upcoming preview of IBM watsonx™ Code Assistant for Enterprise Java Applications.', 'language': 'en', 'id': 19}\n",
      "{'source': 'https://www.ibm.com/blog/announcement/watsonx-orchestrate-ai-z-assistant/', 'title': 'Announcing IBM watsonx Assistant for Z and D&B Ask Procurement, new generative AI assistants built with IBM watsonx Orchestrate', 'description': 'The new AI Assistant Builder in IBM watsonx Orchestrate a low-code platform that helps you automate workflows and decision-making processes.', 'language': 'en', 'id': 20}\n",
      "{'source': 'https://newsroom.ibm.com/Blog-How-IBM-Cloud-is-Accelerating-Business-Outcomes-with-Gen-AI', 'title': 'How IBM Cloud is Accelerating Business Outcomes with Gen AI', 'description': 'IBM Cloud is helping our clients boost competitiveness with the combined power of our hybrid cloud and AI stack. Our enterprise cloud platform is built for even the most regulated industries, and coupled with our heritage in high performance computing (HPC), we are uniquely positioned to support the AI infrastructure needed for performance intensive workloads. We’re working on several new solutions and tailor-made collaborations to help enable a seamless and secure integration of AI.', 'language': 'en-us', 'id': 21}\n",
      "{'source': 'https://newsroom.ibm.com/2024-05-21-IBM-Unveils-Next-Chapter-of-watsonx-with-Open-Source,-Product-Ecosystem-Innovations-to-Drive-Enterprise-AI-at-Scale', 'title': 'IBM Unveils Next Chapter of watsonx with Open Source, Product & Ecosystem Innovations to Drive Enterprise AI at Scale', 'description': 'IBM announced several new updates to its watsonx platform one year after its introduction, as well as upcoming data and automation capabilities designed to make artificial intelligence (AI) more open, cost effective, and flexible for businesses. ', 'language': 'en-us', 'id': 22}\n",
      "{'source': 'https://www.ibm.com/products/concert', 'title': 'IBM Concert', 'description': 'IBM Concert is a generative AI-driven technology automation platform that streamline application management and generate AI insights you can act on.', 'language': 'en', 'id': 23}\n",
      "{'source': 'https://newsroom.ibm.com/2024-01-17-IBM-Introduces-IBM-Consulting-Advantage,-an-AI-Services-Platform-and-Library-of-Assistants-to-Empower-Consultants', 'title': 'IBM Introduces IBM Consulting Advantage, an AI Services Platform and Library of Assistants to Empower Consultants - Jan 17, 2024', 'description': 'The latest news from IBM', 'language': 'en-us', 'id': 24}\n",
      "{'source': 'https://www.ibm.com/consulting/info/ibm-consulting-advantage', 'title': 'IBM Consulting Advantage', 'description': 'IBM Consulting IP assets and pre-configured solutions are now easier to access, deploy, and scale using conversational generative AI tools.', 'language': 'en', 'id': 25}\n"
     ]
    }
   ],
   "source": [
    "doc_id = 0\n",
    "for doc in documents:\n",
    "    doc.page_content = \" \".join(doc.page_content.split()) # remove white space\n",
    "\n",
    "    doc.metadata[\"id\"] = doc_id #make a document id and add it to the document metadata\n",
    "\n",
    "    print(doc.metadata)\n",
    "    doc_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our sample document looks now after we cleaned it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.ibm.com/granite', 'title': 'Granite | IBM', 'description': 'Start building with Granite 3.0, our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications.', 'language': 'en', 'id': 0}, page_content=\"Granite | IBM Home Granite IBM Granite IBM® Granite™ is our family of open, performant, and trusted AI models, tailored for business and optimized to scale your AI applications. Try Granite Read Granite documentation Meet Granite 3.0 Our third generation of AI language models are here. Fit for purpose and open sourced, these enterprise-ready models deliver exceptional performance against safety benchmarks and across a wide range of enterprise tasks from cybersecurity to RAG. IBM Granite 3.0: new open, enterprise-ready models Granite 3.0 technical paper Models Granite 3.0 language models Base and instruction-tuned language models designed for agentic workflows, RAG, text summarization, text analytics and extraction, classification, and content generation. Read Granite 3.0 documentation Get language models on Hugging Face Granite for code Decoder-only models designed for code generative tasks, including code generation, code explanation, and code editing, trained with code written in 116 programming languages. Read Granite for code documentation Get code models on Hugging Face Granite for time series Lightweight and pre-trained for time-series forecasting, optimized to run efficiently across a range of hardware configurations. Read Granite time series documentation Get time series models on Hugging Face Granite Guardian Safeguard AI with Granite Guardian, ensuring enterprise data security and mitigating risks across a variety of user prompts and LLM responses, with top performance in 15+ safety benchmarks. Read Granite Guardian documentation Get Granite Guardian on Hugging Face Granite for geospatial data NASA and IBM teamed up to create an AI Foundation Model for Earth Observations using large-scale satellite and remote sensing data. Get the geospatial model on Hugging Face Benchmarks Previous generations of Granite models prioritized specialized use cases. In addition to offering even greater efficacy in those arenas, IBM Granite 3.0 models match—and, in some cases, exceed—the general performance of leading open weight LLMs across both academic and enterprise benchmarks. Explore more benchmarks Why Granite? Open Choose the right model, from sub-billion to 34B parameters, open-sourced under Apache 2.0. Performant Don’t sacrifice performance for cost. Granite outperforms comparable models1 across a variety of enterprise tasks. Trusted Build responsible AI with a comprehensive set of risk and harm detection capabilities, transparency, and IP protection. Build with Granite Deploy open-source Granite models in production with Red Hat Enterprise Linux AI and watsonx, providing you the support and tooling needed to confidently deploy AI at scale. Build faster with capabilities such as tool-calling, 12 languages, multi-modal adaptors (coming soon), and more. Leverage generative AI to accelerate code generation and increase developer productivity Check out watsonx Code Assistant Explore the lightweight AI coding companion powered by IBM Granite Get started with the Granite family of foundation models on Hugging Face Build AI applications using Granite foundation models on the IBM® watsonx.ai™ studio Seamlessly develop, test and run Granite family LLMs for enterprise applications Explore the tutorials Build a LangChain agentic RAG system using Granite-3.0-8B-Instruct in watsonx.ai Discover how to build an AI agent that can answer questions Function calling with IBM Granite 3.0 8B Instruct In this tutorial, you will use the IBM® Granite-3.0-8B-Instruct model now available on watsonx.ai™ to perform custom function calling. Post training quantization of Granite-3.0-8B-Instruct in Python with watsonx Quantize a pre-trained model in a few different ways to show the size of the models and compare how they perform on a task Evaluate RAG pipeline using Ragas in Python with watsonx Use the Ragas framework for Retrieval-Augmented Generation (RAG) evaluation in Python using LangChain Using foundation models for time series forecasting Forecast the future based on learning with the TinyTimeMixer (TTM) Granite Model Generating SQL from text with LLMs Convert text into a structured representation and generate a semantically correct SQL query Build a local AI co-pilot using IBM Granite Code, Ollama, and Continue Prompt tune a Granite model in Python using a synthetic dataset containing positive and negative customer reviews View the full granite cookbook Granite in the news IBM Research Granite 3.0 technical paper This report presents Granite 3.0 and discloses technical details of pre- and post-training to accelerate the development of open foundation models. IBM blog IBM Granite 3.0: new open, enterprise-ready models Trained on 12 languages + 116 programming languages, the new Granite 3.0 8B and 2B models are here. Explore new benchmarks on performance, safety and security + the latest tutorials. Technical blog IBM Granite Now Available Through the Generative AI Hub in SAP AI Core SAP users can now harness the power of IBM watsonx and IBM Granite, beginning with Granite.13b.chat large language model, available through the generative AI hub on SAP AI core on the SAPBusiness Technology Platform (SAP BTP). Leaderboard Ranked top 5 in Stanford Transparency Index A report from Stanford University’s Center for Research on Foundation Models showed that IBM’s model scored a perfect 100% in several categories designed to measure how open models really are. Next steps Try Granite Read Granite documentation Read the IBM statement on IP protection IBM believes in the creation, deployment and utilization of AI models that advance innovation across the enterprise responsibly. IBM watsonx AI and data platform have an end-to-end process for building and testing foundation models and generative AI. For IBM-developed models, we search for and remove duplication, and we employ URL blocklists, filters for objectionable content and document quality, sentence splitting and tokenization techniques, all before model training. During the data training process, we work to prevent misalignments in the model outputs and use supervised fine-tuning to enable better instruction following so that the model can be used to complete enterprise tasks via prompt engineering. We are continuing to develop the Granite models in several directions, including other modalities, industry-specific content and more data annotations for training, while also deploying regular, ongoing data protection safeguards for IBM developed models. Given the rapidly changing generative AI technology landscape, our end-to-end processes are expected to continuously evolve and improve. As a testament to the rigor IBM puts into the development and testing of its foundation models, the company provides its standard contractual intellectual property indemnification for IBM-developed models, similar to those it provides for IBM hardware and software products. Moreover, contrary to some other providers of large language models and consistent with the IBM standard approach on indemnification, IBM does not require its customers to indemnify IBM for a customer's use of IBM-developed models. Also, consistent with the IBM approach to its indemnification obligation, IBM does not cap its indemnification liability for the IBM-developed models. The current watsonx models now under these protections include: (1) Slate family of encoder-only models. (2) Granite family of a decoder-only model. Learn more about licensing for Granite models Footnotes 1Performance of Granite models conducted by IBM Research against leading open models across both academic and enterprise benchmarks - https://ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split up our text into smaller, more manageable pieces known as \"chunks\". LangChain's `RecursiveCharacterTextSplitter` takes a large text and splits it based on a specified chunk size using a predefined set of characters. In order, the default characters are:\n",
    "- \"\\n\\n\" - two new line characters \n",
    "- \"\\n\" - one new line character\n",
    "- \" \" - a space\n",
    "- \"\" - an empty character\n",
    "\n",
    "The process starts by attempting to split the text using the first character, \"\\n\\n.\" If the resulting chunks are still too large, it moves to the next character, \"\\n,\" and tries splitting again. This continues with each character in the set until the chunks are smaller than the specified chunk size. Since we already removed all the \"\\n\\n\" and \"\\n\" characters when we cleaned up the text, the `RecursiveCharacterTextSplitter` will begin at the \" \"(space) character.\n",
    "\n",
    "We settled on a chunk size of 512 after experimenting with a chunk size of 1000. When the chunks were that large, our model was getting too much context for question-answering; this led to confused responses by the LLM because it was receiving too much information, so we changed it to smaller chunks. Feel free to experiment with chunk size further!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we choose an embedding model to be trained on our ibm.com dataset. The trained embedding model is used to generate embeddings for each data point in the dataset. For text data, popular open-source embedding models include Word2Vec, GloVe, FastText or pre-trained transformer-based models like BERT or RoBERTa. OpenAIembeddings may also be used by leveraging the OpenAI embeddings API endpoint, the `langchain_openai` package and getting an `openai_api_key`, however, there is a cost associated with this usage.\n",
    "\n",
    "Unfortunately, because the embedding models are so large, vector embedding often demands significant computational resources. We can greatly lower the costs linked to embedding vectors, while preserving performance and accuracy by using WatsonxEmbeddings. We'll use the IBM embeddings model, Slate, an encoder-only (RoBERTa-based) model, which while not generative, is fast and effective for many NLP tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = WatsonxEmbeddings(\n",
    "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n",
    "    url=credentials[\"url\"],\n",
    "    apikey=credentials[\"apikey\"],\n",
    "    project_id=project_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our content into a local instance of a vector database, using Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Set up a retriever\n",
    "\n",
    "We'll set up our vector store as a retriever. The retrieved information from the vector store serves as additional context or knowledge that can be used by a generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Generate a response with a Generative Model\n",
    "\n",
    "Finally, we’ll generate a response. The generative model (like GPT-4 or IBM Granite) uses the retrieved information to produce a more accurate and contextually relevant response to our questions.\n",
    "\n",
    "First, we'll establish which LLM we're going to use to generate the response. For this tutorial, we'll use IBM's Granite-3.0-8B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ibm/granite-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters available can be found [here](https://ibm.github.io/watson-machine-learning-sdk/model.html#enums). We experimented with various model parameters, including Temperature, Top P, and Top K. [Here](https://www.ibm.com/docs/en/watsonx/saas?topic=lab-model-parameters-prompting)'s some more information on model parameters and what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: 'greedy',\n",
    "    GenParams.TEMPERATURE: 2,\n",
    "    GenParams.TOP_P: 0,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 10,\n",
    "    GenParams.MAX_NEW_TOKENS: 512,\n",
    "    GenParams.REPETITION_PENALTY:1.2,\n",
    "    GenParams.RETURN_OPTIONS: {'input_tokens': True,'generated_tokens': True, 'token_logprobs': True, 'token_ranks': True, }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a `prompttemplate` to ask multiple questions. The \"context\" will be derived from our retriever (our vector database) with the relevant documents and the \"question\" will be derived from the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Generate a summary of the context that answers the question. Explain the answer in multiple steps if possible. \n",
    "Answer style should match the context. Ideal Answer Length 2-3 sentences.\\n\\n{context}\\nQuestion: {question}\\nAnswer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a helper function to format the docs accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can set up a chain with our context, our prompt and our LLM. The generative model processes the augmented context along with the user's question to produce an LLM-powered response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can ask multiple questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IBM Granite is a family of open, high-performing, and trustworthy AI models designed specifically for businesses. These models are optimized to enhance AI applications and have been updated with Granite 3.0, which features open-sourced, enterprise-ready models. The new Granite 3.0 models excel in various enterprise tasks like cybersecurity and Retrieval Augmented Generation (RAG) while meeting safety benchmarks. They are available through the Generative AI Hub in SAP AI Core and offer a range of options, from sub-billion to 34B parameters, all open-sourced under the Apache 2.0 license.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is IBM Granite?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask about IBM watsonx Orchestrate next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIBM Concert is a generative AI-powered tool set to be generally available in June 2024. It serves as the 'nerve center' of an enterprise's technology and operations, providing AI-driven insights across clients' applications and automating repetitive tasks. The tool aims to improve operational risk and resiliency by transforming data from disparate tools and environments into actionable knowledge. (15 words)\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is watsonx Orchestrate?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's ask about watsonx.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWatsonx.ai is a comprehensive AI platform that enables users to build, deploy, and manage AI applications. It offers user-friendly interfaces, workflows, and access to industry-standard APIs and SDKs, streamlining the AI development process. Additionally, it provides tools for generating various content types and monitoring model accuracy, drift, and bias.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What does watsonx.ai do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Feel free to ask even more questions!\n",
    "\n",
    "# Summary and next steps\n",
    "\n",
    "In this tutorial, you created a simple LangChain RAG workflow in Python with watsonx. You fetched 26 articles from ibm.com to create a vector store as context for an LLM to answer questions about IBM offerings and technologies.\n",
    "\n",
    "You can imagine a situation where we can create chatbots to field these questions.\n",
    "\n",
    "We encourage you to check out the [LangChain documentation page](https://python.langchain.com/v0.2/docs/tutorials/rag/) for more information and tutorials on RAG.\n",
    "\n",
    "\n",
    "## Try watsonx for free\n",
    "\n",
    "Build an AI strategy for your business on one collaborative AI and data platform called [IBM watsonx](https://www.ibm.com/watsonx), which brings together new generative AI capabilities, powered by foundation models, and traditional machine learning into a powerful platform spanning the AI lifecycle. With [watsonx.ai](https://www.ibm.com/products/watsonx-ai), you can train, validate, tune, and deploy models with ease and build AI applications in a fraction of the time with a fraction of the data.\n",
    "\n",
    "Try [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone), the next-generation studio for AI builders.\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Explore more [articles and tutorials about watsonx](https://developer.ibm.com/components/watsonx/?) on IBM Developer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
