{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94aade6-b3af-4fb1-b7ad-3cbd61f86595",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Title: Creating a RAG with Llamaindex\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Retrieval-augmented generation (RAG) is a popular technique for using large language models (LLMs) and generative AI that combines information retrieval with language generation. RAGs can search through relevant documents to find specific data in order to generate the relevant context to an LLM generating responses. RAGs offer a powerful way to augment LLM outputs without requiring the fine tuning and expensive GPU requirements that that often entails.\n",
    "\n",
    "LlamaIndex is a powerful open source framework that simplifies the process of building RAG pipelines. It provides a flexible and efficient way to connect retrieval components (like vector databases and embedding models) with generation models like [IBMs Granite models](https://www.ibm.com/granite), GPT-3 or Metas Llama. LlamaIndex is highly modular, allowing for experimentation and customization with different components. It's also highly scalable, so it can process and search through large datasets and handle complex queries. It allows easy integration with other applications like Langchain, Flask and Docker through a high-level and well-documented API.\n",
    "\n",
    "Use cases for RAGs include self-documenting code bases, chatbots for question-answering or enabling hybrid search across multiple types of documents and data sources without requiring a traditional database or SQL queries. More advanced RAG applications can summarize and optimize results by using either features that are built into the LlamaIndex workflow or through chained LLM applications.\n",
    "\n",
    "In this tutorial, you'll build a RAG application in Python that uses LlamaIndex to extract information from a PDF document and answer questions. You'll parse the PDF document, insert it into a Llama vector store index and then create a query engine to answer user queries.\n",
    "\n",
    "# Steps\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You'll need to create an [watsonx account](https://www.ibm.com/docs/en/watsonx/saas?topic=tutorials-signing-up-watsonx) and have a Python environment with [virtualenv](https://virtualenv.pypa.io/en/latest/installation.html) installed.\n",
    "\n",
    "## Step 1\n",
    "\n",
    "While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook.\n",
    "\n",
    "Log in to [watsonx.ai](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project) with your IBM Cloud account.\n",
    "\n",
    "Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project).\n",
    "\n",
    "You can get your project ID from within your project. Click the Manage tab. Then, copy the project ID from the Details section of the General page. You need this Project ID for this tutorial.\n",
    "\n",
    "Next, associate your project with the [watsonx.ai Runtime](https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-trial)\n",
    "\n",
    "a.  Create a [watsonx.ai Runtime](https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-create-langchain-rag-system-python-watsonx&cm_sp=ibmdev-_-developer-_-trial) service instance (choose the Lite plan, which is a free instance).\n",
    "\n",
    "b.  Generate an API Key in watsonx.ai Runtime. Save this API key for use in this tutorial.\n",
    "\n",
    "c.  Go to your project and select the Manage tab\n",
    "\n",
    "d.  In the left tab, select Services and Integrations\n",
    "\n",
    "e.  Select IBM services\n",
    "\n",
    "f.  Select Associate service and pick waxtsonx data.\n",
    "\n",
    "g.  Associate the waxtsonx data service to the project that you created in watsonx.ai\n",
    "\n",
    "## Step 2\n",
    "\n",
    "In your terminal on your new computer, create a fresh [virtualenv](https://www.ibm.com/docs/en/python-zos/3.12?topic=virtual-environments-considerations) for this project:\n",
    "\n",
    "```\n",
    "virtualenv llamaindex_rag --python=python3.12\n",
    "```\n",
    "\n",
    "Now activate the environment:\n",
    "\n",
    "```\n",
    "source ./llamaindex_rag/bin/activate\n",
    "```\n",
    "\n",
    "In the Python environment for your notebook, install the following Python libraries:\n",
    "\n",
    "```\n",
    "/llamaindex_rag/bin/pip install fqdn getpass4 greenlet isoduration jsonpointer jupyterlab llama-index-embeddings-huggingface llama-index-llms-ibm llama-index-readers-file llama-index-retrievers-bm25 PyMuPDF tinycss2 uri-template webcolors\n",
    "```\n",
    "\n",
    "Now you can start a notebook:\n",
    "\n",
    "```\n",
    "jupyter-lab\n",
    "```\n",
    "\n",
    "## Step 3\n",
    "\n",
    "Use the API key and Project ID that you configured in the first step to access models via watsonx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "386775dd-9d48-43a3-a07d-f9f5b6bf9275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "watsonx_api_key = getpass()\n",
    "os.environ[\"WATSONX_APIKEY\"] = watsonx_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1b4f2692-cbca-40c9-b146-f018c67e7c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "watsonx_project_id = getpass()\n",
    "os.environ[\"WATSONX_PROJECT_ID\"] = watsonx_project_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb4f4fe-5a8c-4c06-b65b-cab9b3a0e11a",
   "metadata": {},
   "source": [
    "You can now configure WatsonxLLM, an interface to watsonx services provided by LlamaIndex. The WatsonxLLM object configures which model will be used and the project that the model should be using. In this case, you'll use the [Granite 3 8-billion parameter Instruct](https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models) model.\n",
    "\n",
    "The parameters configure how the model output should be configured. The [LLM temperature](https://www.ibm.com/think/topics/llm-temperature) should be fairly low and the number of tokens high to encourage the model to generate as much detail as possible without hallucinating entities or relationships that aren't present. A lower [top_k](https://www.ibm.com/docs/en/watsonx/saas?topic=lab-model-parameters-prompting) and higher [top_p](https://www.ibm.com/docs/en/watsonx/saas?topic=lab-model-parameters-prompting) parameter generate some variability but also select only the higher likelihood generated tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "451732d7-6c41-4105-96e8-22b136953ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n",
    "\n",
    "rag_gen_parameters = {\n",
    "    GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n",
    "    GenTextParamsMetaNames.MIN_NEW_TOKENS: 150,\n",
    "    GenTextParamsMetaNames.TEMPERATURE: 0.5,\n",
    "    GenTextParamsMetaNames.TOP_K: 5,\n",
    "    GenTextParamsMetaNames.TOP_P: 0.7\n",
    "}\n",
    "\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=\"ibm/granite-3-8b-instruct\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "    max_new_tokens=512,\n",
    "    params=rag_gen_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6acf2-8cc1-4490-9574-8561dafa19d0",
   "metadata": {},
   "source": [
    "To ensure compatibility between the event loop running in our Jupyter notebook and the RAG processing loop in LlamaIndex, you'll use the asyncio library to generate an independent event loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b8646d29-0f01-4754-bb55-b26f5e30b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65787b6b-a3a0-4181-abeb-934b4b32c95b",
   "metadata": {},
   "source": [
    "Download the [Annual Report](https://www.ibm.com/investor/services/annual-report) from IBM, save it, and then load it into a PyMuPDFReader instance so that you can parse it and generate embeddings for ingestion into the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d1f58bf7-2e91-4212-9dea-3d9418a356db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "import requests\n",
    "\n",
    "def load_data(url):\n",
    "    r = requests.get(url)\n",
    "    name = url.rsplit('/', 1)[1]\n",
    "    # save to a docs dir\n",
    "    with open('docs/' + name, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    \n",
    "    loader = PyMuPDFReader()\n",
    "    return loader.load(file_path=\"./docs/\" + name)\n",
    "\n",
    "pdf_doc = load_pdf(\"https://www.ibm.com/annualreport/assets/downloads/IBM_Annual_Report_2023.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28d2367-6d77-4e40-ab9f-9764262a7b72",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "In this step, you'll generate embeddings and create a vector store. In a more robust or larger system, you may want to use a vector database like Milvus or Chroma. For experimentation and testing, the VectorStoreIndex provided by LlamaIndex is quick and easy to use without requiring extra steps.\n",
    "\n",
    "The first step is to set which embeddings you'll use to generate from the PDF file. In this tutorial, we'll use the [HuggingFace `bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5) embeddings, but other embedding models also work depending on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "27d8410e-d2ac-4e34-afbc-0a4c7ea97a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862dee5-b406-43be-b9e3-a0c91d2d9603",
   "metadata": {},
   "source": [
    "Now you'll generate the actual VectorStoreIndex from the PDF document by splitting the document into smaller chunks, converting them to embeddings and storing them in the VectorStoreIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "933d7507-b50a-4097-8083-11f60a45450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c0b4ddae-05c2-4c13-bc11-2bbd91261d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[splitter], embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07247d-17f6-4906-9dfe-8ac5f3c3eecf",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "In this step, you'll create a retriever that synthesizes the results from multiple query generators to select the best query based on the original user query. First, create a query generation prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2fb13e87-df48-4f65-940c-f708e392950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a single input query. Generate {num_queries} search queries, one on each line related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeefe411-57dc-43fa-a347-63f4cc85759f",
   "metadata": {},
   "source": [
    "Now, use the QueryFusionRetriever for query rewriting. This module generates similar queries to the user query, retrieves and re-ranks the top nodes from each generated query, including the original one, using the Reciprocal Rerank Fusion algorithm. This method (introduced in [this paper](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)) re-ranks retrieved queries and associated nodes without requiring excessive computation or dependence on external models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cb954a1a-4623-48fe-a016-2d963f4d9eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "# this sets the LLM for the rest of the application\n",
    "Settings.llm = watsonx_llm\n",
    "\n",
    "# get retrievers\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=2\n",
    ")\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=4,\n",
    "    num_queries=4,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=False,\n",
    "    query_gen_prompt=query_gen_prompt_str  # we could override the query generation prompt here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02902a-0781-4d76-9b5a-06b1cfb999e5",
   "metadata": {},
   "source": [
    "To see how our retrievers generate and rank queries, use a test query about the IBM financial data from the original PDF document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b0d4ecc4-4055-4998-9034-d825a657aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_scores = retriever.retrieve(\"What was IBMs revenue in 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175bb45f-f0cf-40ee-a2fc-4578e2e6ff39",
   "metadata": {},
   "source": [
    "You can see the different scores and text objects by simply looping through the returned nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "099ebdc2-e10b-402a-93bc-74118d326bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.05 :: Arvind Krishna\n",
      "Chairman and Chief Executive Officer\n",
      "Dear IBM Investor:\n",
      "In 2023, we made significant ...\n",
      "Score: 0.05 :: Reconciliations of IBM as Reported\n",
      "($ in millions)\n",
      "At December 31:\n",
      "2023\n",
      "2022\n",
      "Assets\n",
      "Total reportable...\n",
      "Score: 0.03 :: Infrastructure\n",
      "Consulting\n",
      "Software\n",
      "We also expanded profit margins by emphasizing high-\n",
      "value offeri...\n",
      "Score: 0.03 :: OVERVIEW\n",
      "The financial section of the International Business Machines Corporation (IBM or the compan...\n"
     ]
    }
   ],
   "source": [
    "# also could store in a pandas dataframe\n",
    "for node in nodes_with_scores:\n",
    "    print(f\"Score: {node.score:.2f} :: {node.text[:100]}...\") #first 100 characters only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d247b6c",
   "metadata": {},
   "source": [
    "The output shows the nodes that were created and their relevance to the query about annual revenue. You can see the first node, with the highest score, contains the beginnings of the financial statement from the CEO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0611a-ef45-47ca-96e0-6de383e2743d",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6\n",
    "\n",
    "Now you're ready to generate responses to these generated queries. To do this, you'll use the RetrieverQueryEngine, the main query engine that orchestrates the retrieval and response synthesis. It has three main components:\n",
    "\n",
    "-   retriever: This is the component responsible for fetching relevant documents or nodes from the index based on the query.\n",
    "-   node_postprocessors: A list of post-processors that refine the retrieved nodes before they're used to generate the response.\n",
    "-   response_synthesizer: Responsible for generating the final response based on the retrieved and post-processed nodes.\n",
    "\n",
    "In this tutorial, you'll only use the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "207c1902-3581-4c66-93b1-b07bfe2b4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524735a-40ea-47ee-ba94-3565a6a07bf4",
   "metadata": {},
   "source": [
    "Now you can generate a response for a query. As you saw in Step 4, this will create multiple queries, rank and synthesize them, and then pass the queries to the two different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4a4f10dd-d1a1-4554-8c94-f11714559300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IBM generated $61.9 billion in revenue in 2023, up 3% at constant currency.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What was IBMs revenue in 2023?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2ad2f3b8-f24c-4e19-a1ba-763ebde26409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IBM generated $61.9 billion in revenue in 2023, up 3% at constant currency.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cd340",
   "metadata": {},
   "source": [
    "Now, another query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "05f0d7c5-434c-47cb-a1a1-6b573041b95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Operating (non-GAAP) expense-to-revenue ratio in 2023 was 39.8%.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"What was the Operating (non-GAAP) expense-to-revenue ratio in 2023?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191dc844-1f21-4c17-91a7-2a7caa7ca9f9",
   "metadata": {},
   "source": [
    "You can also make sure that the RAG system doesn’t report on anything that it doesn’t or shouldn’t know about:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "12f6c1fc-7e61-4da5-82f7-20012cebbbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shareholder report does not mention anything about the price of eggs.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"What does the shareholder report say about the price of eggs?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcef03e",
   "metadata": {},
   "source": [
    "You can try an unethical query as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6ed300f7-513e-4af0-a554-1f915c90179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The provided context does not contain any information related to hacking into a wifi network. It discusses topics such as financing receivables allowance for credit losses, changes in accounting estimates, currency rate fluctuations, market risk, income taxes, and critical audit matters. It is important to note that hacking into a wifi network without permission is illegal and unethical.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"How do I hack into a wifi network?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fee4b-703c-4fea-a747-3d3680730ead",
   "metadata": {},
   "source": [
    "We can see that our Granite model not only sticks to topics covered in the document but also behaves in a safe and responsible manner. Granite 3.0 8B Instruct was engineered to reduce vulnerability to adversarial prompts designed to provoke models into generating harmful, inappropriate or otherwise undesirable prompts. In this case, the query about hacking a wifi network wasn't found in the source documents but it also triggered safeguards built into the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23adc67f-1983-4397-80d0-5f9366684c94",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this tutorial you built a RAG application using LlamaIndex, watsonx, and IBM Granite to extract information from a PDF and create a question answering system using query fusion. You can learn more about LlamaIndex at [LlamaIndex.ai](https://www.llamaindex.ai/) or at their [Github repository](https://github.com/run-llama/llama_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
