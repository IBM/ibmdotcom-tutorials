{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tool calling in large language models (LLMs) is the ability of the LLM to interact with external tools, services or application programming interfaces (APIs) to perform tasks. This relationship allows LLMs to extend their functionality, enhancing their ability to handle real-world tasks that might require access to external data, real-time information or specific applications. When an LLM uses a web search tool, it can call the web to fetch real-time data that isn’t available in the model's training data. Other types of tools might include Python for calculations, data analysis or visualization, or calling a service endpoint for data. Tool calling can make a chatbot more dynamic and adaptable allowing it to provide more accurate, relevant and detailed responses based on live data or specialized tasks outside its immediate knowledge base. Popular frameworks for tool-calling include Langchain and now Ollama.\n",
    "\n",
    "Ollama is a platform that offers open source, local AI models for use on personal devices so that users can run LLMs directly on their computers. Unlike a service such as the OpenAI API, there’s no need for an account because the model is on your local machine. Ollama focuses on privacy, performance and ease of use, enabling users to access and interact with AI models without sending data to external servers. This approach can be particularly appealing for those users concerned about data privacy or who want to avoid the reliance on external APIs. Ollama’s platform is designed to be easy to set up and use, supporting various models. This gives users a range of tools for natural language processing, code generation and other AI tasks directly on their own hardware. It is well suited to a tool calling architecture because it can access all the capabilities of a local environment including data, programs and custom software.\n",
    "\n",
    "In this tutorial, you'll learn how to set up tool calling by using Ollama to look through a local file system, a task that would be difficult to do with a remote LLM. Many Ollama models are available for tool calling and building AI agents such as Mistral and Llama 3.2, a full list is available at https://ollama.com/library. In this case, we'll use IBM® Granite™ 3.2 Dense that has tool support. The 2B and 8B models are text-only dense LLMs trained on designed to support tool-based use cases and for retrieval augmented generation (RAG), streamlining code generation, translation and bug fixing.\n",
    "\n",
    "# Step 1\n",
    "\n",
    "First, download Ollama from https://ollama.com/download and install it for your operating system. On OSX this download is done via a .dmg file, on Linux® through a single shell command and on Windows with an installer. You might need admin access on your machine to run the installer.\n",
    "\n",
    "You can test that Ollama is correctly installed by opening a terminal or command prompt and entering:\n",
    "\n",
    "```\n",
    "ollama -v \n",
    "```\n",
    "\n",
    "# Step 2\n",
    "\n",
    "Next, add the initial imports. This demo uses the Ollama Python library to communicate with Ollama and the pymupdf library to read PDF files in the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.25.4-cp39-abi3-macosx_11_0_arm64.whl (18.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.6/18.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.25.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf\n",
    "\n",
    "import ollama\n",
    "import os\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you'll pull the model that you'll be using throughout this tutorial. This downloads the model weights from ollama to your local computer and stores them for use without needing to make any remote API calls later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!ollama pull granite3.2\n",
    "!ollama pull granite3.2-vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the tools\n",
    "\n",
    "Now you'll define the tools that the Ollama tools instance has access to. Because the intent of the tools is to read files and look through images in the local file system, you'll create two Python functions for each of those tools. The first is called `search_text_files` and it takes a keyword to search for in the local files. For the purposes of this demo, the code searches only for files in a specific folder but it can be extended to include a second parameter that sets which folder the tool searches in.\n",
    "\n",
    "You can use simple string matching to see whether the keyword is in the document. And because Ollama makes calling local LLMs easy, `search_text_files` uses Granite 3.2 to determine whether the keyword describes the document text. This step is done by reading the document into a string called `document_text`. The function then calls ollama.chat and prompts the model with the following:\n",
    "\n",
    "```\n",
    "\"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + document_text \n",
    "```\n",
    "\n",
    "If the model responds 'yes', then the function returns the name of the file that contains the keyword that the user indicated in the prompt. If none of the files seem to contain the information, then the function returns 'None' as a string.\n",
    "\n",
    "This function might run slowly the first time because Ollama downloads Granite 3.2 Dense. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_text_files(keyword: str) -> str:\n",
    "  \n",
    "  directory = os.listdir(\"./files/\")\n",
    "  for fname in directory:\n",
    "    \n",
    "    # look through all the files in our directory that aren't hidden files\n",
    "    if os.path.isfile(\"./files/\" + fname) and not fname.startswith('.'):\n",
    "\n",
    "        if(fname.endswith(\".pdf\")):\n",
    "           \n",
    "           document_text = \"\"\n",
    "           doc = pymupdf.open(\"./files/\" + fname)\n",
    "\n",
    "           for page in doc: # iterate the document pages\n",
    "               document_text += page.get_text() # get plain text (is in UTF-8)\n",
    "               \n",
    "           doc.close()\n",
    "\n",
    "           prompt = \"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + document_text \n",
    "\n",
    "           res = ollama.chat(\n",
    "                model=\"granite3.2:8b\",\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "\n",
    "           if 'Yes' in res['message']['content']:\n",
    "                return \"./files/\" + fname\n",
    "\n",
    "        elif(fname.endswith(\".txt\")):\n",
    "\n",
    "            f = open(\"./files/\" + fname, 'r')\n",
    "            file_content = f.read()\n",
    "            \n",
    "            prompt = \"Respond only 'yes' or 'no', do not add any additional information. Is the following text about \" + keyword + \"? \" + file_content \n",
    "\n",
    "            res = ollama.chat(\n",
    "                model=\"granite3.2:8b\",\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "           \n",
    "            if 'Yes' in res['message']['content']:\n",
    "                f.close()\n",
    "                return \"./files/\" + fname\n",
    "\n",
    "  return \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The second tool is called `search_image_files` and it takes a keyword to search for in the local photos. The search is done by using the [Granite 3.2 Vision](https://ollama.com/library/granite3.2-vision) image description model via ollama. This model returns a text description of each image file in the folder and searches for the keyword in the description. One of the strengths of using Ollama is that multiagent systems can easily be built to call one model with another. \n",
    " \n",
    " The function returns a string, which is the name of the file whose description contains the keyword that the user indicated in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 170370233dd5... 100% ▕████████████████▏ 4.1 GB                         \n",
      "pulling 72d6f08a42f6... 100% ▕████████████████▏ 624 MB                         \n",
      "pulling 43070e2d4e53... 100% ▕████████████████▏  11 KB                         \n",
      "pulling c43332387573... 100% ▕████████████████▏   67 B                         \n",
      "pulling ed11eda7790d... 100% ▕████████████████▏   30 B                         \n",
      "pulling 7c658f9561e5... 100% ▕████████████████▏  564 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def search_image_files(keyword:str) -> str:\n",
    "\n",
    "    directory = os.listdir(\"./files/\")\n",
    "    image_file_types = (\"jpg\", \"png\", \"jpeg\")\n",
    "\n",
    "    for fname in directory:\n",
    "\n",
    "        if os.path.isfile(\"./files/\" + fname) and not fname.startswith('.') and fname.endswith(image_file_types):\n",
    "            res = ollama.chat(\n",
    "                model=\"granite3.2-vision\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': 'Describe this image in short sentences. Use simple phrases first and then describe it more fully.',\n",
    "                        'images': [\"./files/\" + fname]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if keyword in res['message']['content']:\n",
    "                return \"./files/\" + fname\n",
    "    \n",
    "    return \"None\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Step 4: Define the tools for ollama\n",
    "\n",
    " Now that the functions for Ollama to call have been defined, you'll configure the tool information for Ollama itself. The first step is to create an object that maps the name of the tool to the functions for Ollama function calling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions = {\n",
    "  'Search inside text files':search_text_files,\n",
    "  'Search inside image files':search_image_files\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure a tools array to tell Ollama what tools it will have access to and what those tools require. This array has one object schema per tool that tells the Ollama tool calling framework how to call the tool and what it returns.\n",
    "\n",
    "In the case of both tools that you created earlier, they are functions that require a `keyword` parameter. Currently, only functions are supported although this can change in the future. The description of the function and of the parameter help the model call the tool correctly. The `description` field for the function of each tool is passed to the LLM when it selects which tool to use. The `description` of the keyword is passed to the model when it generates the parameters that will be passed to the tool. Both are places you may look to fine-tune prompts when you create your own tool calling applications with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools don't need to be defined as an object but this helps pass the correct parameters\n",
    "# to the tool call itself by giving the model a prompt of how the tool is to be used\n",
    "ollama_tools=[\n",
    "     {\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'Search inside text files',\n",
    "        'description': 'This tool searches in PDF or plaintext or text files in the local file system for descriptions or mentions of the keyword.',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'keyword': {\n",
    "              'type': 'string',\n",
    "              'description': 'Generate one keyword from the user request to search for in text files',\n",
    "            },\n",
    "          },\n",
    "          'required': ['keyword'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "    {\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'Search inside image files',\n",
    "        'description': 'This tool searches for photos or image files in the local file system for the keyword.',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'keyword': {\n",
    "              'type': 'string',\n",
    "              'description': 'Generate one keyword from the user request to search for in image files',\n",
    "            },\n",
    "          },\n",
    "          'required': ['keyword'],\n",
    "        },\n",
    "      },\n",
    "    },\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll use this tools definition when you call Ollama with user input.\n",
    "\n",
    "# Step 5: Pass user input to ollama\n",
    "\n",
    "Now it’s time to pass user input to Ollama and have it return the results of the tool calls. First, make sure that Ollama is running on your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['ollama', 'serve']>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if ollama is not currently running, start it\n",
    "import subprocess\n",
    "subprocess.Popen([\"ollama\",\"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ask the user for input. You can also hardcode the input or retrieve from a chat interface depending on how you configure your application. The `input` function waits for user input before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "user_input = input(\"What would you like to search for?\")\n",
    "print(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the user query is passed to Ollama itself. The messages need a role for the user and the content that the user inputs. This input is passed to Ollama by using the `chat` function. The first parameter is the model that you want to use, in this case Granite 3.1 Dense, then the message with the user input, and finally the tools array that you configured earlier.\n",
    "\n",
    "The `chat` function generates an output selecting which tool to use and what parameters should be passed to it in the subsequent tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [{'role': 'user', 'content':user_input}]\n",
    "\n",
    "response: ollama.ChatResponse = ollama.chat(\n",
    "   \n",
    "  # set which model we're using\n",
    "  'granite3.2:8b',\n",
    "\n",
    "  # use the message from the user\n",
    "  messages=messages,\n",
    "\n",
    "  tools=ollama_tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has generated tool calls in the output, run the tool calls with the parameters that the model generated and check the output. In this application, Granite 3.1 Dense is used to generate the final output. The results of the tool calls are added to the initial user input and then passed to the model.\n",
    "\n",
    "Multiple tool calls can return file matches, so the responses are collected in an array that is then passed to Granite 3.1 to generate a response. The prompt that precedes the data instructs the model how to respond:\n",
    "\n",
    "'''\n",
    "If the tool output contains one or more file names, then give the user only the filename found. Do not add additional details. \n",
    "If the tool output is empty ask the user to try again. Here is the tool output: \n",
    "'''\n",
    "\n",
    "The final output is then generated using either the returned file names or the string \"None\" indicating that no files could be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling tool:  Search inside text files  \n",
      " with arguments:  {'keyword': 'dog'}\n",
      " Tool response is ./files/File4.pdf\n",
      "Search inside text files  has output:  ['./files/File4.pdf']\n",
      "Calling tool:  Search inside image files  \n",
      " with arguments:  {'keyword': 'dog'}\n",
      " Tool response is ./files/three.jpg\n",
      "Search inside image files  has output:  ['./files/File4.pdf', './files/three.jpg']\n",
      "Final response: The files containing the keyword \"dog\" were found in File4.pdf and three.jpg.\n"
     ]
    }
   ],
   "source": [
    "# this is a place holder that to use to see whether the tools return anything \n",
    "output = []\n",
    "\n",
    "if response.message.tool_calls:\n",
    "  \n",
    "  # There may be multiple tool calls in the response\n",
    "  for tool_call in response.message.tool_calls:\n",
    "\n",
    "    # Ensure the function is available, and then call it\n",
    "    if function_to_call := available_functions.get(tool_call.function.name):\n",
    "      print('Calling tool: ', tool_call.function.name, ' \\n with arguments: ', tool_call.function.arguments)\n",
    "      tool_res = function_to_call(**tool_call.function.arguments)\n",
    "\n",
    "      print(\" Tool response is \" + str(tool_res))\n",
    "\n",
    "      if(str(tool_res) != \"None\"):\n",
    "        output.append(str(tool_res))\n",
    "        print(tool_call.function.name, ' has output: ', output)\n",
    "    else:\n",
    "      print('Could not find ', tool_call.function.name)\n",
    "\n",
    "  # Now chat with the model using the tool call results\n",
    "  # Add the function response to messages for the model to use\n",
    "  messages.append(response.message)\n",
    "\n",
    "  prompt = '''\n",
    "    If the tool output contains one or more file names, then give the user only the filename found. Do not add additional details. \n",
    "    If the tool output is empty ask the user to try again. Here is the tool output: \n",
    "  '''\n",
    "\n",
    "  messages.append({'role': 'tool', 'content': prompt + \" \" + \", \".join(str(x) for x in output)})\n",
    "  \n",
    "  # Get a response from model with function outputs\n",
    "  final_response = ollama.chat('granite3.2:8b', messages=messages)\n",
    "  print('Final response:', final_response.message.content)\n",
    "\n",
    "else:\n",
    "\n",
    "  # the model wasn't able to pick the correct tool from the prompt\n",
    "  print('No tool calls returned from model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Granite 3.2 picked the correct keyword from the input, 'dogs', and searched through the files in the folder, finding the keyword in a PDF file. Since LLM results are not purely deterministic, you may get slightly different results with the same prompt or very similar prompts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
