{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a document-based question answering system by using Docling with Granite 3.1\n",
    "\n",
    "**Authors:** Ash Minhas, Anna Gutowska\n",
    "\n",
    "In this tutorial, you will use IBM® [Docling](https://github.com/DS4SD/docling) and open-source [Granite™ 3.1](https://www.ibm.com/granite) to perform document visual question answering for various file types. \n",
    "\n",
    "## What is Docling? \n",
    "\n",
    "Docling is an IBM open-source toolkit for parsing documents and exporting them to preferred formats. Input file formats include PDF, DOCX, PPTX, XLSX, Images, HTML, AsciiDoc and Markdown. These documents can be exported to markdown or JSON. Docling also provides [OCR (optical character recognition)](https://www.ibm.com/think/topics/optical-character-recognition) support for scanned documents. Use cases include scanning medical records, banking documents and even travel documents for quicker processing. \n",
    "\n",
    "## RAG and large context windows\n",
    "\n",
    "[Retrieval augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) is an architecture for connecting [large language models (LLMs)](https://www.ibm.com/topics/large-language-models) with external knowledge bases without [fine-tuning](https://www.ibm.com/topics/fine-tuning) or retraining. Text is embedded, stored in a vector database and finally, is used by the pre-trained model to return relevant information for [natural language processing (NLP)](https://www.ibm.com/think/topics/natural-language-processing) and [machine learning](https://www.ibm.com/topics/machine-learning) tasks. \n",
    "\n",
    "When an LLM has a larger [context window](<https://www.ibm.com/think/topics/context-window#:~:text=The%20context%20window%20(or%20%E2%80%9Ccontext,of%20information%20into%20each%20output.>), the generative AI model can process more information at once. This means that we can use both RAG and models with large context windows to leverage the ability to efficiently process more relevant information at a time. The LLM we use in this tutorial is the IBM `Granite-3.1-8B-Instruct` model. This model extends to a context window size of 128K tokens. We will access the model locally by using [Ollama](https://ollama.com/), without the use of an [API](https://www.ibm.com/topics/api). This model is also available on [Hugging Face](https://huggingface.co/ibm-granite). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "This tutorial can be found on our GitHub in the form of a Jupyter Notebook.  Jupyter Notebooks are widely used within [data science](https://www.ibm.com/topics/data-science) to combine code, text, images and [data visualizations](https://www.ibm.com/topics/data-visualization) to formulate a well-formed analysis.\n",
    "\n",
    "### Step 1. Set up your environment\n",
    "We first need to set up our environment by fulfilling some prerequisites. \n",
    "\n",
    "1. Install the latest version of [Ollama](https://ollama.com/) to run locally.\n",
    "\n",
    "2. Pull the latest Granite 3.1 model by running the following command. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Install and import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 0a922eb99317... 100% ▕████████████████▏ 4.9 GB                         \n",
      "pulling f7b956e70ca3... 100% ▕████████████████▏   69 B                         \n",
      "pulling f76a906816c4... 100% ▕████████████████▏ 1.4 KB                         \n",
      "pulling 492069a62c25... 100% ▕████████████████▏  11 KB                         \n",
      "pulling e026ee8ed889... 100% ▕████████████████▏  491 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull granite3.1-dense:8b\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q \"langchain>=0.1.0\" \"langchain-community>=0.0.13\" \"langchain-core>=0.1.17\" \\\n",
    "    \"langchain-ollama>=0.0.1\" \"pdfminer.six>=20221105\" \"markdown>=3.5.2\" \"docling>=2.0.0\" \\\n",
    "    \"beautifulsoup4>=4.12.0\" \"unstructured>=0.12.0\" \"chromadb>=0.4.22\" \"faiss-cpu>=1.7.4\" \\\n",
    "    \"requests>=2.32.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Docling imports\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption, WordFormatOption, SimplePipeline\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Document format detection\n",
    "\n",
    "We will work with various document formats in this tutorial. Let's create a helper function to detect document formats by using the file extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_format(file_path) -> InputFormat:\n",
    "    \"\"\"Determine the document format based on file extension\"\"\"\n",
    "    try:\n",
    "        file_path = str(file_path)\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        format_map = {\n",
    "            '.pdf': InputFormat.PDF,\n",
    "            '.docx': InputFormat.DOCX,\n",
    "            '.doc': InputFormat.DOCX,\n",
    "            '.pptx': InputFormat.PPTX,\n",
    "            '.html': InputFormat.HTML,\n",
    "            '.htm': InputFormat.HTML\n",
    "        }\n",
    "        return format_map.get(extension, None)\n",
    "    except:\n",
    "        return \"Error in get_document_format: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Document conversion\n",
    "\n",
    "Next, we can use the `DocumentConverter` class to create a function that converts any supported document to markdown. This function identifies text, data tables, document images and captions by using Docling. The function takes a file as input, processes it using Docling's advanced document handling, converts it to markdown and saves the results in a Markdown file. Both scanned and text-based documents are supported and document structure is preserved. Key components of this function are:\n",
    "- `PdfPipelineOptions`: Configures how PDFs are processed.\n",
    "- `TesseractCliOcrOptions`: Sets up OCR for scanned documents.\n",
    "- `DocumentConverter`: Handles the actual conversion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_document_to_markdown(doc_path) -> str:\n",
    "    \"\"\"Convert document to markdown using simplified pipeline\"\"\"\n",
    "    try:\n",
    "        # Convert to absolute path string\n",
    "        input_path = os.path.abspath(str(doc_path))\n",
    "        print(f\"Converting document: {doc_path}\")\n",
    "\n",
    "        # Create temporary directory for processing\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Copy input file to temp directory\n",
    "            temp_input = os.path.join(temp_dir, os.path.basename(input_path))\n",
    "            shutil.copy2(input_path, temp_input)\n",
    "\n",
    "            # Configure pipeline options\n",
    "            pipeline_options = PdfPipelineOptions()\n",
    "            pipeline_options.do_ocr = False  # Disable OCR temporarily\n",
    "            pipeline_options.do_table_structure = True\n",
    "\n",
    "            # Create converter with minimal options\n",
    "            converter = DocumentConverter(\n",
    "                allowed_formats=[\n",
    "                    InputFormat.PDF,\n",
    "                    InputFormat.DOCX,\n",
    "                    InputFormat.HTML,\n",
    "                    InputFormat.PPTX,\n",
    "                ],\n",
    "                format_options={\n",
    "                    InputFormat.PDF: PdfFormatOption(\n",
    "                        pipeline_options=pipeline_options,\n",
    "                    ),\n",
    "                    InputFormat.DOCX: WordFormatOption(\n",
    "                        pipeline_cls=SimplePipeline\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Convert document\n",
    "            print(\"Starting conversion...\")\n",
    "            conv_result = converter.convert(temp_input)\n",
    "\n",
    "            if not conv_result or not conv_result.document:\n",
    "                raise ValueError(f\"Failed to convert document: {doc_path}\")\n",
    "\n",
    "            # Export to markdown\n",
    "            print(\"Exporting to markdown...\")\n",
    "            md = conv_result.document.export_to_markdown()\n",
    "\n",
    "            # Create output path\n",
    "            output_dir = os.path.dirname(input_path)\n",
    "            base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "            md_path = os.path.join(output_dir, f\"{base_name}_converted.md\")\n",
    "\n",
    "            # Write markdown file\n",
    "            print(f\"Writing markdown to: {base_name}_converted.md\")\n",
    "            with open(md_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "                fp.write(md)\n",
    "\n",
    "            return md_path\n",
    "    except:\n",
    "        return f\"Error converting document: {doc_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. QA chain setup\n",
    "\n",
    "The QA chain is the heart of our system. It combines several components:\n",
    "\n",
    "1. Document loading:\n",
    "- Loads the markdown file that we created.\n",
    "- Splits it into manageable chunks for processing.\n",
    "\n",
    "2. Text splitting:\n",
    "- Breaks down the document into smaller pieces.\n",
    "- Maintains context with overlap between chunks.\n",
    "- Ensures efficient processing by the language model.\n",
    "\n",
    "3. Vector store:\n",
    "- Creates embeddings for each text chunk.\n",
    "- Stores them in a FAISS index for fast retrieval.\n",
    "- Enables semantic search capabilities.\n",
    "\n",
    "4. Language model:\n",
    "- Uses Ollama for both embeddings and text generation.\n",
    "- Maintains conversation history.\n",
    "- Generates contextual responses.\n",
    "\n",
    "The following `setup_qa_chain` function sets up this entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_qa_chain(markdown_path: Path, embeddings_model_name:str = \"nomic-embed-text:latest\", model_name: str = \"granite3.1-dense:8b\"):\n",
    "    \"\"\"Set up the QA chain for document processing\"\"\"\n",
    "    # Load and split the document\n",
    "    loader = UnstructuredMarkdownLoader(str(markdown_path)) \n",
    "    documents = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    # texts= documents\n",
    "    \n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OllamaEmbeddings(\n",
    "        model=embeddings_model_name\n",
    "        )\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = OllamaLLM(\n",
    "        model=model_name,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Set up conversation memory\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "        return_messages=True\n",
    "    )\n",
    "    \n",
    "    # Create the chain\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": 10}\n",
    "            ),\n",
    "        memory=memory,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Set up question-answering interface\n",
    "\n",
    "Finally, let's create a simple interface for asking questions. This function takes in the chain and user query as parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(qa_chain, question: str):\n",
    "    \"\"\"Ask a question and display the answer\"\"\"\n",
    "    result = qa_chain.invoke({\"question\": question})\n",
    "    display(Markdown(f\"**Question:** {question}\\n\\n**Answer:** {result['answer']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Perform question-answering\n",
    "\n",
    "Let's put it all together and enumerate over our questions for a specific document. The path to this document is stored in `doc_path` and can be any document you want to test. For a sample dataset of Microsoft Word, PowerPoint, HTML and PDF documents, check out our GitHub. The system maintains conversation history and can handle follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting document: ibmredbook.pdf\n",
      "Starting conversion...\n",
      "Exporting to markdown...\n",
      "Writing markdown to: ibmredbook_converted.md\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question:** What is the main topic of this document?\n",
       "\n",
       "**Answer:** The main topics covered in this document are building and managing containers using Red Hat OpenShift, deploying applications, and security aspects related to containerization on IBM Power systems. The document also includes an introduction to Red Hat OpenShift, its benefits, core concepts, and implementation on IBM Power. Additionally, it discusses multi-architecture containerization, monitoring tools and techniques, log management, performance tuning, and optimization."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question:** What are the key points discussed?\n",
       "\n",
       "**Answer:** This document primarily focuses on implementing Red Hat OpenShift Container Platform on IBM Power systems for managing containers in a hybrid cloud environment. Key points covered include:\n",
       "\n",
       "1. Introduction to Red Hat OpenShift: It is an enterprise Kubernetes platform that extends Kubernetes with additional features and tools, enhancing productivity and security for businesses using container technology at scale.\n",
       "\n",
       "2. Benefits of Using Red Hat OpenShift for Container Orchestration: The document highlights the advantages of employing Red Hat OpenShift for managing containers, such as its comprehensive solution for hybrid cloud environments, including a container runtime, networking, monitoring, a container registry, authentication, and authorization.\n",
       "\n",
       "3. Minimum IBM Power Requirements: Red Hat OpenShift Container Platform 4.15 can be installed on IBM Power 9 or IBM Power 10 processor-based systems.\n",
       "\n",
       "4. Deploying Red Hat OpenShift on IBM Power Systems: This involves tailoring the networking infrastructure to leverage the robust capabilities and unique architecture of Power Systems, optimizing network performance for high throughput and low latency, ensuring network security and compliance, and managing network configurations for enterprise-level deployments.\n",
       "\n",
       "5. Optimizing Network Performance: The document emphasizes the importance of faster storage, particularly for etcd on control plane nodes, as Red Hat OpenShift Container Platform is sensitive to disk performance.\n",
       "\n",
       "6. Multi-Architecture Containerization: The text discusses key concepts in multi-architecture containerization and provides guidelines for implementing it using IBM Power control planes. It also addresses challenges and solutions related to multi-architecture containerization.\n",
       "\n",
       "7. Security Aspects: Enterprise-grade security is mentioned as a crucial aspect of Red Hat OpenShift on IBM Power systems, although specific details are not provided in the given context.\n",
       "\n",
       "8. Monitoring Tools and Log Management: The document does not explicitly mention monitoring tools or log management; however, it can be inferred that these aspects are covered within the broader context of managing containers using Red Hat OpenShift on IBM Power systems.\n",
       "\n",
       "9. Performance Tuning and Optimization: While specific tuning and optimization techniques are not detailed in the provided context, the document implies that performance considerations should be taken into account during deployment and configuration."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question:** Can you summarize the conclusions?\n",
       "\n",
       "**Answer:** The document discusses the implementation of Red Hat OpenShift Container Platform on IBM Power systems for managing containers in a hybrid cloud environment. Here are the main conclusions drawn from this document:\n",
       "\n",
       "1. Benefits: Red Hat OpenShift provides a comprehensive solution for hybrid cloud environments, encompassing essential components such as a container runtime, networking, monitoring, a container registry, authentication, and authorization. It extends Kubernetes with additional features and tools to enhance productivity and security, making it an ideal choice for businesses looking to leverage container technology at scale.\n",
       "\n",
       "2. Minimum Requirements: Red Hat OpenShift Container Platform 4.15 can be installed on IBM Power 9 or IBM Power 10 processor-based systems. For comprehensive guidance and further information on installation and configuration, refer to the IBM Redbooks publication Implementing, Tuning, and Optimizing Workloads with Red Hat OpenShift on IBM Power (SG24-8537) and Red Hat OpenShift Documentation.\n",
       "\n",
       "3. Deployment Process: Deploying Red Hat OpenShift on IBM Power Systems involves tailoring the networking infrastructure to fully leverage the robust capabilities and unique architecture of Power Systems. This includes optimizing network performance for high throughput and low latency, ensuring network security and compliance, and managing network configurations to meet enterprise-level demands.\n",
       "\n",
       "4. Network Performance Optimization: Faster storage is recommended, particularly for etcd on control plane nodes. On many cloud platforms, storage size and IOPS scale together, so you might need to over-allocate storage volume to obtain sufficient performance.\n",
       "\n",
       "5. Multi-Architecture Containerization: Red Hat OpenShift supports multiple architectures (x86 and IBM Power) with RHOS 4.14 or later, simplifying the management of your Red Hat OpenShift environment on both x86 and IBM Power servers.\n",
       "\n",
       "6. Security Aspects: The integration of Red Hat OpenShift running on IBM Power servers with existing infrastructure involves strategic networking solutions that bridge on-premises systems with your new cloud environment. This enables organizations to leverage the strengths of both infrastructures for enhanced flexibility, scalability, and resilience while ensuring network security and compliance.\n",
       "\n",
       "7. Performance Tuning: The document does not provide specific details about performance tuning; however, it is mentioned that optimizing network performance for high throughput and low latency is essential. For comprehensive guidance on performance tuning, refer to the IBM Redbooks publication Implementing, Tuning, and Optimizing Workloads with Red Hat OpenShift on IBM Power (SG24-8537) and Red Hat OpenShift Documentation.\n",
       "\n",
       "In summary, this document highlights that implementing Red Hat OpenShift Container Platform on IBM Power systems offers a robust foundation for developing, deploying, and scaling cloud-native applications in a hybrid cloud environment. It emphasizes the importance of optimizing network performance, ensuring security, and leveraging multi-architecture containerization capabilities to create an efficient and flexible solution for managing containers."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process a document\n",
    "doc_path = Path(\"ibmredbook.pdf\")  # Replace with your document path\n",
    "\n",
    "# Check format and process\n",
    "doc_format = get_document_format(doc_path)\n",
    "if doc_format:\n",
    "    md_path = convert_document_to_markdown(doc_path)\n",
    "    qa_chain = setup_qa_chain(md_path)\n",
    "    \n",
    "    # Example questions\n",
    "    questions = [\n",
    "        \"What is the main topic of this document?\",\n",
    "        \"What are the key points discussed?\",\n",
    "        \"Can you summarize the conclusions?\",\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        ask_question(qa_chain, question)\n",
    "else:\n",
    "    print(f\"Unsupported document format: {doc_path.suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The system was able to retrieve relevant information from the document to answer questions. Feel free to test this system with any of your own files and questions!\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Using Docling and Granite 3.1, you built a document question answering system compatible with various file types. As a next step, this methodology can be applied to a [chatbot](https://www.ibm.com/topics/chatbots) with an interactive UI. There are many opportunities to transform this tutorial to apply to specific use cases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
