{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement agentic chunking to optimize LLM inputs with Langchain and watsonx.ai \n",
    " \n",
    "\n",
    "**Author**: Shalini Harkar \n",
    "\n",
    "\n",
    "### What is Agentic Chunking?\n",
    "\n",
    "The way language models process and segment text is changing from the traditional static approach, to a better, more responsive process. Unlike traditional fixed-size chunking , which chunks large documents at fixed points, agentic chunking employs AI-based techniques to analyze content in a dynamic process, and to determine the best way to segment the text.\n",
    "\n",
    "Agentic chunking makes use of AI-based text-splitting methods, recursive chunking, and chunk overlap methods, which work concurrently to polish chunking ability, preserving links between notable ideas while optimizing contextual windows in real time. With agentic chunking, each chunk is enriched with metadata to deepen retrieval accuracy and overall model efficiency. This is particularly important in RAG applications applications , where segmentation of data can directly impact retrieval quality and coherence of the response. Meaningful context is preserved in all chunks, making this approach incredibly important to chatbots, knowledge bases, and generative ai use cases..\n",
    "\n",
    "\n",
    "### Key Elements of Agentic Chunking \n",
    " 1. Adaptive Chunking Strategy – Dynamically choosing the best chunking method based on the type of content, the intent behind the query, and the needs for retrieval to ensure effective segmentation.\n",
    "\n",
    " 2. Dynamic Chunk Sizing – Modifying chunk sizes in real time by taking into account the semantic structure and context, instead of sticking to fixed token limits.\n",
    "\n",
    " 3. Context-Preserving Overlap – Smartly assessing the overlap between chunks to keep coherence intact and avoid losing essential information, thereby enhancing retrieval efficiency.\n",
    "\n",
    "\n",
    "### Advantages of Agentic Chunking Over Traditional Methods\n",
    "1. Retains Context – Maintains crucial information without unnecessary breaks.\n",
    "\n",
    "2. Smart Sizing – Adjusts chunk boundaries according to meaning and significance.\n",
    "\n",
    "3. Query-Optimized – Continuously refines chunks to match specific queries.\n",
    "\n",
    "4. Efficient Retrieval – Improves search and RAG by minimizing unnecessary fragmentation.\n",
    "\n",
    "\n",
    "In this tutorial, you will experiment with agentic chunking strategy by using the IBM Granite-3.0-8B-Instruct model now available on watsonx.ai®. The overall goal is to perform efficient chunking to effectively implement RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite \n",
    "You need an IBM Cloud account® to create a watsonx.ai project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Set up your environment\n",
    "While you can choose from several tools, this tutorial walks you through how to set up an IBM account to use a Jupyter Notebook. \n",
    "\n",
    "1. Log in to [watsonx.ai](https://dataplatform.cloud.ibm.com/registration/stepone?context=wx&apps=all) using your IBM Cloud® account.\n",
    "\n",
    "2. Create a [watsonx.ai project](https://www.ibm.com/docs/en/watsonx/saas?topic=projects-creating-project).\n",
    "\n",
    "\tYou can get your project ID from within your project. Click the **Manage** tab. Then, copy the project ID from the **Details** section of the **General** page. You need this ID for this tutorial.\n",
    "\n",
    "3. Create a [Jupyter Notebook](https://www.ibm.com/docs/en/watsonx/saas?topic=editor-creating-managing-notebooks).\n",
    "\n",
    "\tThis step opens a Jupyter Notebook environment where you can copy the code from this tutorial.  Alternatively, you can download this notebook to your local system and upload it to your watsonx.ai project as an asset. To view more Granite tutorials, check out the [IBM Granite Community](https://github.com/ibm-granite-community). ss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2. Set up a watsonx.ai Runtime instance and API key.\n",
    "\n",
    "1. Create a [watsonx.ai Runtime](https://cloud.ibm.com/catalog/services/watsonxai-runtime) service instance (select your appropriate region and choose the Lite plan, which is a free instance).\n",
    "\n",
    "2. Generate an application programming interface [(API) key](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml-authentication.html). \n",
    "\n",
    "3. Associate the watsonx.ai Runtime service instance to the project that you created in [watsonx.ai](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/assoc-services.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3. Install and import relevant libraries and set up your credentials\n",
    "\n",
    "We need a few libraries and modules for this tutorial. Make sure to import the following ones and if they're not installed, a quick pip installation resolves the problem. \n",
    "\n",
    "*Note, this tutorial was built using Python 3.12.7.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installations\n",
    "!pip install -q langchain langchain-ibm langchain_experimental langchain-text-splitters langchain_chroma transformers bs4 langchain_huggingface sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from transformers import AutoTokenizer\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set our credentials, we need the `WATSONX_APIKEY` and `WATSONX_PROJECT_ID` you generated in step 1. We will also set the URL serving as the API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WATSONX_APIKEY = getpass.getpass(\"Please enter your watsonx.ai Runtime API key (hit enter): \")\n",
    "\n",
    "WATSONX_PROJECT_ID = getpass.getpass(\"Please enter your project ID (hit enter): \")\n",
    "\n",
    "URL = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Initialize your LLM\n",
    "\n",
    "We will use Granite 3.1 as our LLM for this tutorial. To initialize the LLM, we need to set the model parameters. To learn more about these model parameters, such as the minimum and maximum token limits, refer to the [documentation](https://ibm.github.io/watsonx-ai-python-sdk/fm_model.html#metanames.GenTextParamsMetaNames)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "    model_id= \"ibm/granite-3-8b-instruct\", \n",
    "    url=URL,\n",
    "    apikey=WATSONX_APIKEY,\n",
    "    project_id=WATSONX_PROJECT_ID,\n",
    "    params={\n",
    "        GenParams.DECODING_METHOD: \"greedy\",\n",
    "        GenParams.TEMPERATURE: 0,\n",
    "        GenParams.MIN_NEW_TOKENS: 5,\n",
    "        GenParams.MAX_NEW_TOKENS: 2000,\n",
    "        GenParams.REPETITION_PENALTY:1.2,\n",
    "        GenParams.STOP_SEQUENCES: [\"\\n\\n\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Load your document\n",
    "\n",
    "This function extracts the  text content from IBM's explainer page on Machine learning. This function removes unwanted HTML elements (scripts, styles), and returns clean, readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Function to extract text from a web page\n",
    "def get_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to fetch the page, status code: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted elements (scripts, styles)\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    return soup.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ibm.com/think/topics/machine-learning\"  # Example URL\n",
    "web_text = get_text_from_url(url)  # Fetch and clean text from URL\n",
    "web_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's provide sample code for implementing Agentic Chunking using  LangChain and granite model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 . Instead of using a fixed-length chunking method, we used Agentic Chunking here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Agentic Chunking Function\n",
    "def agentic_chunking(text):\n",
    "    \"\"\"\n",
    "    Dynamically splits text into meaningful chunks using LLM.\n",
    "    \"\"\"\n",
    "    system_message = SystemMessage(content=\"You are an AI assistant helping to split text into meaningful chunks based on topics.\")\n",
    "    human_message = HumanMessage(content=f\"Please divide the following text into semantically different, separate and meaningful chunks:\\n\\n{text}\")\n",
    "\n",
    "    response = llm.invoke([system_message, human_message])  # LLM returns a string\n",
    "    return response.split(\"\\n\\n\")  # Split based on meaningful sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step 7- Calling the agentic Chunking defined above for Sample Text\n",
    "chunks = agentic_chunking(web_text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets print out the chunks for better understanding of their output structure. \n",
    "for i, chunk in enumerate(chunks,1):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great! As you can see in the output, the chunks are successfully created by the agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Create vector store\n",
    "Now that we have experimented with Agentic  chunking on the given text , let's move along with our RAG implementation. For this tutorial, we will choose the chunks produced by the Agents and convert them to vector embeddings. An open source vector store we can use is Chroma DB. We can easily access Chroma functionality through the langchain_chroma package.\n",
    "\n",
    "Let's initialize our Chroma vector database, provide it with our embeddings model and add our documents produced by Agentic chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"ibm-granite/granite-embedding-30m-english\")\n",
    "\n",
    "# Create a Chroma vector database\n",
    "vector_db = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings_model\n",
    ")\n",
    "\n",
    "# Convert each text chunk into a Document object\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Add the documents to the vector database\n",
    "vector_db.add_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9. Structure the prompt template\n",
    "Now, we can create a prompt template for our LLM. This template ensures that we can ask multiple questions while maintaining a consistent prompt structure. Additionally, we can integrate our vector store as the retriever, finalizing the RAG framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt_template = \"\"\"<|start_of_role|>user<|end_of_role|>Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {input}<|end_of_text|>\n",
    "<|start_of_role|>assistant<|end_of_role|>\"\"\"\n",
    "\n",
    "qa_chain_prompt = PromptTemplate.from_template(prompt_template)\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, qa_chain_prompt)\n",
    "rag_chain = create_retrieval_chain(vector_db.as_retriever(), combine_docs_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10. Prompt the RAG chain\n",
    "Using our Agentic Chunks in the RAG workflow, let's invoke a user query. First, we can strategically prompt the model without any additional context from the vector store we built to test whether the model is using its built-in knowledge or truly using the RAG context. Using  Machine learning explainer from IBM's   Let's ask the LLM about Docling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(\"What is Model optimization process\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the model was not trained on information about Model optimization process and without outside tools or information, it cannot provide us with the correct information. The model hallucinates. Now, let's try providing the same query to the RAG chain with Agentic Chunks we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_output = rag_chain.invoke({\"input\": \"What is Model optimization process?\"})\n",
    "rag_output['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The Granite model correctly used the Agentic RAG chunks as context to tell us correct information about Model optimization process while preserving semantic coherence. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Summary\n",
    "\n",
    "In this tutorial, we generated smaller pieces of relevant information using AI agents in the chunking process and constructed a retrieval-augmented generation (RAG) pipeline.\n",
    "\n",
    "This method improves information retrieval and context window optimization using artificial intelligence and natural language processing (NLP). It streamlines data chunks to enhance retrieval efficiency when leveraging large language models (LLMs) like OpenAI's GPT models for better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
