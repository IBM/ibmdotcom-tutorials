{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940fe050-f2ca-4319-955d-fe8b931ffccb",
   "metadata": {},
   "source": [
    "# Mastering Llama Stack: A Comprehensive Guide to Building AI Systems with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c271612-6a45-4d0f-944d-ab2758b4f5a2",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d4c77-db2a-48a0-84c5-fd975a9b1c4a",
   "metadata": {},
   "source": [
    " In this tutorial we will explore Meta AI’s Llama Stack, a powerful and standardized API framework designed to simplify the development of generative AI applications. The Llama Stack API offers essential building blocks that cover the entire development lifecycle, including model training, fine-tuning, inference, evaluation and more. \n",
    "\n",
    "These modular APIs are designed to interconnect seamlessly, allowing developers to create scalable AI development solutions that spans both local and cloud environments.\n",
    "\n",
    "The Llama Stack is composed of a variety of APIs, each serving a specific function. These APIs include capabilities such as inference, safety, memory, agentic systems, evaluation, post-training processes, synthetic data generation and reward scoring.\n",
    "\n",
    "Each of  APIs consists of multiple REST endpoints, serving as distinct access points for performing specific actions or retrieving data within the Llama Stack.\n",
    "\n",
    "API providers deliver the functional support for Llama Stack APIs by offering concrete implementations that bring these APIs into action. For example, the inference API can be implemented using libraries like torch, vLLM, or TensorRT.\n",
    "\n",
    " Providers can either run locally or be just a pointer to a remote REST service, such as cloud-based or dedicated inference providers, enabling flexible deployment across different environments. \n",
    "\n",
    "A Llama Stack Distribution integrates APIs and providers into a cohesive framework for developers, enabling the flexibility to mix local and remote providers. This allows smaller models to run locally while larger models can leverage cloud providers. Regardless of the infrastructure, the higher-level APIs remain unchanged, ensuring a consistent development experience. This approach supports smooth transitions between different use cases, such as servers and mobile services, making it ideal for developing scalable generative AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a9586-50d3-47fd-ab4d-22735b2f14ef",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c2e1d-6fba-49ca-a204-e29eead4483c",
   "metadata": {},
   "source": [
    "<li> Python 3.10 installed\n",
    "\n",
    "<li> Access to a GPU (e.g. an NVIDIA GPU) if working with larger models like Meta-Llama3.1-70B\n",
    "\n",
    "<li> Basic familiarity with Large Language models (LLMs) and API integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d4b96-a5a6-4554-be4b-d1dd7270075f",
   "metadata": {},
   "source": [
    "## Setting up Environment and installing Llama Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9e857-955a-4bf0-8822-6e91bc1efd58",
   "metadata": {},
   "source": [
    "### Installing :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c36683-b58d-4135-aaf3-1b0ec544d843",
   "metadata": {},
   "source": [
    " install LlamaStack package using `pip`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d57612-3793-42df-98a4-755d3e4d7a5b",
   "metadata": {},
   "source": [
    "``` shell\n",
    "pip install llama-stack\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e112383-6308-4ebf-ba68-9b29fad84bc5",
   "metadata": {},
   "source": [
    "Alternatively, clone the repository from GitHub and install from source: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4798cc1-3689-44ee-b88a-4180ae3f2419",
   "metadata": {},
   "source": [
    "```shell\n",
    "mkdir -p ~/local\n",
    "\n",
    "cd ~/local\n",
    "\n",
    "git clone git@github.com:meta-llama/llama-stack.git\n",
    "\n",
    "conda create -n stack python=3.10\n",
    "\n",
    "conda activate stack\n",
    "\n",
    "cd llama-stack\n",
    "\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557528a-d1f7-4d38-b676-ebd2d615602c",
   "metadata": {},
   "source": [
    "### Using the Llama CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342012c-f438-4df8-acbd-42cf5c35fccb",
   "metadata": {},
   "source": [
    "The Llama CLI is a core tool that allows you to manage models, build distributions, and run servers. You can explore the availability by running:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a24cc-a851-41a2-a017-5ae521312fa3",
   "metadata": {},
   "source": [
    "```shell\n",
    "llama --help\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849df7b8-ebfb-4ea6-b540-671a96368bdd",
   "metadata": {},
   "source": [
    "The CLI supports subcommands such as: \n",
    "<li> <code>download</code> : For downloading models from Meta or HuggingFace. </li>\n",
    "<li> <code>model</code> : To list or describe available foundation models. </li>\n",
    "<li> <code>stack</code> : To build and run a Llama Stack server. </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4285c-2a83-4b20-b44d-ad05dcc95194",
   "metadata": {},
   "source": [
    "#### Working with models in Llama Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a40a642-26d2-4c14-84f2-a02319afe8ef",
   "metadata": {},
   "source": [
    "Listing the available models: Using the <code>model list</code> to see available models along with their hardware requirements:\n",
    "\n",
    "```shell\n",
    "llama model list\n",
    "```\n",
    "The output will display information such as **Meta-Llama3.1-8B** and **Meta-Llama3.1-70B**, including how many **GPUs** are required and their **context lengths**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3611dd71-79e9-4191-b04c-900f3b4ba235",
   "metadata": {},
   "source": [
    "```shell\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Model Descriptor                      | HuggingFace Repo                            | Context Length | Hardware Requirements      |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-8B                      | meta-llama/Meta-Llama-3.1-8B                | 128K           | 1 GPU, each >= 20GB VRAM   |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-70B                     | meta-llama/Meta-Llama-3.1-70B               | 128K           | 8 GPUs, each >= 20GB VRAM  |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-405B:bf16-mp8           |                                             | 128K           | 8 GPUs, each >= 120GB VRAM |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-405B                    | meta-llama/Meta-Llama-3.1-405B-FP8          | 128K           | 8 GPUs, each >= 70GB VRAM  |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-405B:bf16-mp16          | meta-llama/Meta-Llama-3.1-405B              | 128K           | 16 GPUs, each >= 70GB VRAM |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-8B-Instruct             | meta-llama/Meta-Llama-3.1-8B-Instruct       | 128K           | 1 GPU, each >= 20GB VRAM   |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-70B-Instruct            | meta-llama/Meta-Llama-3.1-70B-Instruct      | 128K           | 8 GPUs, each >= 20GB VRAM  |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-405B-Instruct:bf16-mp8  |                                             | 128K           | 8 GPUs, each >= 120GB VRAM |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-405B-Instruct           | meta-llama/Meta-Llama-3.1-405B-Instruct-FP8 | 128K           | 8 GPUs, each >= 70GB VRAM  |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Meta-Llama3.1-405B-Instruct:bf16-mp16 | meta-llama/Meta-Llama-3.1-405B-Instruct     | 128K           | 16 GPUs, each >= 70GB VRAM |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Llama-Guard-3-8B                      | meta-llama/Llama-Guard-3-8B                 | 128K           | 1 GPU, each >= 20GB VRAM   |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Llama-Guard-3-8B:int8-mp1             | meta-llama/Llama-Guard-3-8B-INT8            | 128K           | 1 GPU, each >= 10GB VRAM   |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "| Prompt-Guard-86M                      | meta-llama/Prompt-Guard-86M                 | 128K           | 1 GPU, each >= 1GB VRAM    |\n",
    "+---------------------------------------+---------------------------------------------+----------------+----------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6863c0-d840-410d-bb52-ddec183dc193",
   "metadata": {},
   "source": [
    "<li> Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df2b3d8-d931-4621-9af6-c6b680ce4686",
   "metadata": {},
   "source": [
    "To download models, use the <code>download</code> subcommand. Here’s an example of downloading **Meta-Llama3.1-8B**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3ccb7-269b-4061-8c43-cc3e49cf9a84",
   "metadata": {},
   "source": [
    "```shell\n",
    "llama download --source meta --model-id Meta-Llama3.1-8B-Instruct --meta-url META_URL\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb04b8-f63b-4b6f-9af8-fbccf974134e",
   "metadata": {},
   "source": [
    "For larger models, such as **Meta-Llama3.1-70B**, make sure you have access to sufficient **GPU** resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26323d1d-3196-40f2-90a0-5414ee289e21",
   "metadata": {},
   "source": [
    "```shell\n",
    "llama download --source meta --model-id Meta-Llama3.1-70B-Instruct --meta-url META_UR\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf14ea8e-4e34-47a2-9a01-bd089c93701c",
   "metadata": {},
   "source": [
    "#### Building and configure Llama Stack Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260a19b-1d7e-43e2-afe8-e8a182080fc5",
   "metadata": {},
   "source": [
    "<li> Building a distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f32e7d-3c6e-427a-98ce-231a0484a328",
   "metadata": {},
   "source": [
    "The Llama Stack lets you build distributions for running \n",
    "AI applications locally or in the cloud. For example, we can build a distribution using the **Meta-Llama3.1-8B-Instruct** model. \n",
    "\n",
    "Create a config file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7f662-1103-43ab-8e96-e9f68df89d7f",
   "metadata": {},
   "source": [
    "```yaml\n",
    " 8b-instruct-build.yaml\n",
    "name: 8b-instruct\n",
    "distribution_spec:\n",
    "  providers:\n",
    "    inference: meta-reference\n",
    "    memory: meta-reference-faiss\n",
    "    safety: meta-reference\n",
    "image_type: conda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b96416-7619-4118-b573-69369c4eeaef",
   "metadata": {},
   "source": [
    "Then, build the distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338492a7-8548-4b31-a245-a219fb34d01b",
   "metadata": {},
   "source": [
    "```shell\n",
    "llama stack build ./8b-instruct-build.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d847cd85-f183-4c53-adaa-ac165b1189a7",
   "metadata": {},
   "source": [
    "<li> Configuring the Distribution\n",
    "After building the distribution, configure it for your specific requirements, such as model parameters and memory options: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af4efa-8f4e-4660-9c56-7ef17d5223e6",
   "metadata": {},
   "source": [
    "```shell\n",
    "llama stack configure ~/.llama/distributions/conda/8b-instruct-build.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24825ed-ac26-40e5-9e4d-56f3664bb248",
   "metadata": {},
   "source": [
    "This command will guid you through configuring each API, including the inference model, safetyAPI, and Agentic System API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190e00f-ef5e-4608-aac5-6f104332d896",
   "metadata": {},
   "source": [
    "#### Running the Llama Stack Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827e9a7-bb4f-4e7f-adf2-a3c0b7fba899",
   "metadata": {},
   "source": [
    "Once the distribution is built and configured, start the server to run the API:\n",
    "\n",
    "```shell\n",
    "llama stack run ~/.llama/builds/conda/8b-instruct-run.yaml\n",
    "```\n",
    "You should see the logs indicating the APIs, including inference, memory, and agentic system, are now active. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de8ec7-7a2d-4db1-ab22-282af4d9d461",
   "metadata": {},
   "source": [
    "#### Using the APIs:\n",
    "\n",
    "##### Inference API:\n",
    "\n",
    "With the Inference API, you can run queries on the model. Here’s an example using the Meta-Llama3.1-8B-Instruct model: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168c484-6982-469e-9e46-7e9f6dacdbd1",
   "metadata": {},
   "source": [
    "```shell\n",
    "curl -X POST http://localhost:5000/inference/completion \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"model\": \"Meta-Llama3.1-8B-Instruct\", \"input\": \"Explain quantum computing.\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1aaf19-f5e2-4d91-aa91-ddff475d1dc5",
   "metadata": {},
   "source": [
    "##### safety API:\n",
    "\n",
    "The safety API ensures that the model’s outputs are ethical and safe. Configure and invoke the Llama Guard model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a791b-2e98-4724-a563-a7cbc108aeb4",
   "metadata": {},
   "source": [
    "```shell\n",
    "curl -X POST http://localhost:5000/safety/run_shields \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\"model\": \"Llama-Guard-3-8B\", \"input\": \"Analyze this text for harmful content.\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd9a51a-d807-4a47-9a78-f70571af3ac1",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19465749-2bdf-459e-9877-0c08c253842f",
   "metadata": {},
   "source": [
    "Llama Stack is open source. The repository containing its specifications and implementations is available on GitHub, allowing developers to contribute and collaborate on improving the platform. The goal of Llama Stack is to provide a standardized set of APIs for building and deploying generative AI applications, and its open-source nature enables broader community involvement in its development. \n",
    "In this tutorial, we covered how to setup and configure the Llama Stack for working with LLM models, including downloading models, building distributions, and running inference servers. With Llama Stack’s robust API system, you can integrate API capabilities across local and cloud environments, ensuring flexibility and scalability for your applications. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
